//! # cudnn
//!
//! Safe Rust wrapper for cudnn C library.
//!
//! This crate was automatically generated by [bindings-generat](https://github.com/your-repo/bindings-generat).
//! It provides safe, idiomatic Rust wrappers around the raw FFI bindings,
//! handling resource management, error conversion, and type safety.
//!
//! ## Features
//!
//! - **RAII Resource Management**: Automatically manages handle lifecycles
//!   with proper Drop implementations for leak-free usage
//! - **Type-Safe Error Handling**: Converts C error codes to Rust Result types
//!   with descriptive error messages
//! - **Idiomatic Rust API**: Methods on wrapper types instead of raw C functions
//! - **Comprehensive Safety**: All unsafe code is carefully encapsulated
//!   with safe interfaces
//!
//! ## Usage
//!
//! ```ignore
//! use cudnn::*;
//!
//! fn main() -> Result<(), Error> {
//!     // Create a handle (specific constructor depends on library)
//!     // let handle = Handle::new()?;
//!     
//!     // Use methods on the handle
//!     // handle.some_operation()?;
//!     
//!     // Handle is automatically cleaned up when dropped
//!     Ok(())
//! }
//! ```
//!
//! ## Error Handling
//!
//! All fallible operations return `Result<T, Error>`. The `Error` type
//! implements `std::error::Error` and provides human-readable error messages.
//!
//! ```ignore
//! match handle.operation() {
//!     Ok(result) => { /* success */ },
//!     Err(Error::NotInitialized) => { /* handle specific error */ },
//!     Err(e) => { /* handle other errors */ },
//! }
//! ```
//!
//! ## Safety
//!
//! This crate encapsulates all `unsafe` FFI calls behind safe Rust interfaces.
//! Resource management is handled automatically through RAII patterns:
//!
//! - Handles are created with safe constructors
//! - Resources are automatically freed when handles are dropped
//! - Null pointer checks prevent undefined behavior
//! - Error codes are converted to Rust Result types
//!
//! ## Thread Safety
//!
//! Thread safety depends on the underlying C library. Check the
//! original library documentation for threading requirements and restrictions.
//! Most handles are `!Send` and `!Sync` by default for safety.
//!
//! ## Performance
//!
//! The wrapper layer has minimal overhead:
//!
//! - Wrapper types are zero-cost abstractions (transparent wrappers)
//! - Methods are marked `#[inline]` for optimization
//! - No runtime overhead beyond error code checks
//!
//! ## Documentation
//!
//! For detailed documentation about the underlying C library, please refer
//! to the official cudnn documentation.
//!
//! ## Generated Code
//!
//! This crate is automatically generated. To customize the bindings:
//!
//! 1. Modify the source headers or configuration
//! 2. Re-run bindings-generat
//! 3. Review the generated code for correctness
//!
//! ## License
//!
//! The generated bindings follow the same license as the original cudnn library.
//! Please check the library's license before using these bindings.

#![allow(dead_code)]
#![allow(non_camel_case_types)]
#![allow(non_snake_case)]
#![allow(non_upper_case_globals)]

// Note: FFI bindings should be in src/ffi.rs
// Run bindgen on your headers and place the output there
#[path = "ffi.rs"]
mod ffi;

// FFI types are available via the `ffi` module (use `ffi::TypeName`)

/// Error type for this library
#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum Error {
    /// Null pointer returned
    NullPointer,
    /// Invalid parameter value
    InvalidParameter,
    /// Invalid string (contains null byte)
    InvalidString,
    /// FFI function returned an error status
    FfiError(i32),
    /// Unknown error
    Unknown,
}

impl From<i32> for Error {
    fn from(code: i32) -> Self {
        if code == 0 {
            // Success code should not become an error
            Error::Unknown
        } else {
            Error::FfiError(code)
        }
    }
}

impl std::fmt::Display for Error {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        match self {
            Error::NullPointer => write!(f, "Null pointer returned"),
            Error::InvalidParameter => write!(f, "Invalid parameter value"),
            Error::InvalidString => write!(f, "Invalid string: contains null byte"),
            Error::FfiError(code) => write!(f, "FFI error: {}", code),
            Error::Unknown => write!(f, "Unknown error"),
        }
    }
}

impl std::error::Error for Error {}

impl Error {
    /// Returns true if this error might be retryable
    pub fn is_retryable(&self) -> bool {
        // Basic errors are generally not retryable
        false
    }

    /// Returns true if this error indicates a fatal condition
    pub fn is_fatal(&self) -> bool {
        match self {
            Error::NullPointer => true,
            Error::InvalidParameter => false,
            Error::InvalidString => false,
            Error::Unknown => true,
            Error::FfiError(_) => false, // Unknown without enum details
        }
    }
}

/// Safe wrapper for `cudnnSeqDataDescriptor_t`
///
/// This wrapper provides RAII-style resource management with automatic cleanup.
/// The wrapper is a zero-cost abstraction - it has the same memory layout
/// as the underlying handle type.
#[repr(transparent)]
pub struct CudnnSeqDataDescriptor {
    handle: ffi::cudnnSeqDataDescriptor_t,
}

impl CudnnSeqDataDescriptor {
    /// Create a new instance
    #[inline]
    /// Create a new instance
    // Warning: Unable to analyze function signature
    #[doc(hidden)]
    pub fn new() -> Result<Self, Error> {
        unsafe {
            // TODO: Implement proper constructor for cudnnCreateSeqDataDescriptor
            Err(Error::Unknown)
        }
    }


    /// Returns the raw FFI handle
    #[inline]
    pub fn as_raw(&self) -> ffi::cudnnSeqDataDescriptor_t {
        self.handle
    }

    /// Returns a mutable pointer to the raw FFI handle
    #[inline]
    pub fn as_raw_mut(&mut self) -> *mut ffi::cudnnSeqDataDescriptor_t {
        &mut self.handle
    }

    /// Constructs a wrapper from a raw FFI handle
    ///
    /// # Safety
    ///
    /// The caller must ensure the handle is valid and properly initialized.
    /// The wrapper will take ownership and call the destructor on drop.
    #[inline]
    pub unsafe fn from_raw(handle: ffi::cudnnSeqDataDescriptor_t) -> Self {
        Self { handle }
    }
}

impl Drop for CudnnSeqDataDescriptor {
    fn drop(&mut self) {
        unsafe {
            if !self.handle.is_null() {
                let _status = ffi::cudnnDestroySeqDataDescriptor(self.handle);
                // Ignoring status in drop - can't propagate errors
            }
        }
    }
}


/// Safe wrapper for `cudnnTensorDescriptor_t`
///
/// This wrapper provides RAII-style resource management with automatic cleanup.
/// The wrapper is a zero-cost abstraction - it has the same memory layout
/// as the underlying handle type.
#[repr(transparent)]
pub struct CudnnTensorDescriptor {
    handle: ffi::cudnnTensorDescriptor_t,
}

impl CudnnTensorDescriptor {
    /// Create a new instance
    #[inline]
    /// Create a new instance
    // Warning: Unable to analyze function signature
    #[doc(hidden)]
    pub fn new() -> Result<Self, Error> {
        unsafe {
            // TODO: Implement proper constructor for cudnnCreateTensorDescriptor
            Err(Error::Unknown)
        }
    }


    /// Returns the raw FFI handle
    #[inline]
    pub fn as_raw(&self) -> ffi::cudnnTensorDescriptor_t {
        self.handle
    }

    /// Returns a mutable pointer to the raw FFI handle
    #[inline]
    pub fn as_raw_mut(&mut self) -> *mut ffi::cudnnTensorDescriptor_t {
        &mut self.handle
    }

    /// Constructs a wrapper from a raw FFI handle
    ///
    /// # Safety
    ///
    /// The caller must ensure the handle is valid and properly initialized.
    /// The wrapper will take ownership and call the destructor on drop.
    #[inline]
    pub unsafe fn from_raw(handle: ffi::cudnnTensorDescriptor_t) -> Self {
        Self { handle }
    }
}

impl Drop for CudnnTensorDescriptor {
    fn drop(&mut self) {
        unsafe {
            if !self.handle.is_null() {
                let _status = ffi::cudnnDestroyTensorDescriptor(self.handle);
                // Ignoring status in drop - can't propagate errors
            }
        }
    }
}


/// Safe wrapper for `cudnnCTCLossDescriptor_t`
///
/// This wrapper provides RAII-style resource management with automatic cleanup.
/// The wrapper is a zero-cost abstraction - it has the same memory layout
/// as the underlying handle type.
#[repr(transparent)]
pub struct CudnnCTCLossDescriptor {
    handle: ffi::cudnnCTCLossDescriptor_t,
}

impl CudnnCTCLossDescriptor {
    /// Create a new instance
    #[inline]
    /// Create a new instance
    // Warning: Unable to analyze function signature
    #[doc(hidden)]
    pub fn new() -> Result<Self, Error> {
        unsafe {
            // TODO: Implement proper constructor for cudnnCreateCTCLossDescriptor
            Err(Error::Unknown)
        }
    }


    /// Returns the raw FFI handle
    #[inline]
    pub fn as_raw(&self) -> ffi::cudnnCTCLossDescriptor_t {
        self.handle
    }

    /// Returns a mutable pointer to the raw FFI handle
    #[inline]
    pub fn as_raw_mut(&mut self) -> *mut ffi::cudnnCTCLossDescriptor_t {
        &mut self.handle
    }

    /// Constructs a wrapper from a raw FFI handle
    ///
    /// # Safety
    ///
    /// The caller must ensure the handle is valid and properly initialized.
    /// The wrapper will take ownership and call the destructor on drop.
    #[inline]
    pub unsafe fn from_raw(handle: ffi::cudnnCTCLossDescriptor_t) -> Self {
        Self { handle }
    }
}

impl Drop for CudnnCTCLossDescriptor {
    fn drop(&mut self) {
        unsafe {
            if !self.handle.is_null() {
                let _status = ffi::cudnnDestroyCTCLossDescriptor(self.handle);
                // Ignoring status in drop - can't propagate errors
            }
        }
    }
}


/// Safe wrapper for `cudnnConvolutionDescriptor_t`
///
/// This wrapper provides RAII-style resource management with automatic cleanup.
/// The wrapper is a zero-cost abstraction - it has the same memory layout
/// as the underlying handle type.
#[repr(transparent)]
pub struct CudnnConvolutionDescriptor {
    handle: ffi::cudnnConvolutionDescriptor_t,
}

impl CudnnConvolutionDescriptor {
    /// Create a new instance
    #[inline]
    /// Create a new instance
    // Warning: Unable to analyze function signature
    #[doc(hidden)]
    pub fn new() -> Result<Self, Error> {
        unsafe {
            // TODO: Implement proper constructor for cudnnCreateConvolutionDescriptor
            Err(Error::Unknown)
        }
    }


    /// Returns the raw FFI handle
    #[inline]
    pub fn as_raw(&self) -> ffi::cudnnConvolutionDescriptor_t {
        self.handle
    }

    /// Returns a mutable pointer to the raw FFI handle
    #[inline]
    pub fn as_raw_mut(&mut self) -> *mut ffi::cudnnConvolutionDescriptor_t {
        &mut self.handle
    }

    /// Constructs a wrapper from a raw FFI handle
    ///
    /// # Safety
    ///
    /// The caller must ensure the handle is valid and properly initialized.
    /// The wrapper will take ownership and call the destructor on drop.
    #[inline]
    pub unsafe fn from_raw(handle: ffi::cudnnConvolutionDescriptor_t) -> Self {
        Self { handle }
    }
}

impl Drop for CudnnConvolutionDescriptor {
    fn drop(&mut self) {
        unsafe {
            if !self.handle.is_null() {
                let _status = ffi::cudnnDestroyConvolutionDescriptor(self.handle);
                // Ignoring status in drop - can't propagate errors
            }
        }
    }
}


/// Safe wrapper for `cudnnHandle_t`
///
/// This wrapper provides RAII-style resource management with automatic cleanup.
/// The wrapper is a zero-cost abstraction - it has the same memory layout
/// as the underlying handle type.
#[repr(transparent)]
pub struct CudnnHandle {
    handle: ffi::cudnnHandle_t,
}

impl CudnnHandle {
    /// Create a new instance
    #[inline]
    /// Create a new instance
    // Warning: Unable to analyze function signature
    #[doc(hidden)]
    pub fn new() -> Result<Self, Error> {
        unsafe {
            // TODO: Implement proper constructor for cudnnCreate
            Err(Error::Unknown)
        }
    }


    /// Returns the raw FFI handle
    #[inline]
    pub fn as_raw(&self) -> ffi::cudnnHandle_t {
        self.handle
    }

    /// Returns a mutable pointer to the raw FFI handle
    #[inline]
    pub fn as_raw_mut(&mut self) -> *mut ffi::cudnnHandle_t {
        &mut self.handle
    }

    /// Constructs a wrapper from a raw FFI handle
    ///
    /// # Safety
    ///
    /// The caller must ensure the handle is valid and properly initialized.
    /// The wrapper will take ownership and call the destructor on drop.
    #[inline]
    pub unsafe fn from_raw(handle: ffi::cudnnHandle_t) -> Self {
        Self { handle }
    }
}

impl Drop for CudnnHandle {
    fn drop(&mut self) {
        unsafe {
            if !self.handle.is_null() {
                let _status = ffi::cudnnDestroy(self.handle);
                // Ignoring status in drop - can't propagate errors
            }
        }
    }
}


/// Safe wrapper for `cudnnAttnDescriptor_t`
///
/// This wrapper provides RAII-style resource management with automatic cleanup.
/// The wrapper is a zero-cost abstraction - it has the same memory layout
/// as the underlying handle type.
#[repr(transparent)]
pub struct CudnnAttnDescriptor {
    handle: ffi::cudnnAttnDescriptor_t,
}

impl CudnnAttnDescriptor {
    /// Create a new instance
    #[inline]
    /// Create a new instance
    // Warning: Unable to analyze function signature
    #[doc(hidden)]
    pub fn new() -> Result<Self, Error> {
        unsafe {
            // TODO: Implement proper constructor for cudnnCreateAttnDescriptor
            Err(Error::Unknown)
        }
    }


    /// Returns the raw FFI handle
    #[inline]
    pub fn as_raw(&self) -> ffi::cudnnAttnDescriptor_t {
        self.handle
    }

    /// Returns a mutable pointer to the raw FFI handle
    #[inline]
    pub fn as_raw_mut(&mut self) -> *mut ffi::cudnnAttnDescriptor_t {
        &mut self.handle
    }

    /// Constructs a wrapper from a raw FFI handle
    ///
    /// # Safety
    ///
    /// The caller must ensure the handle is valid and properly initialized.
    /// The wrapper will take ownership and call the destructor on drop.
    #[inline]
    pub unsafe fn from_raw(handle: ffi::cudnnAttnDescriptor_t) -> Self {
        Self { handle }
    }
}

impl Drop for CudnnAttnDescriptor {
    fn drop(&mut self) {
        unsafe {
            if !self.handle.is_null() {
                let _status = ffi::cudnnDestroyAttnDescriptor(self.handle);
                // Ignoring status in drop - can't propagate errors
            }
        }
    }
}


/// Safe wrapper for `cudaGraph_t`
///
/// This wrapper provides RAII-style resource management with automatic cleanup.
/// The wrapper is a zero-cost abstraction - it has the same memory layout
/// as the underlying handle type.
#[repr(transparent)]
pub struct CudaGraph {
    handle: ffi::cudaGraph_t,
}

impl CudaGraph {
    /// Create a new instance
    #[inline]
    /// Create a new instance
    // Warning: Unable to analyze function signature
    #[doc(hidden)]
    pub fn new() -> Result<Self, Error> {
        unsafe {
            // TODO: Implement proper constructor for cudaGraphCreate
            Err(Error::Unknown)
        }
    }


    /// Returns the raw FFI handle
    #[inline]
    pub fn as_raw(&self) -> ffi::cudaGraph_t {
        self.handle
    }

    /// Returns a mutable pointer to the raw FFI handle
    #[inline]
    pub fn as_raw_mut(&mut self) -> *mut ffi::cudaGraph_t {
        &mut self.handle
    }

    /// Constructs a wrapper from a raw FFI handle
    ///
    /// # Safety
    ///
    /// The caller must ensure the handle is valid and properly initialized.
    /// The wrapper will take ownership and call the destructor on drop.
    #[inline]
    pub unsafe fn from_raw(handle: ffi::cudaGraph_t) -> Self {
        Self { handle }
    }
}

impl Drop for CudaGraph {
    fn drop(&mut self) {
        unsafe {
            if !self.handle.is_null() {
                let _status = ffi::cudaGraphDestroy(self.handle);
                // Ignoring status in drop - can't propagate errors
            }
        }
    }
}


/// Safe wrapper for `cudnnFilterDescriptor_t`
///
/// This wrapper provides RAII-style resource management with automatic cleanup.
/// The wrapper is a zero-cost abstraction - it has the same memory layout
/// as the underlying handle type.
#[repr(transparent)]
pub struct CudnnFilterDescriptor {
    handle: ffi::cudnnFilterDescriptor_t,
}

impl CudnnFilterDescriptor {
    /// Create a new instance
    #[inline]
    /// Create a new instance
    // Warning: Unable to analyze function signature
    #[doc(hidden)]
    pub fn new() -> Result<Self, Error> {
        unsafe {
            // TODO: Implement proper constructor for cudnnCreateFilterDescriptor
            Err(Error::Unknown)
        }
    }


    /// Returns the raw FFI handle
    #[inline]
    pub fn as_raw(&self) -> ffi::cudnnFilterDescriptor_t {
        self.handle
    }

    /// Returns a mutable pointer to the raw FFI handle
    #[inline]
    pub fn as_raw_mut(&mut self) -> *mut ffi::cudnnFilterDescriptor_t {
        &mut self.handle
    }

    /// Constructs a wrapper from a raw FFI handle
    ///
    /// # Safety
    ///
    /// The caller must ensure the handle is valid and properly initialized.
    /// The wrapper will take ownership and call the destructor on drop.
    #[inline]
    pub unsafe fn from_raw(handle: ffi::cudnnFilterDescriptor_t) -> Self {
        Self { handle }
    }
}

impl Drop for CudnnFilterDescriptor {
    fn drop(&mut self) {
        unsafe {
            if !self.handle.is_null() {
                let _status = ffi::cudnnDestroyFilterDescriptor(self.handle);
                // Ignoring status in drop - can't propagate errors
            }
        }
    }
}


/// Safe wrapper for `cudnnRNNDescriptor_t`
///
/// This wrapper provides RAII-style resource management with automatic cleanup.
/// The wrapper is a zero-cost abstraction - it has the same memory layout
/// as the underlying handle type.
#[repr(transparent)]
pub struct CudnnRNNDescriptor {
    handle: ffi::cudnnRNNDescriptor_t,
}

impl CudnnRNNDescriptor {
    /// Create a new instance
    #[inline]
    /// Create a new instance
    // Warning: Unable to analyze function signature
    #[doc(hidden)]
    pub fn new() -> Result<Self, Error> {
        unsafe {
            // TODO: Implement proper constructor for cudnnCreateRNNDescriptor
            Err(Error::Unknown)
        }
    }


    /// Returns the raw FFI handle
    #[inline]
    pub fn as_raw(&self) -> ffi::cudnnRNNDescriptor_t {
        self.handle
    }

    /// Returns a mutable pointer to the raw FFI handle
    #[inline]
    pub fn as_raw_mut(&mut self) -> *mut ffi::cudnnRNNDescriptor_t {
        &mut self.handle
    }

    /// Constructs a wrapper from a raw FFI handle
    ///
    /// # Safety
    ///
    /// The caller must ensure the handle is valid and properly initialized.
    /// The wrapper will take ownership and call the destructor on drop.
    #[inline]
    pub unsafe fn from_raw(handle: ffi::cudnnRNNDescriptor_t) -> Self {
        Self { handle }
    }
}

impl Drop for CudnnRNNDescriptor {
    fn drop(&mut self) {
        unsafe {
            if !self.handle.is_null() {
                let _status = ffi::cudnnDestroyRNNDescriptor(self.handle);
                // Ignoring status in drop - can't propagate errors
            }
        }
    }
}


/// Safe wrapper for `cudnnPoolingDescriptor_t`
///
/// This wrapper provides RAII-style resource management with automatic cleanup.
/// The wrapper is a zero-cost abstraction - it has the same memory layout
/// as the underlying handle type.
#[repr(transparent)]
pub struct CudnnPoolingDescriptor {
    handle: ffi::cudnnPoolingDescriptor_t,
}

impl CudnnPoolingDescriptor {
    /// Create a new instance
    #[inline]
    /// Create a new instance
    // Warning: Unable to analyze function signature
    #[doc(hidden)]
    pub fn new() -> Result<Self, Error> {
        unsafe {
            // TODO: Implement proper constructor for cudnnCreatePoolingDescriptor
            Err(Error::Unknown)
        }
    }


    /// Returns the raw FFI handle
    #[inline]
    pub fn as_raw(&self) -> ffi::cudnnPoolingDescriptor_t {
        self.handle
    }

    /// Returns a mutable pointer to the raw FFI handle
    #[inline]
    pub fn as_raw_mut(&mut self) -> *mut ffi::cudnnPoolingDescriptor_t {
        &mut self.handle
    }

    /// Constructs a wrapper from a raw FFI handle
    ///
    /// # Safety
    ///
    /// The caller must ensure the handle is valid and properly initialized.
    /// The wrapper will take ownership and call the destructor on drop.
    #[inline]
    pub unsafe fn from_raw(handle: ffi::cudnnPoolingDescriptor_t) -> Self {
        Self { handle }
    }
}

impl Drop for CudnnPoolingDescriptor {
    fn drop(&mut self) {
        unsafe {
            if !self.handle.is_null() {
                let _status = ffi::cudnnDestroyPoolingDescriptor(self.handle);
                // Ignoring status in drop - can't propagate errors
            }
        }
    }
}


/// Safe wrapper for `cudnnOpTensorDescriptor_t`
///
/// This wrapper provides RAII-style resource management with automatic cleanup.
/// The wrapper is a zero-cost abstraction - it has the same memory layout
/// as the underlying handle type.
#[repr(transparent)]
pub struct CudnnOpTensorDescriptor {
    handle: ffi::cudnnOpTensorDescriptor_t,
}

impl CudnnOpTensorDescriptor {
    /// Create a new instance
    #[inline]
    /// Create a new instance
    // Warning: Unable to analyze function signature
    #[doc(hidden)]
    pub fn new() -> Result<Self, Error> {
        unsafe {
            // TODO: Implement proper constructor for cudnnCreateOpTensorDescriptor
            Err(Error::Unknown)
        }
    }


    /// Returns the raw FFI handle
    #[inline]
    pub fn as_raw(&self) -> ffi::cudnnOpTensorDescriptor_t {
        self.handle
    }

    /// Returns a mutable pointer to the raw FFI handle
    #[inline]
    pub fn as_raw_mut(&mut self) -> *mut ffi::cudnnOpTensorDescriptor_t {
        &mut self.handle
    }

    /// Constructs a wrapper from a raw FFI handle
    ///
    /// # Safety
    ///
    /// The caller must ensure the handle is valid and properly initialized.
    /// The wrapper will take ownership and call the destructor on drop.
    #[inline]
    pub unsafe fn from_raw(handle: ffi::cudnnOpTensorDescriptor_t) -> Self {
        Self { handle }
    }
}

impl Drop for CudnnOpTensorDescriptor {
    fn drop(&mut self) {
        unsafe {
            if !self.handle.is_null() {
                let _status = ffi::cudnnDestroyOpTensorDescriptor(self.handle);
                // Ignoring status in drop - can't propagate errors
            }
        }
    }
}


/// Safe wrapper for `cudnnActivationDescriptor_t`
///
/// This wrapper provides RAII-style resource management with automatic cleanup.
/// The wrapper is a zero-cost abstraction - it has the same memory layout
/// as the underlying handle type.
#[repr(transparent)]
pub struct CudnnActivationDescriptor {
    handle: ffi::cudnnActivationDescriptor_t,
}

impl CudnnActivationDescriptor {
    /// Create a new instance
    #[inline]
    /// Create a new instance
    // Warning: Unable to analyze function signature
    #[doc(hidden)]
    pub fn new() -> Result<Self, Error> {
        unsafe {
            // TODO: Implement proper constructor for cudnnCreateActivationDescriptor
            Err(Error::Unknown)
        }
    }


    /// Returns the raw FFI handle
    #[inline]
    pub fn as_raw(&self) -> ffi::cudnnActivationDescriptor_t {
        self.handle
    }

    /// Returns a mutable pointer to the raw FFI handle
    #[inline]
    pub fn as_raw_mut(&mut self) -> *mut ffi::cudnnActivationDescriptor_t {
        &mut self.handle
    }

    /// Constructs a wrapper from a raw FFI handle
    ///
    /// # Safety
    ///
    /// The caller must ensure the handle is valid and properly initialized.
    /// The wrapper will take ownership and call the destructor on drop.
    #[inline]
    pub unsafe fn from_raw(handle: ffi::cudnnActivationDescriptor_t) -> Self {
        Self { handle }
    }
}

impl Drop for CudnnActivationDescriptor {
    fn drop(&mut self) {
        unsafe {
            if !self.handle.is_null() {
                let _status = ffi::cudnnDestroyActivationDescriptor(self.handle);
                // Ignoring status in drop - can't propagate errors
            }
        }
    }
}


/// Safe wrapper for `cudnnSpatialTransformerDescriptor_t`
///
/// This wrapper provides RAII-style resource management with automatic cleanup.
/// The wrapper is a zero-cost abstraction - it has the same memory layout
/// as the underlying handle type.
#[repr(transparent)]
pub struct CudnnSpatialTransformerDescriptor {
    handle: ffi::cudnnSpatialTransformerDescriptor_t,
}

impl CudnnSpatialTransformerDescriptor {
    /// Create a new instance
    #[inline]
    /// Create a new instance
    // Warning: Unable to analyze function signature
    #[doc(hidden)]
    pub fn new() -> Result<Self, Error> {
        unsafe {
            // TODO: Implement proper constructor for cudnnCreateSpatialTransformerDescriptor
            Err(Error::Unknown)
        }
    }


    /// Returns the raw FFI handle
    #[inline]
    pub fn as_raw(&self) -> ffi::cudnnSpatialTransformerDescriptor_t {
        self.handle
    }

    /// Returns a mutable pointer to the raw FFI handle
    #[inline]
    pub fn as_raw_mut(&mut self) -> *mut ffi::cudnnSpatialTransformerDescriptor_t {
        &mut self.handle
    }

    /// Constructs a wrapper from a raw FFI handle
    ///
    /// # Safety
    ///
    /// The caller must ensure the handle is valid and properly initialized.
    /// The wrapper will take ownership and call the destructor on drop.
    #[inline]
    pub unsafe fn from_raw(handle: ffi::cudnnSpatialTransformerDescriptor_t) -> Self {
        Self { handle }
    }
}

impl Drop for CudnnSpatialTransformerDescriptor {
    fn drop(&mut self) {
        unsafe {
            if !self.handle.is_null() {
                let _status = ffi::cudnnDestroySpatialTransformerDescriptor(self.handle);
                // Ignoring status in drop - can't propagate errors
            }
        }
    }
}


/// Safe wrapper for `cudnnRNNDataDescriptor_t`
///
/// This wrapper provides RAII-style resource management with automatic cleanup.
/// The wrapper is a zero-cost abstraction - it has the same memory layout
/// as the underlying handle type.
#[repr(transparent)]
pub struct CudnnRNNDataDescriptor {
    handle: ffi::cudnnRNNDataDescriptor_t,
}

impl CudnnRNNDataDescriptor {
    /// Create a new instance
    #[inline]
    /// Create a new instance
    // Warning: Unable to analyze function signature
    #[doc(hidden)]
    pub fn new() -> Result<Self, Error> {
        unsafe {
            // TODO: Implement proper constructor for cudnnCreateRNNDataDescriptor
            Err(Error::Unknown)
        }
    }


    /// Returns the raw FFI handle
    #[inline]
    pub fn as_raw(&self) -> ffi::cudnnRNNDataDescriptor_t {
        self.handle
    }

    /// Returns a mutable pointer to the raw FFI handle
    #[inline]
    pub fn as_raw_mut(&mut self) -> *mut ffi::cudnnRNNDataDescriptor_t {
        &mut self.handle
    }

    /// Constructs a wrapper from a raw FFI handle
    ///
    /// # Safety
    ///
    /// The caller must ensure the handle is valid and properly initialized.
    /// The wrapper will take ownership and call the destructor on drop.
    #[inline]
    pub unsafe fn from_raw(handle: ffi::cudnnRNNDataDescriptor_t) -> Self {
        Self { handle }
    }
}

impl Drop for CudnnRNNDataDescriptor {
    fn drop(&mut self) {
        unsafe {
            if !self.handle.is_null() {
                let _status = ffi::cudnnDestroyRNNDataDescriptor(self.handle);
                // Ignoring status in drop - can't propagate errors
            }
        }
    }
}


/// Safe wrapper for `cudnnFusedOpsConstParamPack_t`
///
/// This wrapper provides RAII-style resource management with automatic cleanup.
/// The wrapper is a zero-cost abstraction - it has the same memory layout
/// as the underlying handle type.
#[repr(transparent)]
pub struct CudnnFusedOpsConstParamPack {
    handle: ffi::cudnnFusedOpsConstParamPack_t,
}

impl CudnnFusedOpsConstParamPack {
    /// Create a new instance
    #[inline]
    /// Create a new instance
    // Warning: Unable to analyze function signature
    #[doc(hidden)]
    pub fn new() -> Result<Self, Error> {
        unsafe {
            // TODO: Implement proper constructor for cudnnCreateFusedOpsConstParamPack
            Err(Error::Unknown)
        }
    }


    /// Returns the raw FFI handle
    #[inline]
    pub fn as_raw(&self) -> ffi::cudnnFusedOpsConstParamPack_t {
        self.handle
    }

    /// Returns a mutable pointer to the raw FFI handle
    #[inline]
    pub fn as_raw_mut(&mut self) -> *mut ffi::cudnnFusedOpsConstParamPack_t {
        &mut self.handle
    }

    /// Constructs a wrapper from a raw FFI handle
    ///
    /// # Safety
    ///
    /// The caller must ensure the handle is valid and properly initialized.
    /// The wrapper will take ownership and call the destructor on drop.
    #[inline]
    pub unsafe fn from_raw(handle: ffi::cudnnFusedOpsConstParamPack_t) -> Self {
        Self { handle }
    }
}

impl Drop for CudnnFusedOpsConstParamPack {
    fn drop(&mut self) {
        unsafe {
            if !self.handle.is_null() {
                let _status = ffi::cudnnDestroyFusedOpsConstParamPack(self.handle);
                // Ignoring status in drop - can't propagate errors
            }
        }
    }
}


/// Safe wrapper for `cudnnReduceTensorDescriptor_t`
///
/// This wrapper provides RAII-style resource management with automatic cleanup.
/// The wrapper is a zero-cost abstraction - it has the same memory layout
/// as the underlying handle type.
#[repr(transparent)]
pub struct CudnnReduceTensorDescriptor {
    handle: ffi::cudnnReduceTensorDescriptor_t,
}

impl CudnnReduceTensorDescriptor {
    /// Create a new instance
    #[inline]
    /// Create a new instance
    // Warning: Unable to analyze function signature
    #[doc(hidden)]
    pub fn new() -> Result<Self, Error> {
        unsafe {
            // TODO: Implement proper constructor for cudnnCreateReduceTensorDescriptor
            Err(Error::Unknown)
        }
    }


    /// Returns the raw FFI handle
    #[inline]
    pub fn as_raw(&self) -> ffi::cudnnReduceTensorDescriptor_t {
        self.handle
    }

    /// Returns a mutable pointer to the raw FFI handle
    #[inline]
    pub fn as_raw_mut(&mut self) -> *mut ffi::cudnnReduceTensorDescriptor_t {
        &mut self.handle
    }

    /// Constructs a wrapper from a raw FFI handle
    ///
    /// # Safety
    ///
    /// The caller must ensure the handle is valid and properly initialized.
    /// The wrapper will take ownership and call the destructor on drop.
    #[inline]
    pub unsafe fn from_raw(handle: ffi::cudnnReduceTensorDescriptor_t) -> Self {
        Self { handle }
    }
}

impl Drop for CudnnReduceTensorDescriptor {
    fn drop(&mut self) {
        unsafe {
            if !self.handle.is_null() {
                let _status = ffi::cudnnDestroyReduceTensorDescriptor(self.handle);
                // Ignoring status in drop - can't propagate errors
            }
        }
    }
}


/// Safe wrapper for `cudnnDropoutDescriptor_t`
///
/// This wrapper provides RAII-style resource management with automatic cleanup.
/// The wrapper is a zero-cost abstraction - it has the same memory layout
/// as the underlying handle type.
#[repr(transparent)]
pub struct CudnnDropoutDescriptor {
    handle: ffi::cudnnDropoutDescriptor_t,
}

impl CudnnDropoutDescriptor {
    /// Create a new instance
    #[inline]
    /// Create a new instance
    // Warning: Unable to analyze function signature
    #[doc(hidden)]
    pub fn new() -> Result<Self, Error> {
        unsafe {
            // TODO: Implement proper constructor for cudnnCreateDropoutDescriptor
            Err(Error::Unknown)
        }
    }


    /// Returns the raw FFI handle
    #[inline]
    pub fn as_raw(&self) -> ffi::cudnnDropoutDescriptor_t {
        self.handle
    }

    /// Returns a mutable pointer to the raw FFI handle
    #[inline]
    pub fn as_raw_mut(&mut self) -> *mut ffi::cudnnDropoutDescriptor_t {
        &mut self.handle
    }

    /// Constructs a wrapper from a raw FFI handle
    ///
    /// # Safety
    ///
    /// The caller must ensure the handle is valid and properly initialized.
    /// The wrapper will take ownership and call the destructor on drop.
    #[inline]
    pub unsafe fn from_raw(handle: ffi::cudnnDropoutDescriptor_t) -> Self {
        Self { handle }
    }
}

impl Drop for CudnnDropoutDescriptor {
    fn drop(&mut self) {
        unsafe {
            if !self.handle.is_null() {
                let _status = ffi::cudnnDestroyDropoutDescriptor(self.handle);
                // Ignoring status in drop - can't propagate errors
            }
        }
    }
}


/// Safe wrapper for `cudaEvent_t`
///
/// This wrapper provides RAII-style resource management with automatic cleanup.
/// The wrapper is a zero-cost abstraction - it has the same memory layout
/// as the underlying handle type.
#[repr(transparent)]
pub struct CudaEvent {
    handle: ffi::cudaEvent_t,
}

impl CudaEvent {
    /// Create a new instance
    #[inline]
    /// Create a new instance
    // Warning: Unable to analyze function signature
    #[doc(hidden)]
    pub fn new() -> Result<Self, Error> {
        unsafe {
            // TODO: Implement proper constructor for cudaIpcOpenEventHandle
            Err(Error::Unknown)
        }
    }


    /// Returns the raw FFI handle
    #[inline]
    pub fn as_raw(&self) -> ffi::cudaEvent_t {
        self.handle
    }

    /// Returns a mutable pointer to the raw FFI handle
    #[inline]
    pub fn as_raw_mut(&mut self) -> *mut ffi::cudaEvent_t {
        &mut self.handle
    }

    /// Constructs a wrapper from a raw FFI handle
    ///
    /// # Safety
    ///
    /// The caller must ensure the handle is valid and properly initialized.
    /// The wrapper will take ownership and call the destructor on drop.
    #[inline]
    pub unsafe fn from_raw(handle: ffi::cudaEvent_t) -> Self {
        Self { handle }
    }
}

impl Drop for CudaEvent {
    fn drop(&mut self) {
        unsafe {
            if !self.handle.is_null() {
                let _status = ffi::cudaEventDestroy(self.handle);
                // Ignoring status in drop - can't propagate errors
            }
        }
    }
}


/// Safe wrapper for `cudaMemPool_t`
///
/// This wrapper provides RAII-style resource management with automatic cleanup.
/// The wrapper is a zero-cost abstraction - it has the same memory layout
/// as the underlying handle type.
#[repr(transparent)]
pub struct CudaMemPool {
    handle: ffi::cudaMemPool_t,
}

impl CudaMemPool {
    /// Create a new instance
    #[inline]
    /// Create a new instance
    // Warning: Unable to analyze function signature
    #[doc(hidden)]
    pub fn new() -> Result<Self, Error> {
        unsafe {
            // TODO: Implement proper constructor for cudaMemPoolCreate
            Err(Error::Unknown)
        }
    }


    /// Returns the raw FFI handle
    #[inline]
    pub fn as_raw(&self) -> ffi::cudaMemPool_t {
        self.handle
    }

    /// Returns a mutable pointer to the raw FFI handle
    #[inline]
    pub fn as_raw_mut(&mut self) -> *mut ffi::cudaMemPool_t {
        &mut self.handle
    }

    /// Constructs a wrapper from a raw FFI handle
    ///
    /// # Safety
    ///
    /// The caller must ensure the handle is valid and properly initialized.
    /// The wrapper will take ownership and call the destructor on drop.
    #[inline]
    pub unsafe fn from_raw(handle: ffi::cudaMemPool_t) -> Self {
        Self { handle }
    }
}

impl Drop for CudaMemPool {
    fn drop(&mut self) {
        unsafe {
            if !self.handle.is_null() {
                let _status = ffi::cudaMemPoolDestroy(self.handle);
                // Ignoring status in drop - can't propagate errors
            }
        }
    }
}


/// Safe wrapper for `cudnnLRNDescriptor_t`
///
/// This wrapper provides RAII-style resource management with automatic cleanup.
/// The wrapper is a zero-cost abstraction - it has the same memory layout
/// as the underlying handle type.
#[repr(transparent)]
pub struct CudnnLRNDescriptor {
    handle: ffi::cudnnLRNDescriptor_t,
}

impl CudnnLRNDescriptor {
    /// Create a new instance
    #[inline]
    /// Create a new instance
    // Warning: Unable to analyze function signature
    #[doc(hidden)]
    pub fn new() -> Result<Self, Error> {
        unsafe {
            // TODO: Implement proper constructor for cudnnCreateLRNDescriptor
            Err(Error::Unknown)
        }
    }


    /// Returns the raw FFI handle
    #[inline]
    pub fn as_raw(&self) -> ffi::cudnnLRNDescriptor_t {
        self.handle
    }

    /// Returns a mutable pointer to the raw FFI handle
    #[inline]
    pub fn as_raw_mut(&mut self) -> *mut ffi::cudnnLRNDescriptor_t {
        &mut self.handle
    }

    /// Constructs a wrapper from a raw FFI handle
    ///
    /// # Safety
    ///
    /// The caller must ensure the handle is valid and properly initialized.
    /// The wrapper will take ownership and call the destructor on drop.
    #[inline]
    pub unsafe fn from_raw(handle: ffi::cudnnLRNDescriptor_t) -> Self {
        Self { handle }
    }
}

impl Drop for CudnnLRNDescriptor {
    fn drop(&mut self) {
        unsafe {
            if !self.handle.is_null() {
                let _status = ffi::cudnnDestroyLRNDescriptor(self.handle);
                // Ignoring status in drop - can't propagate errors
            }
        }
    }
}


/// Safe wrapper for `cudaUserObject_t`
///
/// This wrapper provides RAII-style resource management with automatic cleanup.
/// The wrapper is a zero-cost abstraction - it has the same memory layout
/// as the underlying handle type.
#[repr(transparent)]
pub struct CudaUserObject {
    handle: ffi::cudaUserObject_t,
}

impl CudaUserObject {
    /// Create a new instance
    #[inline]
    /// Create a new instance
    // Warning: Unable to analyze function signature
    #[doc(hidden)]
    pub fn new() -> Result<Self, Error> {
        unsafe {
            // TODO: Implement proper constructor for cudaUserObjectCreate
            Err(Error::Unknown)
        }
    }


    /// Returns the raw FFI handle
    #[inline]
    pub fn as_raw(&self) -> ffi::cudaUserObject_t {
        self.handle
    }

    /// Returns a mutable pointer to the raw FFI handle
    #[inline]
    pub fn as_raw_mut(&mut self) -> *mut ffi::cudaUserObject_t {
        &mut self.handle
    }

    /// Constructs a wrapper from a raw FFI handle
    ///
    /// # Safety
    ///
    /// The caller must ensure the handle is valid and properly initialized.
    /// The wrapper will take ownership and call the destructor on drop.
    #[inline]
    pub unsafe fn from_raw(handle: ffi::cudaUserObject_t) -> Self {
        Self { handle }
    }
}

impl Drop for CudaUserObject {
    fn drop(&mut self) {
        unsafe {
            if !self.handle.is_null() {
                let _status = ffi::cudaUserObjectRelease(self.handle, 1);
                // Ignoring status in drop - can't propagate errors
            }
        }
    }
}


/// Safe wrapper for `cudnnFusedOpsPlan_t`
///
/// This wrapper provides RAII-style resource management with automatic cleanup.
/// The wrapper is a zero-cost abstraction - it has the same memory layout
/// as the underlying handle type.
#[repr(transparent)]
pub struct CudnnFusedOpsPlan {
    handle: ffi::cudnnFusedOpsPlan_t,
}

impl CudnnFusedOpsPlan {
    /// Create a new instance
    #[inline]
    /// Create a new instance
    // Warning: Unable to analyze function signature
    #[doc(hidden)]
    pub fn new() -> Result<Self, Error> {
        unsafe {
            // TODO: Implement proper constructor for cudnnCreateFusedOpsPlan
            Err(Error::Unknown)
        }
    }


    /// Returns the raw FFI handle
    #[inline]
    pub fn as_raw(&self) -> ffi::cudnnFusedOpsPlan_t {
        self.handle
    }

    /// Returns a mutable pointer to the raw FFI handle
    #[inline]
    pub fn as_raw_mut(&mut self) -> *mut ffi::cudnnFusedOpsPlan_t {
        &mut self.handle
    }

    /// Constructs a wrapper from a raw FFI handle
    ///
    /// # Safety
    ///
    /// The caller must ensure the handle is valid and properly initialized.
    /// The wrapper will take ownership and call the destructor on drop.
    #[inline]
    pub unsafe fn from_raw(handle: ffi::cudnnFusedOpsPlan_t) -> Self {
        Self { handle }
    }
}

impl Drop for CudnnFusedOpsPlan {
    fn drop(&mut self) {
        unsafe {
            if !self.handle.is_null() {
                let _status = ffi::cudnnDestroyFusedOpsPlan(self.handle);
                // Ignoring status in drop - can't propagate errors
            }
        }
    }
}


/// Safe wrapper for `cudaStream_t`
///
/// This wrapper provides RAII-style resource management with automatic cleanup.
/// The wrapper is a zero-cost abstraction - it has the same memory layout
/// as the underlying handle type.
#[repr(transparent)]
pub struct CudaStream {
    handle: ffi::cudaStream_t,
}

impl CudaStream {
    /// Create a new instance
    #[inline]
    /// Create a new instance
    // Warning: Unable to analyze function signature
    #[doc(hidden)]
    pub fn new() -> Result<Self, Error> {
        unsafe {
            // TODO: Implement proper constructor for cudaStreamCreate
            Err(Error::Unknown)
        }
    }


    /// Returns the raw FFI handle
    #[inline]
    pub fn as_raw(&self) -> ffi::cudaStream_t {
        self.handle
    }

    /// Returns a mutable pointer to the raw FFI handle
    #[inline]
    pub fn as_raw_mut(&mut self) -> *mut ffi::cudaStream_t {
        &mut self.handle
    }

    /// Constructs a wrapper from a raw FFI handle
    ///
    /// # Safety
    ///
    /// The caller must ensure the handle is valid and properly initialized.
    /// The wrapper will take ownership and call the destructor on drop.
    #[inline]
    pub unsafe fn from_raw(handle: ffi::cudaStream_t) -> Self {
        Self { handle }
    }
}

impl Drop for CudaStream {
    fn drop(&mut self) {
        unsafe {
            if !self.handle.is_null() {
                let _status = ffi::cudaStreamDestroy(self.handle);
                // Ignoring status in drop - can't propagate errors
            }
        }
    }
}


/// Safe wrapper for `cudaMipmappedArray_t`
///
/// This wrapper provides RAII-style resource management with automatic cleanup.
/// The wrapper is a zero-cost abstraction - it has the same memory layout
/// as the underlying handle type.
#[repr(transparent)]
pub struct CudaMipmappedArray {
    handle: ffi::cudaMipmappedArray_t,
}

impl CudaMipmappedArray {
    /// Create a new instance
    #[inline]
    /// Create a new instance
    // Warning: Unable to analyze function signature
    #[doc(hidden)]
    pub fn new() -> Result<Self, Error> {
        unsafe {
            // TODO: Implement proper constructor for cudaMallocMipmappedArray
            Err(Error::Unknown)
        }
    }


    /// Returns the raw FFI handle
    #[inline]
    pub fn as_raw(&self) -> ffi::cudaMipmappedArray_t {
        self.handle
    }

    /// Returns a mutable pointer to the raw FFI handle
    #[inline]
    pub fn as_raw_mut(&mut self) -> *mut ffi::cudaMipmappedArray_t {
        &mut self.handle
    }

    /// Constructs a wrapper from a raw FFI handle
    ///
    /// # Safety
    ///
    /// The caller must ensure the handle is valid and properly initialized.
    /// The wrapper will take ownership and call the destructor on drop.
    #[inline]
    pub unsafe fn from_raw(handle: ffi::cudaMipmappedArray_t) -> Self {
        Self { handle }
    }
}

impl Drop for CudaMipmappedArray {
    fn drop(&mut self) {
        unsafe {
            if !self.handle.is_null() {
                let _status = ffi::cudaFreeMipmappedArray(self.handle);
                // Ignoring status in drop - can't propagate errors
            }
        }
    }
}


/// Safe wrapper for `cudaArray_t`
///
/// This wrapper provides RAII-style resource management with automatic cleanup.
/// The wrapper is a zero-cost abstraction - it has the same memory layout
/// as the underlying handle type.
#[repr(transparent)]
pub struct CudaArray {
    handle: ffi::cudaArray_t,
}

impl CudaArray {
    /// Create a new instance
    #[inline]
    /// Create a new instance
    // Warning: Unable to analyze function signature
    #[doc(hidden)]
    pub fn new() -> Result<Self, Error> {
        unsafe {
            // TODO: Implement proper constructor for cudaMallocArray
            Err(Error::Unknown)
        }
    }


    /// Returns the raw FFI handle
    #[inline]
    pub fn as_raw(&self) -> ffi::cudaArray_t {
        self.handle
    }

    /// Returns a mutable pointer to the raw FFI handle
    #[inline]
    pub fn as_raw_mut(&mut self) -> *mut ffi::cudaArray_t {
        &mut self.handle
    }

    /// Constructs a wrapper from a raw FFI handle
    ///
    /// # Safety
    ///
    /// The caller must ensure the handle is valid and properly initialized.
    /// The wrapper will take ownership and call the destructor on drop.
    #[inline]
    pub unsafe fn from_raw(handle: ffi::cudaArray_t) -> Self {
        Self { handle }
    }
}

impl Drop for CudaArray {
    fn drop(&mut self) {
        unsafe {
            if !self.handle.is_null() {
                let _status = ffi::cudaFreeArray(self.handle);
                // Ignoring status in drop - can't propagate errors
            }
        }
    }
}


/// Safe wrapper for `cudnnFusedOpsVariantParamPack_t`
///
/// This wrapper provides RAII-style resource management with automatic cleanup.
/// The wrapper is a zero-cost abstraction - it has the same memory layout
/// as the underlying handle type.
#[repr(transparent)]
pub struct CudnnFusedOpsVariantParamPack {
    handle: ffi::cudnnFusedOpsVariantParamPack_t,
}

impl CudnnFusedOpsVariantParamPack {
    /// Create a new instance
    #[inline]
    /// Create a new instance
    // Warning: Unable to analyze function signature
    #[doc(hidden)]
    pub fn new() -> Result<Self, Error> {
        unsafe {
            // TODO: Implement proper constructor for cudnnCreateFusedOpsVariantParamPack
            Err(Error::Unknown)
        }
    }


    /// Returns the raw FFI handle
    #[inline]
    pub fn as_raw(&self) -> ffi::cudnnFusedOpsVariantParamPack_t {
        self.handle
    }

    /// Returns a mutable pointer to the raw FFI handle
    #[inline]
    pub fn as_raw_mut(&mut self) -> *mut ffi::cudnnFusedOpsVariantParamPack_t {
        &mut self.handle
    }

    /// Constructs a wrapper from a raw FFI handle
    ///
    /// # Safety
    ///
    /// The caller must ensure the handle is valid and properly initialized.
    /// The wrapper will take ownership and call the destructor on drop.
    #[inline]
    pub unsafe fn from_raw(handle: ffi::cudnnFusedOpsVariantParamPack_t) -> Self {
        Self { handle }
    }
}

impl Drop for CudnnFusedOpsVariantParamPack {
    fn drop(&mut self) {
        unsafe {
            if !self.handle.is_null() {
                let _status = ffi::cudnnDestroyFusedOpsVariantParamPack(self.handle);
                // Ignoring status in drop - can't propagate errors
            }
        }
    }
}


/// Safe wrapper for `cudnnTensorTransformDescriptor_t`
///
/// This wrapper provides RAII-style resource management with automatic cleanup.
/// The wrapper is a zero-cost abstraction - it has the same memory layout
/// as the underlying handle type.
#[repr(transparent)]
pub struct CudnnTensorTransformDescriptor {
    handle: ffi::cudnnTensorTransformDescriptor_t,
}

impl CudnnTensorTransformDescriptor {
    /// Create a new instance
    #[inline]
    /// Create a new instance
    // Warning: Unable to analyze function signature
    #[doc(hidden)]
    pub fn new() -> Result<Self, Error> {
        unsafe {
            // TODO: Implement proper constructor for cudnnCreateTensorTransformDescriptor
            Err(Error::Unknown)
        }
    }


    /// Returns the raw FFI handle
    #[inline]
    pub fn as_raw(&self) -> ffi::cudnnTensorTransformDescriptor_t {
        self.handle
    }

    /// Returns a mutable pointer to the raw FFI handle
    #[inline]
    pub fn as_raw_mut(&mut self) -> *mut ffi::cudnnTensorTransformDescriptor_t {
        &mut self.handle
    }

    /// Constructs a wrapper from a raw FFI handle
    ///
    /// # Safety
    ///
    /// The caller must ensure the handle is valid and properly initialized.
    /// The wrapper will take ownership and call the destructor on drop.
    #[inline]
    pub unsafe fn from_raw(handle: ffi::cudnnTensorTransformDescriptor_t) -> Self {
        Self { handle }
    }
}

impl Drop for CudnnTensorTransformDescriptor {
    fn drop(&mut self) {
        unsafe {
            if !self.handle.is_null() {
                let _status = ffi::cudnnDestroyTensorTransformDescriptor(self.handle);
                // Ignoring status in drop - can't propagate errors
            }
        }
    }
}


/// Wrapper for `cudaKernel_t`
/// Note: No automatic resource management - handle cleanup manually
///
/// This wrapper is a zero-cost abstraction with the same memory layout
/// as the underlying handle type.
#[repr(transparent)]
pub struct CudaKernel {
    handle: ffi::cudaKernel_t,
}

impl CudaKernel {
    /// Returns the raw FFI handle
    #[inline]
    pub fn as_raw(&self) -> ffi::cudaKernel_t {
        self.handle
    }

    /// Returns a mutable pointer to the raw FFI handle
    #[inline]
    pub fn as_raw_mut(&mut self) -> *mut ffi::cudaKernel_t {
        &mut self.handle
    }

    /// Constructs a wrapper from a raw FFI handle
    ///
    /// # Safety
    ///
    /// The caller must ensure the handle is valid and properly initialized.
    #[inline]
    pub unsafe fn from_raw(handle: ffi::cudaKernel_t) -> Self {
        Self { handle }
    }
}

/// Wrapper for `cudaGraphNode_t`
/// Note: No automatic resource management - handle cleanup manually
///
/// This wrapper is a zero-cost abstraction with the same memory layout
/// as the underlying handle type.
#[repr(transparent)]
pub struct CudaGraphNode {
    handle: ffi::cudaGraphNode_t,
}

impl CudaGraphNode {
    /// Returns the raw FFI handle
    #[inline]
    pub fn as_raw(&self) -> ffi::cudaGraphNode_t {
        self.handle
    }

    /// Returns a mutable pointer to the raw FFI handle
    #[inline]
    pub fn as_raw_mut(&mut self) -> *mut ffi::cudaGraphNode_t {
        &mut self.handle
    }

    /// Constructs a wrapper from a raw FFI handle
    ///
    /// # Safety
    ///
    /// The caller must ensure the handle is valid and properly initialized.
    #[inline]
    pub unsafe fn from_raw(handle: ffi::cudaGraphNode_t) -> Self {
        Self { handle }
    }
}

/// Wrapper for `cudaStreamCallback_t`
/// Note: No automatic resource management - handle cleanup manually
///
/// This wrapper is a zero-cost abstraction with the same memory layout
/// as the underlying handle type.
#[repr(transparent)]
pub struct CudaStreamCallback {
    handle: ffi::cudaStreamCallback_t,
}

impl CudaStreamCallback {
    /// Returns the raw FFI handle
    #[inline]
    pub fn as_raw(&self) -> ffi::cudaStreamCallback_t {
        self.handle
    }

    /// Returns a mutable pointer to the raw FFI handle
    #[inline]
    pub fn as_raw_mut(&mut self) -> *mut ffi::cudaStreamCallback_t {
        &mut self.handle
    }

    /// Constructs a wrapper from a raw FFI handle
    ///
    /// # Safety
    ///
    /// The caller must ensure the handle is valid and properly initialized.
    #[inline]
    pub unsafe fn from_raw(handle: ffi::cudaStreamCallback_t) -> Self {
        Self { handle }
    }
}

/// Wrapper for `cudaAsyncCallback`
/// Note: No automatic resource management - handle cleanup manually
///
/// This wrapper is a zero-cost abstraction with the same memory layout
/// as the underlying handle type.
#[repr(transparent)]
pub struct CudaAsyncCallback {
    handle: ffi::cudaAsyncCallback,
}

impl CudaAsyncCallback {
    /// Returns the raw FFI handle
    #[inline]
    pub fn as_raw(&self) -> ffi::cudaAsyncCallback {
        self.handle
    }

    /// Returns a mutable pointer to the raw FFI handle
    #[inline]
    pub fn as_raw_mut(&mut self) -> *mut ffi::cudaAsyncCallback {
        &mut self.handle
    }

    /// Constructs a wrapper from a raw FFI handle
    ///
    /// # Safety
    ///
    /// The caller must ensure the handle is valid and properly initialized.
    #[inline]
    pub unsafe fn from_raw(handle: ffi::cudaAsyncCallback) -> Self {
        Self { handle }
    }
}

/// Wrapper for `cudaLogsCallbackHandle`
/// Note: No automatic resource management - handle cleanup manually
///
/// This wrapper is a zero-cost abstraction with the same memory layout
/// as the underlying handle type.
#[repr(transparent)]
pub struct CudaLogsCallbackHandle {
    handle: ffi::cudaLogsCallbackHandle,
}

impl CudaLogsCallbackHandle {
    /// Returns the raw FFI handle
    #[inline]
    pub fn as_raw(&self) -> ffi::cudaLogsCallbackHandle {
        self.handle
    }

    /// Returns a mutable pointer to the raw FFI handle
    #[inline]
    pub fn as_raw_mut(&mut self) -> *mut ffi::cudaLogsCallbackHandle {
        &mut self.handle
    }

    /// Constructs a wrapper from a raw FFI handle
    ///
    /// # Safety
    ///
    /// The caller must ensure the handle is valid and properly initialized.
    #[inline]
    pub unsafe fn from_raw(handle: ffi::cudaLogsCallbackHandle) -> Self {
        Self { handle }
    }
}

/// Wrapper for `cudaGraphDeviceNode_t`
/// Note: No automatic resource management - handle cleanup manually
///
/// This wrapper is a zero-cost abstraction with the same memory layout
/// as the underlying handle type.
#[repr(transparent)]
pub struct CudaGraphDeviceNode {
    handle: ffi::cudaGraphDeviceNode_t,
}

impl CudaGraphDeviceNode {
    /// Returns the raw FFI handle
    #[inline]
    pub fn as_raw(&self) -> ffi::cudaGraphDeviceNode_t {
        self.handle
    }

    /// Returns a mutable pointer to the raw FFI handle
    #[inline]
    pub fn as_raw_mut(&mut self) -> *mut ffi::cudaGraphDeviceNode_t {
        &mut self.handle
    }

    /// Constructs a wrapper from a raw FFI handle
    ///
    /// # Safety
    ///
    /// The caller must ensure the handle is valid and properly initialized.
    #[inline]
    pub unsafe fn from_raw(handle: ffi::cudaGraphDeviceNode_t) -> Self {
        Self { handle }
    }
}

/// Wrapper for `cudaMipmappedArray_const_t`
/// Note: No automatic resource management - handle cleanup manually
///
/// This wrapper is a zero-cost abstraction with the same memory layout
/// as the underlying handle type.
#[repr(transparent)]
pub struct CudaMipmappedArrayConst {
    handle: ffi::cudaMipmappedArray_const_t,
}

impl CudaMipmappedArrayConst {
    /// Returns the raw FFI handle
    #[inline]
    pub fn as_raw(&self) -> ffi::cudaMipmappedArray_const_t {
        self.handle
    }

    /// Returns a mutable pointer to the raw FFI handle
    #[inline]
    pub fn as_raw_mut(&mut self) -> *mut ffi::cudaMipmappedArray_const_t {
        &mut self.handle
    }

    /// Constructs a wrapper from a raw FFI handle
    ///
    /// # Safety
    ///
    /// The caller must ensure the handle is valid and properly initialized.
    #[inline]
    pub unsafe fn from_raw(handle: ffi::cudaMipmappedArray_const_t) -> Self {
        Self { handle }
    }
}

/// Wrapper for `cudnnBackendDescriptor_t`
/// Note: No automatic resource management - handle cleanup manually
///
/// This wrapper is a zero-cost abstraction with the same memory layout
/// as the underlying handle type.
#[repr(transparent)]
pub struct CudnnBackendDescriptor {
    handle: ffi::cudnnBackendDescriptor_t,
}

impl CudnnBackendDescriptor {
    /// Returns the raw FFI handle
    #[inline]
    pub fn as_raw(&self) -> ffi::cudnnBackendDescriptor_t {
        self.handle
    }

    /// Returns a mutable pointer to the raw FFI handle
    #[inline]
    pub fn as_raw_mut(&mut self) -> *mut ffi::cudnnBackendDescriptor_t {
        &mut self.handle
    }

    /// Constructs a wrapper from a raw FFI handle
    ///
    /// # Safety
    ///
    /// The caller must ensure the handle is valid and properly initialized.
    #[inline]
    pub unsafe fn from_raw(handle: ffi::cudnnBackendDescriptor_t) -> Self {
        Self { handle }
    }
}

/// Wrapper for `cudaGraphExec_t`
/// Note: No automatic resource management - handle cleanup manually
///
/// This wrapper is a zero-cost abstraction with the same memory layout
/// as the underlying handle type.
#[repr(transparent)]
pub struct CudaGraphExec {
    handle: ffi::cudaGraphExec_t,
}

impl CudaGraphExec {
    /// Returns the raw FFI handle
    #[inline]
    pub fn as_raw(&self) -> ffi::cudaGraphExec_t {
        self.handle
    }

    /// Returns a mutable pointer to the raw FFI handle
    #[inline]
    pub fn as_raw_mut(&mut self) -> *mut ffi::cudaGraphExec_t {
        &mut self.handle
    }

    /// Constructs a wrapper from a raw FFI handle
    ///
    /// # Safety
    ///
    /// The caller must ensure the handle is valid and properly initialized.
    #[inline]
    pub unsafe fn from_raw(handle: ffi::cudaGraphExec_t) -> Self {
        Self { handle }
    }
}

/// Wrapper for `cudaExternalMemory_t`
/// Note: No automatic resource management - handle cleanup manually
///
/// This wrapper is a zero-cost abstraction with the same memory layout
/// as the underlying handle type.
#[repr(transparent)]
pub struct CudaExternalMemory {
    handle: ffi::cudaExternalMemory_t,
}

impl CudaExternalMemory {
    /// Returns the raw FFI handle
    #[inline]
    pub fn as_raw(&self) -> ffi::cudaExternalMemory_t {
        self.handle
    }

    /// Returns a mutable pointer to the raw FFI handle
    #[inline]
    pub fn as_raw_mut(&mut self) -> *mut ffi::cudaExternalMemory_t {
        &mut self.handle
    }

    /// Constructs a wrapper from a raw FFI handle
    ///
    /// # Safety
    ///
    /// The caller must ensure the handle is valid and properly initialized.
    #[inline]
    pub unsafe fn from_raw(handle: ffi::cudaExternalMemory_t) -> Self {
        Self { handle }
    }
}

/// Wrapper for `cudaHostFn_t`
/// Note: No automatic resource management - handle cleanup manually
///
/// This wrapper is a zero-cost abstraction with the same memory layout
/// as the underlying handle type.
#[repr(transparent)]
pub struct CudaHostFn {
    handle: ffi::cudaHostFn_t,
}

impl CudaHostFn {
    /// Returns the raw FFI handle
    #[inline]
    pub fn as_raw(&self) -> ffi::cudaHostFn_t {
        self.handle
    }

    /// Returns a mutable pointer to the raw FFI handle
    #[inline]
    pub fn as_raw_mut(&mut self) -> *mut ffi::cudaHostFn_t {
        &mut self.handle
    }

    /// Constructs a wrapper from a raw FFI handle
    ///
    /// # Safety
    ///
    /// The caller must ensure the handle is valid and properly initialized.
    #[inline]
    pub unsafe fn from_raw(handle: ffi::cudaHostFn_t) -> Self {
        Self { handle }
    }
}

/// Wrapper for `cudaExternalSemaphore_t`
/// Note: No automatic resource management - handle cleanup manually
///
/// This wrapper is a zero-cost abstraction with the same memory layout
/// as the underlying handle type.
#[repr(transparent)]
pub struct CudaExternalSemaphore {
    handle: ffi::cudaExternalSemaphore_t,
}

impl CudaExternalSemaphore {
    /// Returns the raw FFI handle
    #[inline]
    pub fn as_raw(&self) -> ffi::cudaExternalSemaphore_t {
        self.handle
    }

    /// Returns a mutable pointer to the raw FFI handle
    #[inline]
    pub fn as_raw_mut(&mut self) -> *mut ffi::cudaExternalSemaphore_t {
        &mut self.handle
    }

    /// Constructs a wrapper from a raw FFI handle
    ///
    /// # Safety
    ///
    /// The caller must ensure the handle is valid and properly initialized.
    #[inline]
    pub unsafe fn from_raw(handle: ffi::cudaExternalSemaphore_t) -> Self {
        Self { handle }
    }
}

/// Wrapper for `cudaRoundMode`
/// Note: No automatic resource management - handle cleanup manually
///
/// This wrapper is a zero-cost abstraction with the same memory layout
/// as the underlying handle type.
#[repr(transparent)]
pub struct CudaRoundMode {
    handle: ffi::cudaRoundMode,
}

impl CudaRoundMode {
    /// Returns the raw FFI handle
    #[inline]
    pub fn as_raw(&self) -> ffi::cudaRoundMode {
        self.handle
    }

    /// Returns a mutable pointer to the raw FFI handle
    #[inline]
    pub fn as_raw_mut(&mut self) -> *mut ffi::cudaRoundMode {
        &mut self.handle
    }

    /// Constructs a wrapper from a raw FFI handle
    ///
    /// # Safety
    ///
    /// The caller must ensure the handle is valid and properly initialized.
    #[inline]
    pub unsafe fn from_raw(handle: ffi::cudaRoundMode) -> Self {
        Self { handle }
    }
}

/// Wrapper for `cudaLogsCallback_t`
/// Note: No automatic resource management - handle cleanup manually
///
/// This wrapper is a zero-cost abstraction with the same memory layout
/// as the underlying handle type.
#[repr(transparent)]
pub struct CudaLogsCallback {
    handle: ffi::cudaLogsCallback_t,
}

impl CudaLogsCallback {
    /// Returns the raw FFI handle
    #[inline]
    pub fn as_raw(&self) -> ffi::cudaLogsCallback_t {
        self.handle
    }

    /// Returns a mutable pointer to the raw FFI handle
    #[inline]
    pub fn as_raw_mut(&mut self) -> *mut ffi::cudaLogsCallback_t {
        &mut self.handle
    }

    /// Constructs a wrapper from a raw FFI handle
    ///
    /// # Safety
    ///
    /// The caller must ensure the handle is valid and properly initialized.
    #[inline]
    pub unsafe fn from_raw(handle: ffi::cudaLogsCallback_t) -> Self {
        Self { handle }
    }
}

/// Wrapper for `cudaArray_const_t`
/// Note: No automatic resource management - handle cleanup manually
///
/// This wrapper is a zero-cost abstraction with the same memory layout
/// as the underlying handle type.
#[repr(transparent)]
pub struct CudaArrayConst {
    handle: ffi::cudaArray_const_t,
}

impl CudaArrayConst {
    /// Returns the raw FFI handle
    #[inline]
    pub fn as_raw(&self) -> ffi::cudaArray_const_t {
        self.handle
    }

    /// Returns a mutable pointer to the raw FFI handle
    #[inline]
    pub fn as_raw_mut(&mut self) -> *mut ffi::cudaArray_const_t {
        &mut self.handle
    }

    /// Constructs a wrapper from a raw FFI handle
    ///
    /// # Safety
    ///
    /// The caller must ensure the handle is valid and properly initialized.
    #[inline]
    pub unsafe fn from_raw(handle: ffi::cudaArray_const_t) -> Self {
        Self { handle }
    }
}

/// Wrapper for `cudnnCallback_t`
/// Note: No automatic resource management - handle cleanup manually
///
/// This wrapper is a zero-cost abstraction with the same memory layout
/// as the underlying handle type.
#[repr(transparent)]
pub struct CudnnCallback {
    handle: ffi::cudnnCallback_t,
}

impl CudnnCallback {
    /// Returns the raw FFI handle
    #[inline]
    pub fn as_raw(&self) -> ffi::cudnnCallback_t {
        self.handle
    }

    /// Returns a mutable pointer to the raw FFI handle
    #[inline]
    pub fn as_raw_mut(&mut self) -> *mut ffi::cudnnCallback_t {
        &mut self.handle
    }

    /// Constructs a wrapper from a raw FFI handle
    ///
    /// # Safety
    ///
    /// The caller must ensure the handle is valid and properly initialized.
    #[inline]
    pub unsafe fn from_raw(handle: ffi::cudnnCallback_t) -> Self {
        Self { handle }
    }
}

/// Wrapper for `cudaLibrary_t`
/// Note: No automatic resource management - handle cleanup manually
///
/// This wrapper is a zero-cost abstraction with the same memory layout
/// as the underlying handle type.
#[repr(transparent)]
pub struct CudaLibrary {
    handle: ffi::cudaLibrary_t,
}

impl CudaLibrary {
    /// Returns the raw FFI handle
    #[inline]
    pub fn as_raw(&self) -> ffi::cudaLibrary_t {
        self.handle
    }

    /// Returns a mutable pointer to the raw FFI handle
    #[inline]
    pub fn as_raw_mut(&mut self) -> *mut ffi::cudaLibrary_t {
        &mut self.handle
    }

    /// Constructs a wrapper from a raw FFI handle
    ///
    /// # Safety
    ///
    /// The caller must ensure the handle is valid and properly initialized.
    #[inline]
    pub unsafe fn from_raw(handle: ffi::cudaLibrary_t) -> Self {
        Self { handle }
    }
}

/// Wrapper for `va_list`
/// Note: No automatic resource management - handle cleanup manually
///
/// This wrapper is a zero-cost abstraction with the same memory layout
/// as the underlying handle type.
#[repr(transparent)]
pub struct VaList {
    handle: ffi::va_list,
}

impl VaList {
    /// Returns the raw FFI handle
    #[inline]
    pub fn as_raw(&self) -> ffi::va_list {
        self.handle
    }

    /// Returns a mutable pointer to the raw FFI handle
    #[inline]
    pub fn as_raw_mut(&mut self) -> *mut ffi::va_list {
        &mut self.handle
    }

    /// Constructs a wrapper from a raw FFI handle
    ///
    /// # Safety
    ///
    /// The caller must ensure the handle is valid and properly initialized.
    #[inline]
    pub unsafe fn from_raw(handle: ffi::va_list) -> Self {
        Self { handle }
    }
}

/// Wrapper for `cudnnLossNormalizationMode_t`
/// Note: No automatic resource management - handle cleanup manually
///
/// This wrapper is a zero-cost abstraction with the same memory layout
/// as the underlying handle type.
#[repr(transparent)]
pub struct CudnnLossNormalizationMode {
    handle: ffi::cudnnLossNormalizationMode_t,
}

impl CudnnLossNormalizationMode {
    /// Returns the raw FFI handle
    #[inline]
    pub fn as_raw(&self) -> ffi::cudnnLossNormalizationMode_t {
        self.handle
    }

    /// Returns a mutable pointer to the raw FFI handle
    #[inline]
    pub fn as_raw_mut(&mut self) -> *mut ffi::cudnnLossNormalizationMode_t {
        &mut self.handle
    }

    /// Constructs a wrapper from a raw FFI handle
    ///
    /// # Safety
    ///
    /// The caller must ensure the handle is valid and properly initialized.
    #[inline]
    pub unsafe fn from_raw(handle: ffi::cudnnLossNormalizationMode_t) -> Self {
        Self { handle }
    }
}

/// Wrapper for `cudaAsyncCallbackHandle_t`
/// Note: No automatic resource management - handle cleanup manually
///
/// This wrapper is a zero-cost abstraction with the same memory layout
/// as the underlying handle type.
#[repr(transparent)]
pub struct CudaAsyncCallbackHandle {
    handle: ffi::cudaAsyncCallbackHandle_t,
}

impl CudaAsyncCallbackHandle {
    /// Returns the raw FFI handle
    #[inline]
    pub fn as_raw(&self) -> ffi::cudaAsyncCallbackHandle_t {
        self.handle
    }

    /// Returns a mutable pointer to the raw FFI handle
    #[inline]
    pub fn as_raw_mut(&mut self) -> *mut ffi::cudaAsyncCallbackHandle_t {
        &mut self.handle
    }

    /// Constructs a wrapper from a raw FFI handle
    ///
    /// # Safety
    ///
    /// The caller must ensure the handle is valid and properly initialized.
    #[inline]
    pub unsafe fn from_raw(handle: ffi::cudaAsyncCallbackHandle_t) -> Self {
        Self { handle }
    }
}

/// Wrapper for `cudaGraphicsResource_t`
/// Note: No automatic resource management - handle cleanup manually
///
/// This wrapper is a zero-cost abstraction with the same memory layout
/// as the underlying handle type.
#[repr(transparent)]
pub struct CudaGraphicsResource {
    handle: ffi::cudaGraphicsResource_t,
}

impl CudaGraphicsResource {
    /// Returns the raw FFI handle
    #[inline]
    pub fn as_raw(&self) -> ffi::cudaGraphicsResource_t {
        self.handle
    }

    /// Returns a mutable pointer to the raw FFI handle
    #[inline]
    pub fn as_raw_mut(&mut self) -> *mut ffi::cudaGraphicsResource_t {
        &mut self.handle
    }

    /// Constructs a wrapper from a raw FFI handle
    ///
    /// # Safety
    ///
    /// The caller must ensure the handle is valid and properly initialized.
    #[inline]
    pub unsafe fn from_raw(handle: ffi::cudaGraphicsResource_t) -> Self {
        Self { handle }
    }
}

/// Wrapper for `cudaFunction_t`
/// Note: No automatic resource management - handle cleanup manually
///
/// This wrapper is a zero-cost abstraction with the same memory layout
/// as the underlying handle type.
#[repr(transparent)]
pub struct CudaFunction {
    handle: ffi::cudaFunction_t,
}

impl CudaFunction {
    /// Returns the raw FFI handle
    #[inline]
    pub fn as_raw(&self) -> ffi::cudaFunction_t {
        self.handle
    }

    /// Returns a mutable pointer to the raw FFI handle
    #[inline]
    pub fn as_raw_mut(&mut self) -> *mut ffi::cudaFunction_t {
        &mut self.handle
    }

    /// Constructs a wrapper from a raw FFI handle
    ///
    /// # Safety
    ///
    /// The caller must ensure the handle is valid and properly initialized.
    #[inline]
    pub unsafe fn from_raw(handle: ffi::cudaFunction_t) -> Self {
        Self { handle }
    }
}

// Additional methods for cudaKernel_t
impl CudaKernel {
    #[inline]
    pub fn kernel_set_attribute_for_device(
        &mut self,
        attr: ffi::cudaFuncAttribute,
        value: :: core :: ffi :: c_int,
        device: :: core :: ffi :: c_int,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudaKernelSetAttributeForDevice").entered();
            let result = ffi::cudaKernelSetAttributeForDevice(self.handle, attr, value, device);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

}

// Additional methods for cudaGraphNode_t
impl CudaGraphNode {
pub fn stream_begin_capture_to_graph(
    stream: ffi::cudaStream_t,
    graph: ffi::cudaGraph_t,
    dependencies: *constffi::cudaGraphNode_t,
    dependencydata: *constffi::cudaGraphEdgeData,
    numdependencies: usize,
    mode: ffi::cudaStreamCaptureMode
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaStreamBeginCaptureToGraph(stream, graph, dependencies, dependencydata, numdependencies, mode);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn stream_get_capture_info(
    stream: ffi::cudaStream_t,
    capturestatus_out: *mutffi::cudaStreamCaptureStatus,
    id_out: *mut :: core :: ffi ::c_ulonglong,
    graph_out: *mutffi::cudaGraph_t,
    dependencies_out: *mut *constffi::cudaGraphNode_t,
    edgedata_out: *mut *constffi::cudaGraphEdgeData,
    numdependencies_out: *mutusize
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaStreamGetCaptureInfo(stream, capturestatus_out, id_out, graph_out, dependencies_out, edgedata_out, numdependencies_out);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn stream_update_capture_dependencies(
    stream: ffi::cudaStream_t,
    dependencies: *mutffi::cudaGraphNode_t,
    dependencydata: *constffi::cudaGraphEdgeData,
    numdependencies: usize,
    flags: :: core :: ffi :: c_uint
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaStreamUpdateCaptureDependencies(stream, dependencies, dependencydata, numdependencies, flags);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

    #[inline]
    pub fn graph_kernel_node_get_params(
        &mut self,
        pnodeparams: *mutffi::cudaKernelNodeParams,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if pnodeparams.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if pnodeparams.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudaGraphKernelNodeGetParams").entered();
            let result = ffi::cudaGraphKernelNodeGetParams(self.handle, pnodeparams);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn graph_kernel_node_set_params(
        &mut self,
        pnodeparams: *constffi::cudaKernelNodeParams,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if pnodeparams.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if pnodeparams.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudaGraphKernelNodeSetParams").entered();
            let result = ffi::cudaGraphKernelNodeSetParams(self.handle, pnodeparams);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn graph_kernel_node_copy_attributes(
        &mut self,
        hsrc: ffi::cudaGraphNode_t,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudaGraphKernelNodeCopyAttributes").entered();
            let result = ffi::cudaGraphKernelNodeCopyAttributes(self.handle, hsrc);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn graph_kernel_node_get_attribute(
        &mut self,
        attr: ffi::cudaLaunchAttributeID,
        value_out: *mutffi::cudaLaunchAttributeValue,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if value_out.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if value_out.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudaGraphKernelNodeGetAttribute").entered();
            let result = ffi::cudaGraphKernelNodeGetAttribute(self.handle, attr, value_out);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn graph_kernel_node_set_attribute(
        &mut self,
        attr: ffi::cudaLaunchAttributeID,
        value: *constffi::cudaLaunchAttributeValue,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if value.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if value.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudaGraphKernelNodeSetAttribute").entered();
            let result = ffi::cudaGraphKernelNodeSetAttribute(self.handle, attr, value);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn graph_memcpy_node_get_params(
        &mut self,
        pnodeparams: *mutffi::cudaMemcpy3DParms,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if pnodeparams.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if pnodeparams.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudaGraphMemcpyNodeGetParams").entered();
            let result = ffi::cudaGraphMemcpyNodeGetParams(self.handle, pnodeparams);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn graph_memcpy_node_set_params(
        &mut self,
        pnodeparams: *constffi::cudaMemcpy3DParms,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if pnodeparams.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if pnodeparams.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudaGraphMemcpyNodeSetParams").entered();
            let result = ffi::cudaGraphMemcpyNodeSetParams(self.handle, pnodeparams);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn graph_memcpy_node_set_params_to_symbol(
        &mut self,
        symbol: *const :: core :: ffi ::c_void,
        src: *const :: core :: ffi ::c_void,
        count: usize,
        offset: usize,
        kind: ffi::cudaMemcpyKind,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if symbol.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if symbol.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if src.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if src.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudaGraphMemcpyNodeSetParamsToSymbol").entered();
            let result = ffi::cudaGraphMemcpyNodeSetParamsToSymbol(self.handle, symbol, src, count, offset, kind);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn graph_memcpy_node_set_params_from_symbol(
        &mut self,
        dst: *mut :: core :: ffi ::c_void,
        symbol: *const :: core :: ffi ::c_void,
        count: usize,
        offset: usize,
        kind: ffi::cudaMemcpyKind,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if dst.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if dst.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if symbol.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if symbol.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudaGraphMemcpyNodeSetParamsFromSymbol").entered();
            let result = ffi::cudaGraphMemcpyNodeSetParamsFromSymbol(self.handle, dst, symbol, count, offset, kind);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn graph_memcpy_node_set_params_1d(
        &mut self,
        dst: *mut :: core :: ffi ::c_void,
        src: *const :: core :: ffi ::c_void,
        count: usize,
        kind: ffi::cudaMemcpyKind,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if dst.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if dst.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if src.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if src.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudaGraphMemcpyNodeSetParams1D").entered();
            let result = ffi::cudaGraphMemcpyNodeSetParams1D(self.handle, dst, src, count, kind);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn graph_memset_node_get_params(
        &mut self,
        pnodeparams: *mutffi::cudaMemsetParams,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if pnodeparams.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if pnodeparams.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudaGraphMemsetNodeGetParams").entered();
            let result = ffi::cudaGraphMemsetNodeGetParams(self.handle, pnodeparams);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn graph_memset_node_set_params(
        &mut self,
        pnodeparams: *constffi::cudaMemsetParams,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if pnodeparams.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if pnodeparams.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudaGraphMemsetNodeSetParams").entered();
            let result = ffi::cudaGraphMemsetNodeSetParams(self.handle, pnodeparams);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn graph_host_node_get_params(
        &mut self,
        pnodeparams: *mutffi::cudaHostNodeParams,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if pnodeparams.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if pnodeparams.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudaGraphHostNodeGetParams").entered();
            let result = ffi::cudaGraphHostNodeGetParams(self.handle, pnodeparams);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn graph_host_node_set_params(
        &mut self,
        pnodeparams: *constffi::cudaHostNodeParams,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if pnodeparams.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if pnodeparams.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudaGraphHostNodeSetParams").entered();
            let result = ffi::cudaGraphHostNodeSetParams(self.handle, pnodeparams);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn graph_child_graph_node_get_graph(
        &mut self,
        pgraph: *mutffi::cudaGraph_t,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if pgraph.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if pgraph.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudaGraphChildGraphNodeGetGraph").entered();
            let result = ffi::cudaGraphChildGraphNodeGetGraph(self.handle, pgraph);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn graph_event_record_node_get_event(
        &mut self,
        event_out: *mutffi::cudaEvent_t,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if event_out.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if event_out.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudaGraphEventRecordNodeGetEvent").entered();
            let result = ffi::cudaGraphEventRecordNodeGetEvent(self.handle, event_out);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn graph_event_record_node_set_event(
        &mut self,
        event: ffi::cudaEvent_t,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudaGraphEventRecordNodeSetEvent").entered();
            let result = ffi::cudaGraphEventRecordNodeSetEvent(self.handle, event);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn graph_event_wait_node_get_event(
        &mut self,
        event_out: *mutffi::cudaEvent_t,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if event_out.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if event_out.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudaGraphEventWaitNodeGetEvent").entered();
            let result = ffi::cudaGraphEventWaitNodeGetEvent(self.handle, event_out);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn graph_event_wait_node_set_event(
        &mut self,
        event: ffi::cudaEvent_t,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudaGraphEventWaitNodeSetEvent").entered();
            let result = ffi::cudaGraphEventWaitNodeSetEvent(self.handle, event);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn graph_external_semaphores_signal_node_get_params(
        &mut self,
        params_out: *mutffi::cudaExternalSemaphoreSignalNodeParams,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if params_out.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if params_out.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudaGraphExternalSemaphoresSignalNodeGetParams").entered();
            let result = ffi::cudaGraphExternalSemaphoresSignalNodeGetParams(self.handle, params_out);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn graph_external_semaphores_signal_node_set_params(
        &mut self,
        nodeparams: *constffi::cudaExternalSemaphoreSignalNodeParams,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if nodeparams.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if nodeparams.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudaGraphExternalSemaphoresSignalNodeSetParams").entered();
            let result = ffi::cudaGraphExternalSemaphoresSignalNodeSetParams(self.handle, nodeparams);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn graph_external_semaphores_wait_node_get_params(
        &mut self,
        params_out: *mutffi::cudaExternalSemaphoreWaitNodeParams,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if params_out.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if params_out.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudaGraphExternalSemaphoresWaitNodeGetParams").entered();
            let result = ffi::cudaGraphExternalSemaphoresWaitNodeGetParams(self.handle, params_out);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn graph_external_semaphores_wait_node_set_params(
        &mut self,
        nodeparams: *constffi::cudaExternalSemaphoreWaitNodeParams,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if nodeparams.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if nodeparams.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudaGraphExternalSemaphoresWaitNodeSetParams").entered();
            let result = ffi::cudaGraphExternalSemaphoresWaitNodeSetParams(self.handle, nodeparams);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn graph_mem_alloc_node_get_params(
        &mut self,
        params_out: *mutffi::cudaMemAllocNodeParams,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if params_out.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if params_out.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudaGraphMemAllocNodeGetParams").entered();
            let result = ffi::cudaGraphMemAllocNodeGetParams(self.handle, params_out);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn graph_mem_free_node_get_params(
        &mut self,
        dptr_out: *mut :: core :: ffi ::c_void,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if dptr_out.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if dptr_out.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudaGraphMemFreeNodeGetParams").entered();
            let result = ffi::cudaGraphMemFreeNodeGetParams(self.handle, dptr_out);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn graph_node_get_type(
        &mut self,
        ptype: *mutffi::cudaGraphNodeType,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if ptype.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if ptype.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudaGraphNodeGetType").entered();
            let result = ffi::cudaGraphNodeGetType(self.handle, ptype);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

pub fn graph_get_nodes(
    graph: ffi::cudaGraph_t,
    nodes: *mutffi::cudaGraphNode_t,
    numnodes: *mutusize
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaGraphGetNodes(graph, nodes, numnodes);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn graph_get_root_nodes(
    graph: ffi::cudaGraph_t,
    prootnodes: *mutffi::cudaGraphNode_t,
    pnumrootnodes: *mutusize
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaGraphGetRootNodes(graph, prootnodes, pnumrootnodes);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn graph_get_edges(
    graph: ffi::cudaGraph_t,
    from: *mutffi::cudaGraphNode_t,
    to: *mutffi::cudaGraphNode_t,
    edgedata: *mutffi::cudaGraphEdgeData,
    numedges: *mutusize
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaGraphGetEdges(graph, from, to, edgedata, numedges);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

    #[inline]
    pub fn graph_node_get_dependencies(
        &mut self,
        pdependencies: *mutffi::cudaGraphNode_t,
        edgedata: *mutffi::cudaGraphEdgeData,
        pnumdependencies: *mutusize,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if pdependencies.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if pdependencies.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if edgedata.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if edgedata.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if pnumdependencies.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if pnumdependencies.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudaGraphNodeGetDependencies").entered();
            let result = ffi::cudaGraphNodeGetDependencies(self.handle, pdependencies, edgedata, pnumdependencies);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn graph_node_get_dependent_nodes(
        &mut self,
        pdependentnodes: *mutffi::cudaGraphNode_t,
        edgedata: *mutffi::cudaGraphEdgeData,
        pnumdependentnodes: *mutusize,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if pdependentnodes.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if pdependentnodes.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if edgedata.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if edgedata.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if pnumdependentnodes.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if pnumdependentnodes.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudaGraphNodeGetDependentNodes").entered();
            let result = ffi::cudaGraphNodeGetDependentNodes(self.handle, pdependentnodes, edgedata, pnumdependentnodes);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

pub fn graph_add_dependencies(
    graph: ffi::cudaGraph_t,
    from: *constffi::cudaGraphNode_t,
    to: *constffi::cudaGraphNode_t,
    edgedata: *constffi::cudaGraphEdgeData,
    numdependencies: usize
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaGraphAddDependencies(graph, from, to, edgedata, numdependencies);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn graph_remove_dependencies(
    graph: ffi::cudaGraph_t,
    from: *constffi::cudaGraphNode_t,
    to: *constffi::cudaGraphNode_t,
    edgedata: *constffi::cudaGraphEdgeData,
    numdependencies: usize
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaGraphRemoveDependencies(graph, from, to, edgedata, numdependencies);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

    #[inline]
    pub fn graph_destroy_node(
        &mut self,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudaGraphDestroyNode").entered();
            let result = ffi::cudaGraphDestroyNode(self.handle);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

pub fn graph_exec_kernel_node_set_params(
    hgraphexec: ffi::cudaGraphExec_t,
    node: ffi::cudaGraphNode_t,
    pnodeparams: *constffi::cudaKernelNodeParams
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaGraphExecKernelNodeSetParams(hgraphexec, node, pnodeparams);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn graph_exec_memcpy_node_set_params(
    hgraphexec: ffi::cudaGraphExec_t,
    node: ffi::cudaGraphNode_t,
    pnodeparams: *constffi::cudaMemcpy3DParms
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaGraphExecMemcpyNodeSetParams(hgraphexec, node, pnodeparams);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn graph_exec_memcpy_node_set_params_to_symbol(
    hgraphexec: ffi::cudaGraphExec_t,
    node: ffi::cudaGraphNode_t,
    symbol: *const :: core :: ffi ::c_void,
    src: *const :: core :: ffi ::c_void,
    count: usize,
    offset: usize,
    kind: ffi::cudaMemcpyKind
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaGraphExecMemcpyNodeSetParamsToSymbol(hgraphexec, node, symbol, src, count, offset, kind);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn graph_exec_memcpy_node_set_params_from_symbol(
    hgraphexec: ffi::cudaGraphExec_t,
    node: ffi::cudaGraphNode_t,
    dst: *mut :: core :: ffi ::c_void,
    symbol: *const :: core :: ffi ::c_void,
    count: usize,
    offset: usize,
    kind: ffi::cudaMemcpyKind
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaGraphExecMemcpyNodeSetParamsFromSymbol(hgraphexec, node, dst, symbol, count, offset, kind);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn graph_exec_memcpy_node_set_params_1d(
    hgraphexec: ffi::cudaGraphExec_t,
    node: ffi::cudaGraphNode_t,
    dst: *mut :: core :: ffi ::c_void,
    src: *const :: core :: ffi ::c_void,
    count: usize,
    kind: ffi::cudaMemcpyKind
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaGraphExecMemcpyNodeSetParams1D(hgraphexec, node, dst, src, count, kind);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn graph_exec_memset_node_set_params(
    hgraphexec: ffi::cudaGraphExec_t,
    node: ffi::cudaGraphNode_t,
    pnodeparams: *constffi::cudaMemsetParams
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaGraphExecMemsetNodeSetParams(hgraphexec, node, pnodeparams);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn graph_exec_host_node_set_params(
    hgraphexec: ffi::cudaGraphExec_t,
    node: ffi::cudaGraphNode_t,
    pnodeparams: *constffi::cudaHostNodeParams
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaGraphExecHostNodeSetParams(hgraphexec, node, pnodeparams);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn graph_exec_child_graph_node_set_params(
    hgraphexec: ffi::cudaGraphExec_t,
    node: ffi::cudaGraphNode_t,
    childgraph: ffi::cudaGraph_t
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaGraphExecChildGraphNodeSetParams(hgraphexec, node, childgraph);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn graph_exec_event_record_node_set_event(
    hgraphexec: ffi::cudaGraphExec_t,
    hnode: ffi::cudaGraphNode_t,
    event: ffi::cudaEvent_t
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaGraphExecEventRecordNodeSetEvent(hgraphexec, hnode, event);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn graph_exec_event_wait_node_set_event(
    hgraphexec: ffi::cudaGraphExec_t,
    hnode: ffi::cudaGraphNode_t,
    event: ffi::cudaEvent_t
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaGraphExecEventWaitNodeSetEvent(hgraphexec, hnode, event);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn graph_exec_external_semaphores_signal_node_set_params(
    hgraphexec: ffi::cudaGraphExec_t,
    hnode: ffi::cudaGraphNode_t,
    nodeparams: *constffi::cudaExternalSemaphoreSignalNodeParams
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaGraphExecExternalSemaphoresSignalNodeSetParams(hgraphexec, hnode, nodeparams);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn graph_exec_external_semaphores_wait_node_set_params(
    hgraphexec: ffi::cudaGraphExec_t,
    hnode: ffi::cudaGraphNode_t,
    nodeparams: *constffi::cudaExternalSemaphoreWaitNodeParams
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaGraphExecExternalSemaphoresWaitNodeSetParams(hgraphexec, hnode, nodeparams);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn graph_node_set_enabled(
    hgraphexec: ffi::cudaGraphExec_t,
    hnode: ffi::cudaGraphNode_t,
    isenabled: :: core :: ffi :: c_uint
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaGraphNodeSetEnabled(hgraphexec, hnode, isenabled);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn graph_node_get_enabled(
    hgraphexec: ffi::cudaGraphExec_t,
    hnode: ffi::cudaGraphNode_t,
    isenabled: *mut :: core :: ffi ::c_uint
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaGraphNodeGetEnabled(hgraphexec, hnode, isenabled);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

    #[inline]
    pub fn graph_node_set_params(
        &mut self,
        nodeparams: *mutffi::cudaGraphNodeParams,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if nodeparams.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if nodeparams.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudaGraphNodeSetParams").entered();
            let result = ffi::cudaGraphNodeSetParams(self.handle, nodeparams);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

pub fn graph_exec_node_set_params(
    graphexec: ffi::cudaGraphExec_t,
    node: ffi::cudaGraphNode_t,
    nodeparams: *mutffi::cudaGraphNodeParams
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaGraphExecNodeSetParams(graphexec, node, nodeparams);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

}

// Additional methods for cudnnSeqDataDescriptor_t
impl CudnnSeqDataDescriptor {
    #[inline]
    pub fn set_seq_data_descriptor(
        &mut self,
        datatype: ffi::cudnnDataType_t,
        nbdims: :: core :: ffi :: c_int,
        dima: *const :: core :: ffi ::c_int,
        axes: *constffi::cudnnSeqDataAxis_t,
        seqlengtharraysize: usize,
        seqlengtharray: *const :: core :: ffi ::c_int,
        paddingfill: *mut :: core :: ffi ::c_void,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if dima.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if dima.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if axes.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if axes.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if seqlengtharray.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if seqlengtharray.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if paddingfill.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if paddingfill.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnSetSeqDataDescriptor").entered();
            let result = ffi::cudnnSetSeqDataDescriptor(self.handle, datatype, nbdims, dima, axes, seqlengtharraysize, seqlengtharray, paddingfill);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn get_seq_data_descriptor(
        &mut self,
        datatype: *mutffi::cudnnDataType_t,
        nbdims: *mut :: core :: ffi ::c_int,
        nbdimsrequested: :: core :: ffi :: c_int,
        dima: *mut :: core :: ffi ::c_int,
        axes: *mutffi::cudnnSeqDataAxis_t,
        seqlengtharraysize: *mutusize,
        seqlengthsizerequested: usize,
        seqlengtharray: *mut :: core :: ffi ::c_int,
        paddingfill: *mut :: core :: ffi ::c_void,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if datatype.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if datatype.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if nbdims.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if nbdims.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if dima.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if dima.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if axes.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if axes.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if seqlengtharraysize.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if seqlengtharraysize.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if seqlengtharray.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if seqlengtharray.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if paddingfill.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if paddingfill.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnGetSeqDataDescriptor").entered();
            let result = ffi::cudnnGetSeqDataDescriptor(self.handle, datatype, nbdims, nbdimsrequested, dima, axes, seqlengtharraysize, seqlengthsizerequested, seqlengtharray, paddingfill);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

pub fn multi_head_attn_forward(
    handle: ffi::cudnnHandle_t,
    attndesc: ffi::cudnnAttnDescriptor_t,
    curridx: :: core :: ffi :: c_int,
    lowinidx: *const :: core :: ffi ::c_int,
    hiwinidx: *const :: core :: ffi ::c_int,
    devseqlengthsqo: *const :: core :: ffi ::c_int,
    devseqlengthskv: *const :: core :: ffi ::c_int,
    qdesc: ffi::cudnnSeqDataDescriptor_t,
    queries: *const :: core :: ffi ::c_void,
    residuals: *const :: core :: ffi ::c_void,
    kdesc: ffi::cudnnSeqDataDescriptor_t,
    keys: *const :: core :: ffi ::c_void,
    vdesc: ffi::cudnnSeqDataDescriptor_t,
    values: *const :: core :: ffi ::c_void,
    odesc: ffi::cudnnSeqDataDescriptor_t,
    out: *mut :: core :: ffi ::c_void,
    weightsizeinbytes: usize,
    weights: *const :: core :: ffi ::c_void,
    workspacesizeinbytes: usize,
    workspace: *mut :: core :: ffi ::c_void,
    reservespacesizeinbytes: usize,
    reservespace: *mut :: core :: ffi ::c_void
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnMultiHeadAttnForward(handle, attndesc, curridx, lowinidx, hiwinidx, devseqlengthsqo, devseqlengthskv, qdesc, queries, residuals, kdesc, keys, vdesc, values, odesc, out, weightsizeinbytes, weights, workspacesizeinbytes, workspace, reservespacesizeinbytes, reservespace);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn multi_head_attn_backward_data(
    handle: ffi::cudnnHandle_t,
    attndesc: ffi::cudnnAttnDescriptor_t,
    lowinidx: *const :: core :: ffi ::c_int,
    hiwinidx: *const :: core :: ffi ::c_int,
    devseqlengthsdqdo: *const :: core :: ffi ::c_int,
    devseqlengthsdkdv: *const :: core :: ffi ::c_int,
    dodesc: ffi::cudnnSeqDataDescriptor_t,
    dout: *const :: core :: ffi ::c_void,
    dqdesc: ffi::cudnnSeqDataDescriptor_t,
    dqueries: *mut :: core :: ffi ::c_void,
    queries: *const :: core :: ffi ::c_void,
    dkdesc: ffi::cudnnSeqDataDescriptor_t,
    dkeys: *mut :: core :: ffi ::c_void,
    keys: *const :: core :: ffi ::c_void,
    dvdesc: ffi::cudnnSeqDataDescriptor_t,
    dvalues: *mut :: core :: ffi ::c_void,
    values: *const :: core :: ffi ::c_void,
    weightsizeinbytes: usize,
    weights: *const :: core :: ffi ::c_void,
    workspacesizeinbytes: usize,
    workspace: *mut :: core :: ffi ::c_void,
    reservespacesizeinbytes: usize,
    reservespace: *mut :: core :: ffi ::c_void
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnMultiHeadAttnBackwardData(handle, attndesc, lowinidx, hiwinidx, devseqlengthsdqdo, devseqlengthsdkdv, dodesc, dout, dqdesc, dqueries, queries, dkdesc, dkeys, keys, dvdesc, dvalues, values, weightsizeinbytes, weights, workspacesizeinbytes, workspace, reservespacesizeinbytes, reservespace);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn multi_head_attn_backward_weights(
    handle: ffi::cudnnHandle_t,
    attndesc: ffi::cudnnAttnDescriptor_t,
    addgrad: ffi::cudnnWgradMode_t,
    qdesc: ffi::cudnnSeqDataDescriptor_t,
    queries: *const :: core :: ffi ::c_void,
    kdesc: ffi::cudnnSeqDataDescriptor_t,
    keys: *const :: core :: ffi ::c_void,
    vdesc: ffi::cudnnSeqDataDescriptor_t,
    values: *const :: core :: ffi ::c_void,
    dodesc: ffi::cudnnSeqDataDescriptor_t,
    dout: *const :: core :: ffi ::c_void,
    weightsizeinbytes: usize,
    weights: *const :: core :: ffi ::c_void,
    dweights: *mut :: core :: ffi ::c_void,
    workspacesizeinbytes: usize,
    workspace: *mut :: core :: ffi ::c_void,
    reservespacesizeinbytes: usize,
    reservespace: *mut :: core :: ffi ::c_void
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnMultiHeadAttnBackwardWeights(handle, attndesc, addgrad, qdesc, queries, kdesc, keys, vdesc, values, dodesc, dout, weightsizeinbytes, weights, dweights, workspacesizeinbytes, workspace, reservespacesizeinbytes, reservespace);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

}

// Additional methods for cudnnTensorDescriptor_t
impl CudnnTensorDescriptor {
    #[inline]
    pub fn set_tensor_4d_descriptor(
        &mut self,
        format: ffi::cudnnTensorFormat_t,
        datatype: ffi::cudnnDataType_t,
        n: :: core :: ffi :: c_int,
        c: :: core :: ffi :: c_int,
        h: :: core :: ffi :: c_int,
        w: :: core :: ffi :: c_int,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnSetTensor4dDescriptor").entered();
            let result = ffi::cudnnSetTensor4dDescriptor(self.handle, format, datatype, n, c, h, w);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn set_tensor_4d_descriptor_ex(
        &mut self,
        datatype: ffi::cudnnDataType_t,
        n: :: core :: ffi :: c_int,
        c: :: core :: ffi :: c_int,
        h: :: core :: ffi :: c_int,
        w: :: core :: ffi :: c_int,
        nstride: :: core :: ffi :: c_int,
        cstride: :: core :: ffi :: c_int,
        hstride: :: core :: ffi :: c_int,
        wstride: :: core :: ffi :: c_int,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnSetTensor4dDescriptorEx").entered();
            let result = ffi::cudnnSetTensor4dDescriptorEx(self.handle, datatype, n, c, h, w, nstride, cstride, hstride, wstride);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn get_tensor_4d_descriptor(
        &mut self,
        datatype: *mutffi::cudnnDataType_t,
        n: *mut :: core :: ffi ::c_int,
        c: *mut :: core :: ffi ::c_int,
        h: *mut :: core :: ffi ::c_int,
        w: *mut :: core :: ffi ::c_int,
        nstride: *mut :: core :: ffi ::c_int,
        cstride: *mut :: core :: ffi ::c_int,
        hstride: *mut :: core :: ffi ::c_int,
        wstride: *mut :: core :: ffi ::c_int,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if datatype.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if datatype.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if n.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if n.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if c.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if c.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if h.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if h.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if w.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if w.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if nstride.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if nstride.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if cstride.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if cstride.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if hstride.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if hstride.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if wstride.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if wstride.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnGetTensor4dDescriptor").entered();
            let result = ffi::cudnnGetTensor4dDescriptor(self.handle, datatype, n, c, h, w, nstride, cstride, hstride, wstride);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn set_tensor_nd_descriptor(
        &mut self,
        datatype: ffi::cudnnDataType_t,
        nbdims: :: core :: ffi :: c_int,
        dima: *const :: core :: ffi ::c_int,
        stridea: *const :: core :: ffi ::c_int,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if dima.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if dima.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if stridea.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if stridea.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnSetTensorNdDescriptor").entered();
            let result = ffi::cudnnSetTensorNdDescriptor(self.handle, datatype, nbdims, dima, stridea);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn set_tensor_nd_descriptor_ex(
        &mut self,
        format: ffi::cudnnTensorFormat_t,
        datatype: ffi::cudnnDataType_t,
        nbdims: :: core :: ffi :: c_int,
        dima: *const :: core :: ffi ::c_int,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if dima.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if dima.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnSetTensorNdDescriptorEx").entered();
            let result = ffi::cudnnSetTensorNdDescriptorEx(self.handle, format, datatype, nbdims, dima);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn get_tensor_nd_descriptor(
        &mut self,
        nbdimsrequested: :: core :: ffi :: c_int,
        datatype: *mutffi::cudnnDataType_t,
        nbdims: *mut :: core :: ffi ::c_int,
        dima: *mut :: core :: ffi ::c_int,
        stridea: *mut :: core :: ffi ::c_int,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if datatype.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if datatype.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if nbdims.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if nbdims.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if dima.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if dima.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if stridea.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if stridea.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnGetTensorNdDescriptor").entered();
            let result = ffi::cudnnGetTensorNdDescriptor(self.handle, nbdimsrequested, datatype, nbdims, dima, stridea);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn get_tensor_size_in_bytes(
        &mut self,
        size: *mutusize,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if size.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if size.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnGetTensorSizeInBytes").entered();
            let result = ffi::cudnnGetTensorSizeInBytes(self.handle, size);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

pub fn init_transform_dest(
    transformdesc: ffi::cudnnTensorTransformDescriptor_t,
    srcdesc: ffi::cudnnTensorDescriptor_t,
    destdesc: ffi::cudnnTensorDescriptor_t,
    destsizeinbytes: *mutusize
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnInitTransformDest(transformdesc, srcdesc, destdesc, destsizeinbytes);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn transform_tensor(
    handle: ffi::cudnnHandle_t,
    alpha: *const :: core :: ffi ::c_void,
    xdesc: ffi::cudnnTensorDescriptor_t,
    x: *const :: core :: ffi ::c_void,
    beta: *const :: core :: ffi ::c_void,
    ydesc: ffi::cudnnTensorDescriptor_t,
    y: *mut :: core :: ffi ::c_void
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnTransformTensor(handle, alpha, xdesc, x, beta, ydesc, y);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn transform_tensor_ex(
    handle: ffi::cudnnHandle_t,
    transdesc: ffi::cudnnTensorTransformDescriptor_t,
    alpha: *const :: core :: ffi ::c_void,
    srcdesc: ffi::cudnnTensorDescriptor_t,
    srcdata: *const :: core :: ffi ::c_void,
    beta: *const :: core :: ffi ::c_void,
    destdesc: ffi::cudnnTensorDescriptor_t,
    destdata: *mut :: core :: ffi ::c_void
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnTransformTensorEx(handle, transdesc, alpha, srcdesc, srcdata, beta, destdesc, destdata);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn add_tensor(
    handle: ffi::cudnnHandle_t,
    alpha: *const :: core :: ffi ::c_void,
    adesc: ffi::cudnnTensorDescriptor_t,
    a: *const :: core :: ffi ::c_void,
    beta: *const :: core :: ffi ::c_void,
    cdesc: ffi::cudnnTensorDescriptor_t,
    c: *mut :: core :: ffi ::c_void
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnAddTensor(handle, alpha, adesc, a, beta, cdesc, c);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn op_tensor(
    handle: ffi::cudnnHandle_t,
    optensordesc: ffi::cudnnOpTensorDescriptor_t,
    alpha1: *const :: core :: ffi ::c_void,
    adesc: ffi::cudnnTensorDescriptor_t,
    a: *const :: core :: ffi ::c_void,
    alpha2: *const :: core :: ffi ::c_void,
    bdesc: ffi::cudnnTensorDescriptor_t,
    b: *const :: core :: ffi ::c_void,
    beta: *const :: core :: ffi ::c_void,
    cdesc: ffi::cudnnTensorDescriptor_t,
    c: *mut :: core :: ffi ::c_void
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnOpTensor(handle, optensordesc, alpha1, adesc, a, alpha2, bdesc, b, beta, cdesc, c);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn get_reduction_indices_size(
    handle: ffi::cudnnHandle_t,
    reducetensordesc: ffi::cudnnReduceTensorDescriptor_t,
    adesc: ffi::cudnnTensorDescriptor_t,
    cdesc: ffi::cudnnTensorDescriptor_t,
    sizeinbytes: *mutusize
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnGetReductionIndicesSize(handle, reducetensordesc, adesc, cdesc, sizeinbytes);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn get_reduction_workspace_size(
    handle: ffi::cudnnHandle_t,
    reducetensordesc: ffi::cudnnReduceTensorDescriptor_t,
    adesc: ffi::cudnnTensorDescriptor_t,
    cdesc: ffi::cudnnTensorDescriptor_t,
    sizeinbytes: *mutusize
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnGetReductionWorkspaceSize(handle, reducetensordesc, adesc, cdesc, sizeinbytes);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn reduce_tensor(
    handle: ffi::cudnnHandle_t,
    reducetensordesc: ffi::cudnnReduceTensorDescriptor_t,
    indices: *mut :: core :: ffi ::c_void,
    indicessizeinbytes: usize,
    workspace: *mut :: core :: ffi ::c_void,
    workspacesizeinbytes: usize,
    alpha: *const :: core :: ffi ::c_void,
    adesc: ffi::cudnnTensorDescriptor_t,
    a: *const :: core :: ffi ::c_void,
    beta: *const :: core :: ffi ::c_void,
    cdesc: ffi::cudnnTensorDescriptor_t,
    c: *mut :: core :: ffi ::c_void
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnReduceTensor(handle, reducetensordesc, indices, indicessizeinbytes, workspace, workspacesizeinbytes, alpha, adesc, a, beta, cdesc, c);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn set_tensor(
    handle: ffi::cudnnHandle_t,
    ydesc: ffi::cudnnTensorDescriptor_t,
    y: *mut :: core :: ffi ::c_void,
    valueptr: *const :: core :: ffi ::c_void
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnSetTensor(handle, ydesc, y, valueptr);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn scale_tensor(
    handle: ffi::cudnnHandle_t,
    ydesc: ffi::cudnnTensorDescriptor_t,
    y: *mut :: core :: ffi ::c_void,
    alpha: *const :: core :: ffi ::c_void
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnScaleTensor(handle, ydesc, y, alpha);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn softmax_forward(
    handle: ffi::cudnnHandle_t,
    algo: ffi::cudnnSoftmaxAlgorithm_t,
    mode: ffi::cudnnSoftmaxMode_t,
    alpha: *const :: core :: ffi ::c_void,
    xdesc: ffi::cudnnTensorDescriptor_t,
    x: *const :: core :: ffi ::c_void,
    beta: *const :: core :: ffi ::c_void,
    ydesc: ffi::cudnnTensorDescriptor_t,
    y: *mut :: core :: ffi ::c_void
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnSoftmaxForward(handle, algo, mode, alpha, xdesc, x, beta, ydesc, y);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn get_pooling_nd_forward_output_dim(
    poolingdesc: ffi::cudnnPoolingDescriptor_t,
    inputtensordesc: ffi::cudnnTensorDescriptor_t,
    nbdims: :: core :: ffi :: c_int,
    outputtensordima: *mut :: core :: ffi ::c_int
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnGetPoolingNdForwardOutputDim(poolingdesc, inputtensordesc, nbdims, outputtensordima);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn get_pooling_2d_forward_output_dim(
    poolingdesc: ffi::cudnnPoolingDescriptor_t,
    inputtensordesc: ffi::cudnnTensorDescriptor_t,
    n: *mut :: core :: ffi ::c_int,
    c: *mut :: core :: ffi ::c_int,
    h: *mut :: core :: ffi ::c_int,
    w: *mut :: core :: ffi ::c_int
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnGetPooling2dForwardOutputDim(poolingdesc, inputtensordesc, n, c, h, w);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn pooling_forward(
    handle: ffi::cudnnHandle_t,
    poolingdesc: ffi::cudnnPoolingDescriptor_t,
    alpha: *const :: core :: ffi ::c_void,
    xdesc: ffi::cudnnTensorDescriptor_t,
    x: *const :: core :: ffi ::c_void,
    beta: *const :: core :: ffi ::c_void,
    ydesc: ffi::cudnnTensorDescriptor_t,
    y: *mut :: core :: ffi ::c_void
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnPoolingForward(handle, poolingdesc, alpha, xdesc, x, beta, ydesc, y);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn activation_forward(
    handle: ffi::cudnnHandle_t,
    activationdesc: ffi::cudnnActivationDescriptor_t,
    alpha: *const :: core :: ffi ::c_void,
    xdesc: ffi::cudnnTensorDescriptor_t,
    x: *const :: core :: ffi ::c_void,
    beta: *const :: core :: ffi ::c_void,
    ydesc: ffi::cudnnTensorDescriptor_t,
    y: *mut :: core :: ffi ::c_void
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnActivationForward(handle, activationdesc, alpha, xdesc, x, beta, ydesc, y);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn lrn_cross_channel_forward(
    handle: ffi::cudnnHandle_t,
    normdesc: ffi::cudnnLRNDescriptor_t,
    lrnmode: ffi::cudnnLRNMode_t,
    alpha: *const :: core :: ffi ::c_void,
    xdesc: ffi::cudnnTensorDescriptor_t,
    x: *const :: core :: ffi ::c_void,
    beta: *const :: core :: ffi ::c_void,
    ydesc: ffi::cudnnTensorDescriptor_t,
    y: *mut :: core :: ffi ::c_void
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnLRNCrossChannelForward(handle, normdesc, lrnmode, alpha, xdesc, x, beta, ydesc, y);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn divisive_normalization_forward(
    handle: ffi::cudnnHandle_t,
    normdesc: ffi::cudnnLRNDescriptor_t,
    mode: ffi::cudnnDivNormMode_t,
    alpha: *const :: core :: ffi ::c_void,
    xdesc: ffi::cudnnTensorDescriptor_t,
    x: *const :: core :: ffi ::c_void,
    means: *const :: core :: ffi ::c_void,
    temp: *mut :: core :: ffi ::c_void,
    temp2: *mut :: core :: ffi ::c_void,
    beta: *const :: core :: ffi ::c_void,
    ydesc: ffi::cudnnTensorDescriptor_t,
    y: *mut :: core :: ffi ::c_void
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnDivisiveNormalizationForward(handle, normdesc, mode, alpha, xdesc, x, means, temp, temp2, beta, ydesc, y);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

    #[inline]
    pub fn derive_bn_tensor_descriptor(
        &mut self,
        xdesc: ffi::cudnnTensorDescriptor_t,
        mode: ffi::cudnnBatchNormMode_t,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnDeriveBNTensorDescriptor").entered();
            let result = ffi::cudnnDeriveBNTensorDescriptor(self.handle, xdesc, mode);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

pub fn batch_normalization_forward_inference(
    handle: ffi::cudnnHandle_t,
    mode: ffi::cudnnBatchNormMode_t,
    alpha: *const :: core :: ffi ::c_void,
    beta: *const :: core :: ffi ::c_void,
    xdesc: ffi::cudnnTensorDescriptor_t,
    x: *const :: core :: ffi ::c_void,
    ydesc: ffi::cudnnTensorDescriptor_t,
    y: *mut :: core :: ffi ::c_void,
    bnscalebiasmeanvardesc: ffi::cudnnTensorDescriptor_t,
    bnscale: *const :: core :: ffi ::c_void,
    bnbias: *const :: core :: ffi ::c_void,
    estimatedmean: *const :: core :: ffi ::c_void,
    estimatedvariance: *const :: core :: ffi ::c_void,
    epsilon: f64
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnBatchNormalizationForwardInference(handle, mode, alpha, beta, xdesc, x, ydesc, y, bnscalebiasmeanvardesc, bnscale, bnbias, estimatedmean, estimatedvariance, epsilon);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

    #[inline]
    pub fn derive_norm_tensor_descriptor(
        &mut self,
        derivednormmeanvardesc: ffi::cudnnTensorDescriptor_t,
        xdesc: ffi::cudnnTensorDescriptor_t,
        mode: ffi::cudnnNormMode_t,
        groupcnt: :: core :: ffi :: c_int,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnDeriveNormTensorDescriptor").entered();
            let result = ffi::cudnnDeriveNormTensorDescriptor(self.handle, derivednormmeanvardesc, xdesc, mode, groupcnt);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

pub fn normalization_forward_inference(
    handle: ffi::cudnnHandle_t,
    mode: ffi::cudnnNormMode_t,
    normops: ffi::cudnnNormOps_t,
    algo: ffi::cudnnNormAlgo_t,
    alpha: *const :: core :: ffi ::c_void,
    beta: *const :: core :: ffi ::c_void,
    xdesc: ffi::cudnnTensorDescriptor_t,
    x: *const :: core :: ffi ::c_void,
    normscalebiasdesc: ffi::cudnnTensorDescriptor_t,
    normscale: *const :: core :: ffi ::c_void,
    normbias: *const :: core :: ffi ::c_void,
    normmeanvardesc: ffi::cudnnTensorDescriptor_t,
    estimatedmean: *const :: core :: ffi ::c_void,
    estimatedvariance: *const :: core :: ffi ::c_void,
    zdesc: ffi::cudnnTensorDescriptor_t,
    z: *const :: core :: ffi ::c_void,
    activationdesc: ffi::cudnnActivationDescriptor_t,
    ydesc: ffi::cudnnTensorDescriptor_t,
    y: *mut :: core :: ffi ::c_void,
    epsilon: f64,
    groupcnt: :: core :: ffi :: c_int
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnNormalizationForwardInference(handle, mode, normops, algo, alpha, beta, xdesc, x, normscalebiasdesc, normscale, normbias, normmeanvardesc, estimatedmean, estimatedvariance, zdesc, z, activationdesc, ydesc, y, epsilon, groupcnt);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn spatial_tf_sampler_forward(
    handle: ffi::cudnnHandle_t,
    stdesc: ffi::cudnnSpatialTransformerDescriptor_t,
    alpha: *const :: core :: ffi ::c_void,
    xdesc: ffi::cudnnTensorDescriptor_t,
    x: *const :: core :: ffi ::c_void,
    grid: *const :: core :: ffi ::c_void,
    beta: *const :: core :: ffi ::c_void,
    ydesc: ffi::cudnnTensorDescriptor_t,
    y: *mut :: core :: ffi ::c_void
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnSpatialTfSamplerForward(handle, stdesc, alpha, xdesc, x, grid, beta, ydesc, y);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

    #[inline]
    pub fn dropout_get_reserve_space_size(
        &mut self,
        sizeinbytes: *mutusize,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if sizeinbytes.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if sizeinbytes.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnDropoutGetReserveSpaceSize").entered();
            let result = ffi::cudnnDropoutGetReserveSpaceSize(self.handle, sizeinbytes);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

pub fn dropout_forward(
    handle: ffi::cudnnHandle_t,
    dropoutdesc: ffi::cudnnDropoutDescriptor_t,
    xdesc: ffi::cudnnTensorDescriptor_t,
    x: *const :: core :: ffi ::c_void,
    ydesc: ffi::cudnnTensorDescriptor_t,
    y: *mut :: core :: ffi ::c_void,
    reservespace: *mut :: core :: ffi ::c_void,
    reservespacesizeinbytes: usize
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnDropoutForward(handle, dropoutdesc, xdesc, x, ydesc, y, reservespace, reservespacesizeinbytes);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn softmax_backward(
    handle: ffi::cudnnHandle_t,
    algo: ffi::cudnnSoftmaxAlgorithm_t,
    mode: ffi::cudnnSoftmaxMode_t,
    alpha: *const :: core :: ffi ::c_void,
    ydesc: ffi::cudnnTensorDescriptor_t,
    y: *const :: core :: ffi ::c_void,
    dydesc: ffi::cudnnTensorDescriptor_t,
    dy: *const :: core :: ffi ::c_void,
    beta: *const :: core :: ffi ::c_void,
    dxdesc: ffi::cudnnTensorDescriptor_t,
    dx: *mut :: core :: ffi ::c_void
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnSoftmaxBackward(handle, algo, mode, alpha, ydesc, y, dydesc, dy, beta, dxdesc, dx);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn pooling_backward(
    handle: ffi::cudnnHandle_t,
    poolingdesc: ffi::cudnnPoolingDescriptor_t,
    alpha: *const :: core :: ffi ::c_void,
    ydesc: ffi::cudnnTensorDescriptor_t,
    y: *const :: core :: ffi ::c_void,
    dydesc: ffi::cudnnTensorDescriptor_t,
    dy: *const :: core :: ffi ::c_void,
    xdesc: ffi::cudnnTensorDescriptor_t,
    x: *const :: core :: ffi ::c_void,
    beta: *const :: core :: ffi ::c_void,
    dxdesc: ffi::cudnnTensorDescriptor_t,
    dx: *mut :: core :: ffi ::c_void
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnPoolingBackward(handle, poolingdesc, alpha, ydesc, y, dydesc, dy, xdesc, x, beta, dxdesc, dx);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn activation_backward(
    handle: ffi::cudnnHandle_t,
    activationdesc: ffi::cudnnActivationDescriptor_t,
    alpha: *const :: core :: ffi ::c_void,
    ydesc: ffi::cudnnTensorDescriptor_t,
    y: *const :: core :: ffi ::c_void,
    dydesc: ffi::cudnnTensorDescriptor_t,
    dy: *const :: core :: ffi ::c_void,
    xdesc: ffi::cudnnTensorDescriptor_t,
    x: *const :: core :: ffi ::c_void,
    beta: *const :: core :: ffi ::c_void,
    dxdesc: ffi::cudnnTensorDescriptor_t,
    dx: *mut :: core :: ffi ::c_void
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnActivationBackward(handle, activationdesc, alpha, ydesc, y, dydesc, dy, xdesc, x, beta, dxdesc, dx);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn lrn_cross_channel_backward(
    handle: ffi::cudnnHandle_t,
    normdesc: ffi::cudnnLRNDescriptor_t,
    lrnmode: ffi::cudnnLRNMode_t,
    alpha: *const :: core :: ffi ::c_void,
    ydesc: ffi::cudnnTensorDescriptor_t,
    y: *const :: core :: ffi ::c_void,
    dydesc: ffi::cudnnTensorDescriptor_t,
    dy: *const :: core :: ffi ::c_void,
    xdesc: ffi::cudnnTensorDescriptor_t,
    x: *const :: core :: ffi ::c_void,
    beta: *const :: core :: ffi ::c_void,
    dxdesc: ffi::cudnnTensorDescriptor_t,
    dx: *mut :: core :: ffi ::c_void
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnLRNCrossChannelBackward(handle, normdesc, lrnmode, alpha, ydesc, y, dydesc, dy, xdesc, x, beta, dxdesc, dx);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn divisive_normalization_backward(
    handle: ffi::cudnnHandle_t,
    normdesc: ffi::cudnnLRNDescriptor_t,
    mode: ffi::cudnnDivNormMode_t,
    alpha: *const :: core :: ffi ::c_void,
    xdesc: ffi::cudnnTensorDescriptor_t,
    x: *const :: core :: ffi ::c_void,
    means: *const :: core :: ffi ::c_void,
    dy: *const :: core :: ffi ::c_void,
    temp: *mut :: core :: ffi ::c_void,
    temp2: *mut :: core :: ffi ::c_void,
    beta: *const :: core :: ffi ::c_void,
    dxdmeansdesc: ffi::cudnnTensorDescriptor_t,
    dx: *mut :: core :: ffi ::c_void,
    dmeans: *mut :: core :: ffi ::c_void
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnDivisiveNormalizationBackward(handle, normdesc, mode, alpha, xdesc, x, means, dy, temp, temp2, beta, dxdmeansdesc, dx, dmeans);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn get_batch_normalization_forward_training_ex_workspace_size(
    handle: ffi::cudnnHandle_t,
    mode: ffi::cudnnBatchNormMode_t,
    bnops: ffi::cudnnBatchNormOps_t,
    xdesc: ffi::cudnnTensorDescriptor_t,
    zdesc: ffi::cudnnTensorDescriptor_t,
    ydesc: ffi::cudnnTensorDescriptor_t,
    bnscalebiasmeanvardesc: ffi::cudnnTensorDescriptor_t,
    activationdesc: ffi::cudnnActivationDescriptor_t,
    sizeinbytes: *mutusize
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnGetBatchNormalizationForwardTrainingExWorkspaceSize(handle, mode, bnops, xdesc, zdesc, ydesc, bnscalebiasmeanvardesc, activationdesc, sizeinbytes);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn get_batch_normalization_backward_ex_workspace_size(
    handle: ffi::cudnnHandle_t,
    mode: ffi::cudnnBatchNormMode_t,
    bnops: ffi::cudnnBatchNormOps_t,
    xdesc: ffi::cudnnTensorDescriptor_t,
    ydesc: ffi::cudnnTensorDescriptor_t,
    dydesc: ffi::cudnnTensorDescriptor_t,
    dzdesc: ffi::cudnnTensorDescriptor_t,
    dxdesc: ffi::cudnnTensorDescriptor_t,
    dbnscalebiasdesc: ffi::cudnnTensorDescriptor_t,
    activationdesc: ffi::cudnnActivationDescriptor_t,
    sizeinbytes: *mutusize
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnGetBatchNormalizationBackwardExWorkspaceSize(handle, mode, bnops, xdesc, ydesc, dydesc, dzdesc, dxdesc, dbnscalebiasdesc, activationdesc, sizeinbytes);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn get_batch_normalization_training_ex_reserve_space_size(
    handle: ffi::cudnnHandle_t,
    mode: ffi::cudnnBatchNormMode_t,
    bnops: ffi::cudnnBatchNormOps_t,
    activationdesc: ffi::cudnnActivationDescriptor_t,
    xdesc: ffi::cudnnTensorDescriptor_t,
    sizeinbytes: *mutusize
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnGetBatchNormalizationTrainingExReserveSpaceSize(handle, mode, bnops, activationdesc, xdesc, sizeinbytes);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn batch_normalization_forward_training(
    handle: ffi::cudnnHandle_t,
    mode: ffi::cudnnBatchNormMode_t,
    alpha: *const :: core :: ffi ::c_void,
    beta: *const :: core :: ffi ::c_void,
    xdesc: ffi::cudnnTensorDescriptor_t,
    x: *const :: core :: ffi ::c_void,
    ydesc: ffi::cudnnTensorDescriptor_t,
    y: *mut :: core :: ffi ::c_void,
    bnscalebiasmeanvardesc: ffi::cudnnTensorDescriptor_t,
    bnscale: *const :: core :: ffi ::c_void,
    bnbias: *const :: core :: ffi ::c_void,
    exponentialaveragefactor: f64,
    resultrunningmean: *mut :: core :: ffi ::c_void,
    resultrunningvariance: *mut :: core :: ffi ::c_void,
    epsilon: f64,
    resultsavemean: *mut :: core :: ffi ::c_void,
    resultsaveinvvariance: *mut :: core :: ffi ::c_void
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnBatchNormalizationForwardTraining(handle, mode, alpha, beta, xdesc, x, ydesc, y, bnscalebiasmeanvardesc, bnscale, bnbias, exponentialaveragefactor, resultrunningmean, resultrunningvariance, epsilon, resultsavemean, resultsaveinvvariance);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn batch_normalization_forward_training_ex(
    handle: ffi::cudnnHandle_t,
    mode: ffi::cudnnBatchNormMode_t,
    bnops: ffi::cudnnBatchNormOps_t,
    alpha: *const :: core :: ffi ::c_void,
    beta: *const :: core :: ffi ::c_void,
    xdesc: ffi::cudnnTensorDescriptor_t,
    xdata: *const :: core :: ffi ::c_void,
    zdesc: ffi::cudnnTensorDescriptor_t,
    zdata: *const :: core :: ffi ::c_void,
    ydesc: ffi::cudnnTensorDescriptor_t,
    ydata: *mut :: core :: ffi ::c_void,
    bnscalebiasmeanvardesc: ffi::cudnnTensorDescriptor_t,
    bnscale: *const :: core :: ffi ::c_void,
    bnbias: *const :: core :: ffi ::c_void,
    exponentialaveragefactor: f64,
    resultrunningmean: *mut :: core :: ffi ::c_void,
    resultrunningvariance: *mut :: core :: ffi ::c_void,
    epsilon: f64,
    resultsavemean: *mut :: core :: ffi ::c_void,
    resultsaveinvvariance: *mut :: core :: ffi ::c_void,
    activationdesc: ffi::cudnnActivationDescriptor_t,
    workspace: *mut :: core :: ffi ::c_void,
    workspacesizeinbytes: usize,
    reservespace: *mut :: core :: ffi ::c_void,
    reservespacesizeinbytes: usize
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnBatchNormalizationForwardTrainingEx(handle, mode, bnops, alpha, beta, xdesc, xdata, zdesc, zdata, ydesc, ydata, bnscalebiasmeanvardesc, bnscale, bnbias, exponentialaveragefactor, resultrunningmean, resultrunningvariance, epsilon, resultsavemean, resultsaveinvvariance, activationdesc, workspace, workspacesizeinbytes, reservespace, reservespacesizeinbytes);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn batch_normalization_backward(
    handle: ffi::cudnnHandle_t,
    mode: ffi::cudnnBatchNormMode_t,
    alphadatadiff: *const :: core :: ffi ::c_void,
    betadatadiff: *const :: core :: ffi ::c_void,
    alphaparamdiff: *const :: core :: ffi ::c_void,
    betaparamdiff: *const :: core :: ffi ::c_void,
    xdesc: ffi::cudnnTensorDescriptor_t,
    x: *const :: core :: ffi ::c_void,
    dydesc: ffi::cudnnTensorDescriptor_t,
    dy: *const :: core :: ffi ::c_void,
    dxdesc: ffi::cudnnTensorDescriptor_t,
    dx: *mut :: core :: ffi ::c_void,
    dbnscalebiasdesc: ffi::cudnnTensorDescriptor_t,
    bnscale: *const :: core :: ffi ::c_void,
    dbnscaleresult: *mut :: core :: ffi ::c_void,
    dbnbiasresult: *mut :: core :: ffi ::c_void,
    epsilon: f64,
    savedmean: *const :: core :: ffi ::c_void,
    savedinvvariance: *const :: core :: ffi ::c_void
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnBatchNormalizationBackward(handle, mode, alphadatadiff, betadatadiff, alphaparamdiff, betaparamdiff, xdesc, x, dydesc, dy, dxdesc, dx, dbnscalebiasdesc, bnscale, dbnscaleresult, dbnbiasresult, epsilon, savedmean, savedinvvariance);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn batch_normalization_backward_ex(
    handle: ffi::cudnnHandle_t,
    mode: ffi::cudnnBatchNormMode_t,
    bnops: ffi::cudnnBatchNormOps_t,
    alphadatadiff: *const :: core :: ffi ::c_void,
    betadatadiff: *const :: core :: ffi ::c_void,
    alphaparamdiff: *const :: core :: ffi ::c_void,
    betaparamdiff: *const :: core :: ffi ::c_void,
    xdesc: ffi::cudnnTensorDescriptor_t,
    xdata: *const :: core :: ffi ::c_void,
    ydesc: ffi::cudnnTensorDescriptor_t,
    ydata: *const :: core :: ffi ::c_void,
    dydesc: ffi::cudnnTensorDescriptor_t,
    dydata: *const :: core :: ffi ::c_void,
    dzdesc: ffi::cudnnTensorDescriptor_t,
    dzdata: *mut :: core :: ffi ::c_void,
    dxdesc: ffi::cudnnTensorDescriptor_t,
    dxdata: *mut :: core :: ffi ::c_void,
    dbnscalebiasdesc: ffi::cudnnTensorDescriptor_t,
    bnscaledata: *const :: core :: ffi ::c_void,
    bnbiasdata: *const :: core :: ffi ::c_void,
    dbnscaledata: *mut :: core :: ffi ::c_void,
    dbnbiasdata: *mut :: core :: ffi ::c_void,
    epsilon: f64,
    savedmean: *const :: core :: ffi ::c_void,
    savedinvvariance: *const :: core :: ffi ::c_void,
    activationdesc: ffi::cudnnActivationDescriptor_t,
    workspace: *mut :: core :: ffi ::c_void,
    workspacesizeinbytes: usize,
    reservespace: *mut :: core :: ffi ::c_void,
    reservespacesizeinbytes: usize
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnBatchNormalizationBackwardEx(handle, mode, bnops, alphadatadiff, betadatadiff, alphaparamdiff, betaparamdiff, xdesc, xdata, ydesc, ydata, dydesc, dydata, dzdesc, dzdata, dxdesc, dxdata, dbnscalebiasdesc, bnscaledata, bnbiasdata, dbnscaledata, dbnbiasdata, epsilon, savedmean, savedinvvariance, activationdesc, workspace, workspacesizeinbytes, reservespace, reservespacesizeinbytes);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn get_normalization_forward_training_workspace_size(
    handle: ffi::cudnnHandle_t,
    mode: ffi::cudnnNormMode_t,
    normops: ffi::cudnnNormOps_t,
    algo: ffi::cudnnNormAlgo_t,
    xdesc: ffi::cudnnTensorDescriptor_t,
    zdesc: ffi::cudnnTensorDescriptor_t,
    ydesc: ffi::cudnnTensorDescriptor_t,
    normscalebiasdesc: ffi::cudnnTensorDescriptor_t,
    activationdesc: ffi::cudnnActivationDescriptor_t,
    normmeanvardesc: ffi::cudnnTensorDescriptor_t,
    sizeinbytes: *mutusize,
    groupcnt: :: core :: ffi :: c_int
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnGetNormalizationForwardTrainingWorkspaceSize(handle, mode, normops, algo, xdesc, zdesc, ydesc, normscalebiasdesc, activationdesc, normmeanvardesc, sizeinbytes, groupcnt);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn get_normalization_backward_workspace_size(
    handle: ffi::cudnnHandle_t,
    mode: ffi::cudnnNormMode_t,
    normops: ffi::cudnnNormOps_t,
    algo: ffi::cudnnNormAlgo_t,
    xdesc: ffi::cudnnTensorDescriptor_t,
    ydesc: ffi::cudnnTensorDescriptor_t,
    dydesc: ffi::cudnnTensorDescriptor_t,
    dzdesc: ffi::cudnnTensorDescriptor_t,
    dxdesc: ffi::cudnnTensorDescriptor_t,
    dnormscalebiasdesc: ffi::cudnnTensorDescriptor_t,
    activationdesc: ffi::cudnnActivationDescriptor_t,
    normmeanvardesc: ffi::cudnnTensorDescriptor_t,
    sizeinbytes: *mutusize,
    groupcnt: :: core :: ffi :: c_int
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnGetNormalizationBackwardWorkspaceSize(handle, mode, normops, algo, xdesc, ydesc, dydesc, dzdesc, dxdesc, dnormscalebiasdesc, activationdesc, normmeanvardesc, sizeinbytes, groupcnt);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn get_normalization_training_reserve_space_size(
    handle: ffi::cudnnHandle_t,
    mode: ffi::cudnnNormMode_t,
    normops: ffi::cudnnNormOps_t,
    algo: ffi::cudnnNormAlgo_t,
    activationdesc: ffi::cudnnActivationDescriptor_t,
    xdesc: ffi::cudnnTensorDescriptor_t,
    sizeinbytes: *mutusize,
    groupcnt: :: core :: ffi :: c_int
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnGetNormalizationTrainingReserveSpaceSize(handle, mode, normops, algo, activationdesc, xdesc, sizeinbytes, groupcnt);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn normalization_forward_training(
    handle: ffi::cudnnHandle_t,
    mode: ffi::cudnnNormMode_t,
    normops: ffi::cudnnNormOps_t,
    algo: ffi::cudnnNormAlgo_t,
    alpha: *const :: core :: ffi ::c_void,
    beta: *const :: core :: ffi ::c_void,
    xdesc: ffi::cudnnTensorDescriptor_t,
    xdata: *const :: core :: ffi ::c_void,
    normscalebiasdesc: ffi::cudnnTensorDescriptor_t,
    normscale: *const :: core :: ffi ::c_void,
    normbias: *const :: core :: ffi ::c_void,
    exponentialaveragefactor: f64,
    normmeanvardesc: ffi::cudnnTensorDescriptor_t,
    resultrunningmean: *mut :: core :: ffi ::c_void,
    resultrunningvariance: *mut :: core :: ffi ::c_void,
    epsilon: f64,
    resultsavemean: *mut :: core :: ffi ::c_void,
    resultsaveinvvariance: *mut :: core :: ffi ::c_void,
    activationdesc: ffi::cudnnActivationDescriptor_t,
    zdesc: ffi::cudnnTensorDescriptor_t,
    zdata: *const :: core :: ffi ::c_void,
    ydesc: ffi::cudnnTensorDescriptor_t,
    ydata: *mut :: core :: ffi ::c_void,
    workspace: *mut :: core :: ffi ::c_void,
    workspacesizeinbytes: usize,
    reservespace: *mut :: core :: ffi ::c_void,
    reservespacesizeinbytes: usize,
    groupcnt: :: core :: ffi :: c_int
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnNormalizationForwardTraining(handle, mode, normops, algo, alpha, beta, xdesc, xdata, normscalebiasdesc, normscale, normbias, exponentialaveragefactor, normmeanvardesc, resultrunningmean, resultrunningvariance, epsilon, resultsavemean, resultsaveinvvariance, activationdesc, zdesc, zdata, ydesc, ydata, workspace, workspacesizeinbytes, reservespace, reservespacesizeinbytes, groupcnt);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn normalization_backward(
    handle: ffi::cudnnHandle_t,
    mode: ffi::cudnnNormMode_t,
    normops: ffi::cudnnNormOps_t,
    algo: ffi::cudnnNormAlgo_t,
    alphadatadiff: *const :: core :: ffi ::c_void,
    betadatadiff: *const :: core :: ffi ::c_void,
    alphaparamdiff: *const :: core :: ffi ::c_void,
    betaparamdiff: *const :: core :: ffi ::c_void,
    xdesc: ffi::cudnnTensorDescriptor_t,
    xdata: *const :: core :: ffi ::c_void,
    ydesc: ffi::cudnnTensorDescriptor_t,
    ydata: *const :: core :: ffi ::c_void,
    dydesc: ffi::cudnnTensorDescriptor_t,
    dydata: *const :: core :: ffi ::c_void,
    dzdesc: ffi::cudnnTensorDescriptor_t,
    dzdata: *mut :: core :: ffi ::c_void,
    dxdesc: ffi::cudnnTensorDescriptor_t,
    dxdata: *mut :: core :: ffi ::c_void,
    dnormscalebiasdesc: ffi::cudnnTensorDescriptor_t,
    normscaledata: *const :: core :: ffi ::c_void,
    normbiasdata: *const :: core :: ffi ::c_void,
    dnormscaledata: *mut :: core :: ffi ::c_void,
    dnormbiasdata: *mut :: core :: ffi ::c_void,
    epsilon: f64,
    normmeanvardesc: ffi::cudnnTensorDescriptor_t,
    savedmean: *const :: core :: ffi ::c_void,
    savedinvvariance: *const :: core :: ffi ::c_void,
    activationdesc: ffi::cudnnActivationDescriptor_t,
    workspace: *mut :: core :: ffi ::c_void,
    workspacesizeinbytes: usize,
    reservespace: *mut :: core :: ffi ::c_void,
    reservespacesizeinbytes: usize,
    groupcnt: :: core :: ffi :: c_int
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnNormalizationBackward(handle, mode, normops, algo, alphadatadiff, betadatadiff, alphaparamdiff, betaparamdiff, xdesc, xdata, ydesc, ydata, dydesc, dydata, dzdesc, dzdata, dxdesc, dxdata, dnormscalebiasdesc, normscaledata, normbiasdata, dnormscaledata, dnormbiasdata, epsilon, normmeanvardesc, savedmean, savedinvvariance, activationdesc, workspace, workspacesizeinbytes, reservespace, reservespacesizeinbytes, groupcnt);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn spatial_tf_sampler_backward(
    handle: ffi::cudnnHandle_t,
    stdesc: ffi::cudnnSpatialTransformerDescriptor_t,
    alpha: *const :: core :: ffi ::c_void,
    xdesc: ffi::cudnnTensorDescriptor_t,
    x: *const :: core :: ffi ::c_void,
    beta: *const :: core :: ffi ::c_void,
    dxdesc: ffi::cudnnTensorDescriptor_t,
    dx: *mut :: core :: ffi ::c_void,
    alphadgrid: *const :: core :: ffi ::c_void,
    dydesc: ffi::cudnnTensorDescriptor_t,
    dy: *const :: core :: ffi ::c_void,
    grid: *const :: core :: ffi ::c_void,
    betadgrid: *const :: core :: ffi ::c_void,
    dgrid: *mut :: core :: ffi ::c_void
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnSpatialTfSamplerBackward(handle, stdesc, alpha, xdesc, x, beta, dxdesc, dx, alphadgrid, dydesc, dy, grid, betadgrid, dgrid);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn dropout_backward(
    handle: ffi::cudnnHandle_t,
    dropoutdesc: ffi::cudnnDropoutDescriptor_t,
    dydesc: ffi::cudnnTensorDescriptor_t,
    dy: *const :: core :: ffi ::c_void,
    dxdesc: ffi::cudnnTensorDescriptor_t,
    dx: *mut :: core :: ffi ::c_void,
    reservespace: *mut :: core :: ffi ::c_void,
    reservespacesizeinbytes: usize
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnDropoutBackward(handle, dropoutdesc, dydesc, dy, dxdesc, dx, reservespace, reservespacesizeinbytes);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn get_rnn_weight_params(
    handle: ffi::cudnnHandle_t,
    rnndesc: ffi::cudnnRNNDescriptor_t,
    pseudolayer: i32,
    weightspacesize: usize,
    weightspace: *const :: core :: ffi ::c_void,
    linlayerid: i32,
    mdesc: ffi::cudnnTensorDescriptor_t,
    maddr: *mut *mut :: core :: ffi ::c_void,
    bdesc: ffi::cudnnTensorDescriptor_t,
    baddr: *mut *mut :: core :: ffi ::c_void
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnGetRNNWeightParams(handle, rnndesc, pseudolayer, weightspacesize, weightspace, linlayerid, mdesc, maddr, bdesc, baddr);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn rnn_forward(
    handle: ffi::cudnnHandle_t,
    rnndesc: ffi::cudnnRNNDescriptor_t,
    fwdmode: ffi::cudnnForwardMode_t,
    devseqlengths: *consti32,
    xdesc: ffi::cudnnRNNDataDescriptor_t,
    x: *const :: core :: ffi ::c_void,
    ydesc: ffi::cudnnRNNDataDescriptor_t,
    y: *mut :: core :: ffi ::c_void,
    hdesc: ffi::cudnnTensorDescriptor_t,
    hx: *const :: core :: ffi ::c_void,
    hy: *mut :: core :: ffi ::c_void,
    cdesc: ffi::cudnnTensorDescriptor_t,
    cx: *const :: core :: ffi ::c_void,
    cy: *mut :: core :: ffi ::c_void,
    weightspacesize: usize,
    weightspace: *const :: core :: ffi ::c_void,
    workspacesize: usize,
    workspace: *mut :: core :: ffi ::c_void,
    reservespacesize: usize,
    reservespace: *mut :: core :: ffi ::c_void
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnRNNForward(handle, rnndesc, fwdmode, devseqlengths, xdesc, x, ydesc, y, hdesc, hx, hy, cdesc, cx, cy, weightspacesize, weightspace, workspacesize, workspace, reservespacesize, reservespace);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn get_multi_head_attn_weights(
    handle: ffi::cudnnHandle_t,
    attndesc: ffi::cudnnAttnDescriptor_t,
    wkind: ffi::cudnnMultiHeadAttnWeightKind_t,
    weightsizeinbytes: usize,
    weights: *const :: core :: ffi ::c_void,
    wdesc: ffi::cudnnTensorDescriptor_t,
    waddr: *mut *mut :: core :: ffi ::c_void
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnGetMultiHeadAttnWeights(handle, attndesc, wkind, weightsizeinbytes, weights, wdesc, waddr);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn rnn_backward_data_v_8(
    handle: ffi::cudnnHandle_t,
    rnndesc: ffi::cudnnRNNDescriptor_t,
    devseqlengths: *consti32,
    ydesc: ffi::cudnnRNNDataDescriptor_t,
    y: *const :: core :: ffi ::c_void,
    dy: *const :: core :: ffi ::c_void,
    xdesc: ffi::cudnnRNNDataDescriptor_t,
    dx: *mut :: core :: ffi ::c_void,
    hdesc: ffi::cudnnTensorDescriptor_t,
    hx: *const :: core :: ffi ::c_void,
    dhy: *const :: core :: ffi ::c_void,
    dhx: *mut :: core :: ffi ::c_void,
    cdesc: ffi::cudnnTensorDescriptor_t,
    cx: *const :: core :: ffi ::c_void,
    dcy: *const :: core :: ffi ::c_void,
    dcx: *mut :: core :: ffi ::c_void,
    weightspacesize: usize,
    weightspace: *const :: core :: ffi ::c_void,
    workspacesize: usize,
    workspace: *mut :: core :: ffi ::c_void,
    reservespacesize: usize,
    reservespace: *mut :: core :: ffi ::c_void
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnRNNBackwardData_v8(handle, rnndesc, devseqlengths, ydesc, y, dy, xdesc, dx, hdesc, hx, dhy, dhx, cdesc, cx, dcy, dcx, weightspacesize, weightspace, workspacesize, workspace, reservespacesize, reservespace);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn rnn_backward_weights_v_8(
    handle: ffi::cudnnHandle_t,
    rnndesc: ffi::cudnnRNNDescriptor_t,
    addgrad: ffi::cudnnWgradMode_t,
    devseqlengths: *consti32,
    xdesc: ffi::cudnnRNNDataDescriptor_t,
    x: *const :: core :: ffi ::c_void,
    hdesc: ffi::cudnnTensorDescriptor_t,
    hx: *const :: core :: ffi ::c_void,
    ydesc: ffi::cudnnRNNDataDescriptor_t,
    y: *const :: core :: ffi ::c_void,
    weightspacesize: usize,
    dweightspace: *mut :: core :: ffi ::c_void,
    workspacesize: usize,
    workspace: *mut :: core :: ffi ::c_void,
    reservespacesize: usize,
    reservespace: *mut :: core :: ffi ::c_void
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnRNNBackwardWeights_v8(handle, rnndesc, addgrad, devseqlengths, xdesc, x, hdesc, hx, ydesc, y, weightspacesize, dweightspace, workspacesize, workspace, reservespacesize, reservespace);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn ctc_loss(
    handle: ffi::cudnnHandle_t,
    probsdesc: ffi::cudnnTensorDescriptor_t,
    probs: *const :: core :: ffi ::c_void,
    hostlabels: *const :: core :: ffi ::c_int,
    hostlabellengths: *const :: core :: ffi ::c_int,
    hostinputlengths: *const :: core :: ffi ::c_int,
    costs: *mut :: core :: ffi ::c_void,
    gradientsdesc: ffi::cudnnTensorDescriptor_t,
    gradients: *mut :: core :: ffi ::c_void,
    algo: ffi::cudnnCTCLossAlgo_t,
    ctclossdesc: ffi::cudnnCTCLossDescriptor_t,
    workspace: *mut :: core :: ffi ::c_void,
    workspacesizeinbytes: usize
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnCTCLoss(handle, probsdesc, probs, hostlabels, hostlabellengths, hostinputlengths, costs, gradientsdesc, gradients, algo, ctclossdesc, workspace, workspacesizeinbytes);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn ctc_loss_v_8(
    handle: ffi::cudnnHandle_t,
    algo: ffi::cudnnCTCLossAlgo_t,
    ctclossdesc: ffi::cudnnCTCLossDescriptor_t,
    probsdesc: ffi::cudnnTensorDescriptor_t,
    probs: *const :: core :: ffi ::c_void,
    labels: *const :: core :: ffi ::c_int,
    labellengths: *const :: core :: ffi ::c_int,
    inputlengths: *const :: core :: ffi ::c_int,
    costs: *mut :: core :: ffi ::c_void,
    gradientsdesc: ffi::cudnnTensorDescriptor_t,
    gradients: *mut :: core :: ffi ::c_void,
    workspacesizeinbytes: usize,
    workspace: *mut :: core :: ffi ::c_void
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnCTCLoss_v8(handle, algo, ctclossdesc, probsdesc, probs, labels, labellengths, inputlengths, costs, gradientsdesc, gradients, workspacesizeinbytes, workspace);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn get_ctc_loss_workspace_size(
    handle: ffi::cudnnHandle_t,
    probsdesc: ffi::cudnnTensorDescriptor_t,
    gradientsdesc: ffi::cudnnTensorDescriptor_t,
    labels: *const :: core :: ffi ::c_int,
    labellengths: *const :: core :: ffi ::c_int,
    inputlengths: *const :: core :: ffi ::c_int,
    algo: ffi::cudnnCTCLossAlgo_t,
    ctclossdesc: ffi::cudnnCTCLossDescriptor_t,
    sizeinbytes: *mutusize
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnGetCTCLossWorkspaceSize(handle, probsdesc, gradientsdesc, labels, labellengths, inputlengths, algo, ctclossdesc, sizeinbytes);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn get_ctc_loss_workspace_size_v_8(
    handle: ffi::cudnnHandle_t,
    algo: ffi::cudnnCTCLossAlgo_t,
    ctclossdesc: ffi::cudnnCTCLossDescriptor_t,
    probsdesc: ffi::cudnnTensorDescriptor_t,
    gradientsdesc: ffi::cudnnTensorDescriptor_t,
    sizeinbytes: *mutusize
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnGetCTCLossWorkspaceSize_v8(handle, algo, ctclossdesc, probsdesc, gradientsdesc, sizeinbytes);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn get_convolution_2d_forward_output_dim(
    convdesc: ffi::cudnnConvolutionDescriptor_t,
    inputtensordesc: ffi::cudnnTensorDescriptor_t,
    filterdesc: ffi::cudnnFilterDescriptor_t,
    n: *mut :: core :: ffi ::c_int,
    c: *mut :: core :: ffi ::c_int,
    h: *mut :: core :: ffi ::c_int,
    w: *mut :: core :: ffi ::c_int
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnGetConvolution2dForwardOutputDim(convdesc, inputtensordesc, filterdesc, n, c, h, w);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn get_convolution_nd_forward_output_dim(
    convdesc: ffi::cudnnConvolutionDescriptor_t,
    inputtensordesc: ffi::cudnnTensorDescriptor_t,
    filterdesc: ffi::cudnnFilterDescriptor_t,
    nbdims: :: core :: ffi :: c_int,
    tensorouputdima: *mut :: core :: ffi ::c_int
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnGetConvolutionNdForwardOutputDim(convdesc, inputtensordesc, filterdesc, nbdims, tensorouputdima);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn get_convolution_forward_algorithm_v_7(
    handle: ffi::cudnnHandle_t,
    srcdesc: ffi::cudnnTensorDescriptor_t,
    filterdesc: ffi::cudnnFilterDescriptor_t,
    convdesc: ffi::cudnnConvolutionDescriptor_t,
    destdesc: ffi::cudnnTensorDescriptor_t,
    requestedalgocount: :: core :: ffi :: c_int,
    returnedalgocount: *mut :: core :: ffi ::c_int,
    perfresults: *mutffi::cudnnConvolutionFwdAlgoPerf_t
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnGetConvolutionForwardAlgorithm_v7(handle, srcdesc, filterdesc, convdesc, destdesc, requestedalgocount, returnedalgocount, perfresults);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn find_convolution_forward_algorithm(
    handle: ffi::cudnnHandle_t,
    xdesc: ffi::cudnnTensorDescriptor_t,
    wdesc: ffi::cudnnFilterDescriptor_t,
    convdesc: ffi::cudnnConvolutionDescriptor_t,
    ydesc: ffi::cudnnTensorDescriptor_t,
    requestedalgocount: :: core :: ffi :: c_int,
    returnedalgocount: *mut :: core :: ffi ::c_int,
    perfresults: *mutffi::cudnnConvolutionFwdAlgoPerf_t
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnFindConvolutionForwardAlgorithm(handle, xdesc, wdesc, convdesc, ydesc, requestedalgocount, returnedalgocount, perfresults);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn find_convolution_forward_algorithm_ex(
    handle: ffi::cudnnHandle_t,
    xdesc: ffi::cudnnTensorDescriptor_t,
    x: *const :: core :: ffi ::c_void,
    wdesc: ffi::cudnnFilterDescriptor_t,
    w: *const :: core :: ffi ::c_void,
    convdesc: ffi::cudnnConvolutionDescriptor_t,
    ydesc: ffi::cudnnTensorDescriptor_t,
    y: *mut :: core :: ffi ::c_void,
    requestedalgocount: :: core :: ffi :: c_int,
    returnedalgocount: *mut :: core :: ffi ::c_int,
    perfresults: *mutffi::cudnnConvolutionFwdAlgoPerf_t,
    workspace: *mut :: core :: ffi ::c_void,
    workspacesizeinbytes: usize
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnFindConvolutionForwardAlgorithmEx(handle, xdesc, x, wdesc, w, convdesc, ydesc, y, requestedalgocount, returnedalgocount, perfresults, workspace, workspacesizeinbytes);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn im_2col(
    handle: ffi::cudnnHandle_t,
    xdesc: ffi::cudnnTensorDescriptor_t,
    x: *const :: core :: ffi ::c_void,
    wdesc: ffi::cudnnFilterDescriptor_t,
    convdesc: ffi::cudnnConvolutionDescriptor_t,
    colbuffer: *mut :: core :: ffi ::c_void
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnIm2Col(handle, xdesc, x, wdesc, convdesc, colbuffer);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn get_convolution_forward_workspace_size(
    handle: ffi::cudnnHandle_t,
    xdesc: ffi::cudnnTensorDescriptor_t,
    wdesc: ffi::cudnnFilterDescriptor_t,
    convdesc: ffi::cudnnConvolutionDescriptor_t,
    ydesc: ffi::cudnnTensorDescriptor_t,
    algo: ffi::cudnnConvolutionFwdAlgo_t,
    sizeinbytes: *mutusize
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnGetConvolutionForwardWorkspaceSize(handle, xdesc, wdesc, convdesc, ydesc, algo, sizeinbytes);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn convolution_forward(
    handle: ffi::cudnnHandle_t,
    alpha: *const :: core :: ffi ::c_void,
    xdesc: ffi::cudnnTensorDescriptor_t,
    x: *const :: core :: ffi ::c_void,
    wdesc: ffi::cudnnFilterDescriptor_t,
    w: *const :: core :: ffi ::c_void,
    convdesc: ffi::cudnnConvolutionDescriptor_t,
    algo: ffi::cudnnConvolutionFwdAlgo_t,
    workspace: *mut :: core :: ffi ::c_void,
    workspacesizeinbytes: usize,
    beta: *const :: core :: ffi ::c_void,
    ydesc: ffi::cudnnTensorDescriptor_t,
    y: *mut :: core :: ffi ::c_void
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnConvolutionForward(handle, alpha, xdesc, x, wdesc, w, convdesc, algo, workspace, workspacesizeinbytes, beta, ydesc, y);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn convolution_bias_activation_forward(
    handle: ffi::cudnnHandle_t,
    alpha1: *const :: core :: ffi ::c_void,
    xdesc: ffi::cudnnTensorDescriptor_t,
    x: *const :: core :: ffi ::c_void,
    wdesc: ffi::cudnnFilterDescriptor_t,
    w: *const :: core :: ffi ::c_void,
    convdesc: ffi::cudnnConvolutionDescriptor_t,
    algo: ffi::cudnnConvolutionFwdAlgo_t,
    workspace: *mut :: core :: ffi ::c_void,
    workspacesizeinbytes: usize,
    alpha2: *const :: core :: ffi ::c_void,
    zdesc: ffi::cudnnTensorDescriptor_t,
    z: *const :: core :: ffi ::c_void,
    biasdesc: ffi::cudnnTensorDescriptor_t,
    bias: *const :: core :: ffi ::c_void,
    activationdesc: ffi::cudnnActivationDescriptor_t,
    ydesc: ffi::cudnnTensorDescriptor_t,
    y: *mut :: core :: ffi ::c_void
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnConvolutionBiasActivationForward(handle, alpha1, xdesc, x, wdesc, w, convdesc, algo, workspace, workspacesizeinbytes, alpha2, zdesc, z, biasdesc, bias, activationdesc, ydesc, y);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn find_convolution_backward_data_algorithm(
    handle: ffi::cudnnHandle_t,
    wdesc: ffi::cudnnFilterDescriptor_t,
    dydesc: ffi::cudnnTensorDescriptor_t,
    convdesc: ffi::cudnnConvolutionDescriptor_t,
    dxdesc: ffi::cudnnTensorDescriptor_t,
    requestedalgocount: :: core :: ffi :: c_int,
    returnedalgocount: *mut :: core :: ffi ::c_int,
    perfresults: *mutffi::cudnnConvolutionBwdDataAlgoPerf_t
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnFindConvolutionBackwardDataAlgorithm(handle, wdesc, dydesc, convdesc, dxdesc, requestedalgocount, returnedalgocount, perfresults);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn find_convolution_backward_data_algorithm_ex(
    handle: ffi::cudnnHandle_t,
    wdesc: ffi::cudnnFilterDescriptor_t,
    w: *const :: core :: ffi ::c_void,
    dydesc: ffi::cudnnTensorDescriptor_t,
    dy: *const :: core :: ffi ::c_void,
    convdesc: ffi::cudnnConvolutionDescriptor_t,
    dxdesc: ffi::cudnnTensorDescriptor_t,
    dx: *mut :: core :: ffi ::c_void,
    requestedalgocount: :: core :: ffi :: c_int,
    returnedalgocount: *mut :: core :: ffi ::c_int,
    perfresults: *mutffi::cudnnConvolutionBwdDataAlgoPerf_t,
    workspace: *mut :: core :: ffi ::c_void,
    workspacesizeinbytes: usize
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnFindConvolutionBackwardDataAlgorithmEx(handle, wdesc, w, dydesc, dy, convdesc, dxdesc, dx, requestedalgocount, returnedalgocount, perfresults, workspace, workspacesizeinbytes);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn get_convolution_backward_data_algorithm_v_7(
    handle: ffi::cudnnHandle_t,
    filterdesc: ffi::cudnnFilterDescriptor_t,
    diffdesc: ffi::cudnnTensorDescriptor_t,
    convdesc: ffi::cudnnConvolutionDescriptor_t,
    graddesc: ffi::cudnnTensorDescriptor_t,
    requestedalgocount: :: core :: ffi :: c_int,
    returnedalgocount: *mut :: core :: ffi ::c_int,
    perfresults: *mutffi::cudnnConvolutionBwdDataAlgoPerf_t
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnGetConvolutionBackwardDataAlgorithm_v7(handle, filterdesc, diffdesc, convdesc, graddesc, requestedalgocount, returnedalgocount, perfresults);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn get_convolution_backward_data_workspace_size(
    handle: ffi::cudnnHandle_t,
    wdesc: ffi::cudnnFilterDescriptor_t,
    dydesc: ffi::cudnnTensorDescriptor_t,
    convdesc: ffi::cudnnConvolutionDescriptor_t,
    dxdesc: ffi::cudnnTensorDescriptor_t,
    algo: ffi::cudnnConvolutionBwdDataAlgo_t,
    sizeinbytes: *mutusize
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnGetConvolutionBackwardDataWorkspaceSize(handle, wdesc, dydesc, convdesc, dxdesc, algo, sizeinbytes);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn convolution_backward_data(
    handle: ffi::cudnnHandle_t,
    alpha: *const :: core :: ffi ::c_void,
    wdesc: ffi::cudnnFilterDescriptor_t,
    w: *const :: core :: ffi ::c_void,
    dydesc: ffi::cudnnTensorDescriptor_t,
    dy: *const :: core :: ffi ::c_void,
    convdesc: ffi::cudnnConvolutionDescriptor_t,
    algo: ffi::cudnnConvolutionBwdDataAlgo_t,
    workspace: *mut :: core :: ffi ::c_void,
    workspacesizeinbytes: usize,
    beta: *const :: core :: ffi ::c_void,
    dxdesc: ffi::cudnnTensorDescriptor_t,
    dx: *mut :: core :: ffi ::c_void
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnConvolutionBackwardData(handle, alpha, wdesc, w, dydesc, dy, convdesc, algo, workspace, workspacesizeinbytes, beta, dxdesc, dx);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn get_folded_conv_backward_data_descriptors(
    handle: ffi::cudnnHandle_t,
    filterdesc: ffi::cudnnFilterDescriptor_t,
    diffdesc: ffi::cudnnTensorDescriptor_t,
    convdesc: ffi::cudnnConvolutionDescriptor_t,
    graddesc: ffi::cudnnTensorDescriptor_t,
    transformformat: ffi::cudnnTensorFormat_t,
    foldedfilterdesc: ffi::cudnnFilterDescriptor_t,
    paddeddiffdesc: ffi::cudnnTensorDescriptor_t,
    foldedconvdesc: ffi::cudnnConvolutionDescriptor_t,
    foldedgraddesc: ffi::cudnnTensorDescriptor_t,
    filterfoldtransdesc: ffi::cudnnTensorTransformDescriptor_t,
    diffpadtransdesc: ffi::cudnnTensorTransformDescriptor_t,
    gradfoldtransdesc: ffi::cudnnTensorTransformDescriptor_t,
    gradunfoldtransdesc: ffi::cudnnTensorTransformDescriptor_t
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnGetFoldedConvBackwardDataDescriptors(handle, filterdesc, diffdesc, convdesc, graddesc, transformformat, foldedfilterdesc, paddeddiffdesc, foldedconvdesc, foldedgraddesc, filterfoldtransdesc, diffpadtransdesc, gradfoldtransdesc, gradunfoldtransdesc);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn find_convolution_backward_filter_algorithm(
    handle: ffi::cudnnHandle_t,
    xdesc: ffi::cudnnTensorDescriptor_t,
    dydesc: ffi::cudnnTensorDescriptor_t,
    convdesc: ffi::cudnnConvolutionDescriptor_t,
    dwdesc: ffi::cudnnFilterDescriptor_t,
    requestedalgocount: :: core :: ffi :: c_int,
    returnedalgocount: *mut :: core :: ffi ::c_int,
    perfresults: *mutffi::cudnnConvolutionBwdFilterAlgoPerf_t
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnFindConvolutionBackwardFilterAlgorithm(handle, xdesc, dydesc, convdesc, dwdesc, requestedalgocount, returnedalgocount, perfresults);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn find_convolution_backward_filter_algorithm_ex(
    handle: ffi::cudnnHandle_t,
    xdesc: ffi::cudnnTensorDescriptor_t,
    x: *const :: core :: ffi ::c_void,
    dydesc: ffi::cudnnTensorDescriptor_t,
    y: *const :: core :: ffi ::c_void,
    convdesc: ffi::cudnnConvolutionDescriptor_t,
    dwdesc: ffi::cudnnFilterDescriptor_t,
    dw: *mut :: core :: ffi ::c_void,
    requestedalgocount: :: core :: ffi :: c_int,
    returnedalgocount: *mut :: core :: ffi ::c_int,
    perfresults: *mutffi::cudnnConvolutionBwdFilterAlgoPerf_t,
    workspace: *mut :: core :: ffi ::c_void,
    workspacesizeinbytes: usize
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnFindConvolutionBackwardFilterAlgorithmEx(handle, xdesc, x, dydesc, y, convdesc, dwdesc, dw, requestedalgocount, returnedalgocount, perfresults, workspace, workspacesizeinbytes);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn get_convolution_backward_filter_algorithm_v_7(
    handle: ffi::cudnnHandle_t,
    srcdesc: ffi::cudnnTensorDescriptor_t,
    diffdesc: ffi::cudnnTensorDescriptor_t,
    convdesc: ffi::cudnnConvolutionDescriptor_t,
    graddesc: ffi::cudnnFilterDescriptor_t,
    requestedalgocount: :: core :: ffi :: c_int,
    returnedalgocount: *mut :: core :: ffi ::c_int,
    perfresults: *mutffi::cudnnConvolutionBwdFilterAlgoPerf_t
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnGetConvolutionBackwardFilterAlgorithm_v7(handle, srcdesc, diffdesc, convdesc, graddesc, requestedalgocount, returnedalgocount, perfresults);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn get_convolution_backward_filter_workspace_size(
    handle: ffi::cudnnHandle_t,
    xdesc: ffi::cudnnTensorDescriptor_t,
    dydesc: ffi::cudnnTensorDescriptor_t,
    convdesc: ffi::cudnnConvolutionDescriptor_t,
    graddesc: ffi::cudnnFilterDescriptor_t,
    algo: ffi::cudnnConvolutionBwdFilterAlgo_t,
    sizeinbytes: *mutusize
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnGetConvolutionBackwardFilterWorkspaceSize(handle, xdesc, dydesc, convdesc, graddesc, algo, sizeinbytes);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn convolution_backward_filter(
    handle: ffi::cudnnHandle_t,
    alpha: *const :: core :: ffi ::c_void,
    xdesc: ffi::cudnnTensorDescriptor_t,
    x: *const :: core :: ffi ::c_void,
    dydesc: ffi::cudnnTensorDescriptor_t,
    dy: *const :: core :: ffi ::c_void,
    convdesc: ffi::cudnnConvolutionDescriptor_t,
    algo: ffi::cudnnConvolutionBwdFilterAlgo_t,
    workspace: *mut :: core :: ffi ::c_void,
    workspacesizeinbytes: usize,
    beta: *const :: core :: ffi ::c_void,
    dwdesc: ffi::cudnnFilterDescriptor_t,
    dw: *mut :: core :: ffi ::c_void
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnConvolutionBackwardFilter(handle, alpha, xdesc, x, dydesc, dy, convdesc, algo, workspace, workspacesizeinbytes, beta, dwdesc, dw);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn convolution_backward_bias(
    handle: ffi::cudnnHandle_t,
    alpha: *const :: core :: ffi ::c_void,
    dydesc: ffi::cudnnTensorDescriptor_t,
    dy: *const :: core :: ffi ::c_void,
    beta: *const :: core :: ffi ::c_void,
    dbdesc: ffi::cudnnTensorDescriptor_t,
    db: *mut :: core :: ffi ::c_void
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnConvolutionBackwardBias(handle, alpha, dydesc, dy, beta, dbdesc, db);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

}

// Additional methods for cudnnCTCLossDescriptor_t
impl CudnnCTCLossDescriptor {
    #[inline]
    pub fn set_ctc_loss_descriptor(
        &mut self,
        comptype: ffi::cudnnDataType_t,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnSetCTCLossDescriptor").entered();
            let result = ffi::cudnnSetCTCLossDescriptor(self.handle, comptype);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn set_ctc_loss_descriptor_ex(
        &mut self,
        comptype: ffi::cudnnDataType_t,
        normmode: ffi::cudnnLossNormalizationMode_t,
        gradmode: ffi::cudnnNanPropagation_t,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnSetCTCLossDescriptorEx").entered();
            let result = ffi::cudnnSetCTCLossDescriptorEx(self.handle, comptype, normmode, gradmode);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn set_ctc_loss_descriptor_v_8(
        &mut self,
        comptype: ffi::cudnnDataType_t,
        normmode: ffi::cudnnLossNormalizationMode_t,
        gradmode: ffi::cudnnNanPropagation_t,
        maxlabellength: :: core :: ffi :: c_int,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnSetCTCLossDescriptor_v8").entered();
            let result = ffi::cudnnSetCTCLossDescriptor_v8(self.handle, comptype, normmode, gradmode, maxlabellength);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn set_ctc_loss_descriptor_v_9(
        &mut self,
        comptype: ffi::cudnnDataType_t,
        normmode: ffi::cudnnLossNormalizationMode_t,
        ctcgradmode: ffi::cudnnCTCGradMode_t,
        maxlabellength: :: core :: ffi :: c_int,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnSetCTCLossDescriptor_v9").entered();
            let result = ffi::cudnnSetCTCLossDescriptor_v9(self.handle, comptype, normmode, ctcgradmode, maxlabellength);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn get_ctc_loss_descriptor(
        &mut self,
        comptype: *mutffi::cudnnDataType_t,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if comptype.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if comptype.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnGetCTCLossDescriptor").entered();
            let result = ffi::cudnnGetCTCLossDescriptor(self.handle, comptype);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn get_ctc_loss_descriptor_ex(
        &mut self,
        comptype: *mutffi::cudnnDataType_t,
        normmode: *mutffi::cudnnLossNormalizationMode_t,
        gradmode: *mutffi::cudnnNanPropagation_t,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if comptype.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if comptype.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if normmode.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if normmode.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if gradmode.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if gradmode.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnGetCTCLossDescriptorEx").entered();
            let result = ffi::cudnnGetCTCLossDescriptorEx(self.handle, comptype, normmode, gradmode);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn get_ctc_loss_descriptor_v_8(
        &mut self,
        comptype: *mutffi::cudnnDataType_t,
        normmode: *mutffi::cudnnLossNormalizationMode_t,
        gradmode: *mutffi::cudnnNanPropagation_t,
        maxlabellength: *mut :: core :: ffi ::c_int,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if comptype.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if comptype.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if normmode.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if normmode.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if gradmode.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if gradmode.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if maxlabellength.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if maxlabellength.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnGetCTCLossDescriptor_v8").entered();
            let result = ffi::cudnnGetCTCLossDescriptor_v8(self.handle, comptype, normmode, gradmode, maxlabellength);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn get_ctc_loss_descriptor_v_9(
        &mut self,
        comptype: *mutffi::cudnnDataType_t,
        normmode: *mutffi::cudnnLossNormalizationMode_t,
        ctcgradmode: *mutffi::cudnnCTCGradMode_t,
        maxlabellength: *mut :: core :: ffi ::c_int,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if comptype.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if comptype.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if normmode.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if normmode.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if ctcgradmode.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if ctcgradmode.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if maxlabellength.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if maxlabellength.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnGetCTCLossDescriptor_v9").entered();
            let result = ffi::cudnnGetCTCLossDescriptor_v9(self.handle, comptype, normmode, ctcgradmode, maxlabellength);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

pub fn ctc_loss(
    handle: ffi::cudnnHandle_t,
    probsdesc: ffi::cudnnTensorDescriptor_t,
    probs: *const :: core :: ffi ::c_void,
    hostlabels: *const :: core :: ffi ::c_int,
    hostlabellengths: *const :: core :: ffi ::c_int,
    hostinputlengths: *const :: core :: ffi ::c_int,
    costs: *mut :: core :: ffi ::c_void,
    gradientsdesc: ffi::cudnnTensorDescriptor_t,
    gradients: *mut :: core :: ffi ::c_void,
    algo: ffi::cudnnCTCLossAlgo_t,
    ctclossdesc: ffi::cudnnCTCLossDescriptor_t,
    workspace: *mut :: core :: ffi ::c_void,
    workspacesizeinbytes: usize
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnCTCLoss(handle, probsdesc, probs, hostlabels, hostlabellengths, hostinputlengths, costs, gradientsdesc, gradients, algo, ctclossdesc, workspace, workspacesizeinbytes);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn ctc_loss_v_8(
    handle: ffi::cudnnHandle_t,
    algo: ffi::cudnnCTCLossAlgo_t,
    ctclossdesc: ffi::cudnnCTCLossDescriptor_t,
    probsdesc: ffi::cudnnTensorDescriptor_t,
    probs: *const :: core :: ffi ::c_void,
    labels: *const :: core :: ffi ::c_int,
    labellengths: *const :: core :: ffi ::c_int,
    inputlengths: *const :: core :: ffi ::c_int,
    costs: *mut :: core :: ffi ::c_void,
    gradientsdesc: ffi::cudnnTensorDescriptor_t,
    gradients: *mut :: core :: ffi ::c_void,
    workspacesizeinbytes: usize,
    workspace: *mut :: core :: ffi ::c_void
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnCTCLoss_v8(handle, algo, ctclossdesc, probsdesc, probs, labels, labellengths, inputlengths, costs, gradientsdesc, gradients, workspacesizeinbytes, workspace);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn get_ctc_loss_workspace_size(
    handle: ffi::cudnnHandle_t,
    probsdesc: ffi::cudnnTensorDescriptor_t,
    gradientsdesc: ffi::cudnnTensorDescriptor_t,
    labels: *const :: core :: ffi ::c_int,
    labellengths: *const :: core :: ffi ::c_int,
    inputlengths: *const :: core :: ffi ::c_int,
    algo: ffi::cudnnCTCLossAlgo_t,
    ctclossdesc: ffi::cudnnCTCLossDescriptor_t,
    sizeinbytes: *mutusize
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnGetCTCLossWorkspaceSize(handle, probsdesc, gradientsdesc, labels, labellengths, inputlengths, algo, ctclossdesc, sizeinbytes);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn get_ctc_loss_workspace_size_v_8(
    handle: ffi::cudnnHandle_t,
    algo: ffi::cudnnCTCLossAlgo_t,
    ctclossdesc: ffi::cudnnCTCLossDescriptor_t,
    probsdesc: ffi::cudnnTensorDescriptor_t,
    gradientsdesc: ffi::cudnnTensorDescriptor_t,
    sizeinbytes: *mutusize
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnGetCTCLossWorkspaceSize_v8(handle, algo, ctclossdesc, probsdesc, gradientsdesc, sizeinbytes);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

}

// Additional methods for cudaStreamCallback_t
impl CudaStreamCallback {
pub fn stream_add_callback(
    stream: ffi::cudaStream_t,
    callback: ffi::cudaStreamCallback_t,
    userdata: *mut :: core :: ffi ::c_void,
    flags: :: core :: ffi :: c_uint
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaStreamAddCallback(stream, callback, userdata, flags);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

}

// Additional methods for cudaAsyncCallback
impl CudaAsyncCallback {
pub fn device_register_async_notification(
    device: :: core :: ffi :: c_int,
    callbackfunc: ffi::cudaAsyncCallback,
    userdata: *mut :: core :: ffi ::c_void,
    callback: *mutffi::cudaAsyncCallbackHandle_t
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaDeviceRegisterAsyncNotification(device, callbackfunc, userdata, callback);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn device_unregister_async_notification(
    device: :: core :: ffi :: c_int,
    callback: ffi::cudaAsyncCallbackHandle_t
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaDeviceUnregisterAsyncNotification(device, callback);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

}

// Additional methods for cudaLogsCallbackHandle
impl CudaLogsCallbackHandle {
pub fn logs_register_callback(
    callbackfunc: ffi::cudaLogsCallback_t,
    userdata: *mut :: core :: ffi ::c_void,
    callback_out: *mutffi::cudaLogsCallbackHandle
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaLogsRegisterCallback(callbackfunc, userdata, callback_out);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

    #[inline]
    pub fn logs_unregister_callback(
        &mut self,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudaLogsUnregisterCallback").entered();
            let result = ffi::cudaLogsUnregisterCallback(self.handle);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

}

// Additional methods for cudnnConvolutionDescriptor_t
impl CudnnConvolutionDescriptor {
    #[inline]
    pub fn set_convolution_math_type(
        &mut self,
        mathtype: ffi::cudnnMathType_t,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnSetConvolutionMathType").entered();
            let result = ffi::cudnnSetConvolutionMathType(self.handle, mathtype);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn get_convolution_math_type(
        &mut self,
        mathtype: *mutffi::cudnnMathType_t,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if mathtype.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if mathtype.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnGetConvolutionMathType").entered();
            let result = ffi::cudnnGetConvolutionMathType(self.handle, mathtype);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn set_convolution_group_count(
        &mut self,
        groupcount: :: core :: ffi :: c_int,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnSetConvolutionGroupCount").entered();
            let result = ffi::cudnnSetConvolutionGroupCount(self.handle, groupcount);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn get_convolution_group_count(
        &mut self,
        groupcount: *mut :: core :: ffi ::c_int,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if groupcount.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if groupcount.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnGetConvolutionGroupCount").entered();
            let result = ffi::cudnnGetConvolutionGroupCount(self.handle, groupcount);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn set_convolution_reorder_type(
        &mut self,
        reordertype: ffi::cudnnReorderType_t,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnSetConvolutionReorderType").entered();
            let result = ffi::cudnnSetConvolutionReorderType(self.handle, reordertype);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn get_convolution_reorder_type(
        &mut self,
        reordertype: *mutffi::cudnnReorderType_t,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if reordertype.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if reordertype.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnGetConvolutionReorderType").entered();
            let result = ffi::cudnnGetConvolutionReorderType(self.handle, reordertype);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn set_convolution_2d_descriptor(
        &mut self,
        pad_h: :: core :: ffi :: c_int,
        pad_w: :: core :: ffi :: c_int,
        u: :: core :: ffi :: c_int,
        v: :: core :: ffi :: c_int,
        dilation_h: :: core :: ffi :: c_int,
        dilation_w: :: core :: ffi :: c_int,
        mode: ffi::cudnnConvolutionMode_t,
        computetype: ffi::cudnnDataType_t,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnSetConvolution2dDescriptor").entered();
            let result = ffi::cudnnSetConvolution2dDescriptor(self.handle, pad_h, pad_w, u, v, dilation_h, dilation_w, mode, computetype);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn get_convolution_2d_descriptor(
        &mut self,
        pad_h: *mut :: core :: ffi ::c_int,
        pad_w: *mut :: core :: ffi ::c_int,
        u: *mut :: core :: ffi ::c_int,
        v: *mut :: core :: ffi ::c_int,
        dilation_h: *mut :: core :: ffi ::c_int,
        dilation_w: *mut :: core :: ffi ::c_int,
        mode: *mutffi::cudnnConvolutionMode_t,
        computetype: *mutffi::cudnnDataType_t,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if pad_h.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if pad_h.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if pad_w.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if pad_w.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if u.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if u.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if v.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if v.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if dilation_h.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if dilation_h.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if dilation_w.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if dilation_w.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if mode.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if mode.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if computetype.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if computetype.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnGetConvolution2dDescriptor").entered();
            let result = ffi::cudnnGetConvolution2dDescriptor(self.handle, pad_h, pad_w, u, v, dilation_h, dilation_w, mode, computetype);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn set_convolution_nd_descriptor(
        &mut self,
        arraylength: :: core :: ffi :: c_int,
        pada: *const :: core :: ffi ::c_int,
        filterstridea: *const :: core :: ffi ::c_int,
        dilationa: *const :: core :: ffi ::c_int,
        mode: ffi::cudnnConvolutionMode_t,
        computetype: ffi::cudnnDataType_t,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if pada.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if pada.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if filterstridea.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if filterstridea.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if dilationa.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if dilationa.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnSetConvolutionNdDescriptor").entered();
            let result = ffi::cudnnSetConvolutionNdDescriptor(self.handle, arraylength, pada, filterstridea, dilationa, mode, computetype);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn get_convolution_nd_descriptor(
        &mut self,
        arraylengthrequested: :: core :: ffi :: c_int,
        arraylength: *mut :: core :: ffi ::c_int,
        pada: *mut :: core :: ffi ::c_int,
        stridea: *mut :: core :: ffi ::c_int,
        dilationa: *mut :: core :: ffi ::c_int,
        mode: *mutffi::cudnnConvolutionMode_t,
        computetype: *mutffi::cudnnDataType_t,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if arraylength.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if arraylength.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if pada.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if pada.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if stridea.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if stridea.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if dilationa.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if dilationa.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if mode.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if mode.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if computetype.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if computetype.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnGetConvolutionNdDescriptor").entered();
            let result = ffi::cudnnGetConvolutionNdDescriptor(self.handle, arraylengthrequested, arraylength, pada, stridea, dilationa, mode, computetype);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn get_convolution_2d_forward_output_dim(
        &mut self,
        inputtensordesc: ffi::cudnnTensorDescriptor_t,
        filterdesc: ffi::cudnnFilterDescriptor_t,
        n: *mut :: core :: ffi ::c_int,
        c: *mut :: core :: ffi ::c_int,
        h: *mut :: core :: ffi ::c_int,
        w: *mut :: core :: ffi ::c_int,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if n.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if n.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if c.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if c.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if h.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if h.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if w.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if w.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnGetConvolution2dForwardOutputDim").entered();
            let result = ffi::cudnnGetConvolution2dForwardOutputDim(self.handle, inputtensordesc, filterdesc, n, c, h, w);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn get_convolution_nd_forward_output_dim(
        &mut self,
        inputtensordesc: ffi::cudnnTensorDescriptor_t,
        filterdesc: ffi::cudnnFilterDescriptor_t,
        nbdims: :: core :: ffi :: c_int,
        tensorouputdima: *mut :: core :: ffi ::c_int,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if tensorouputdima.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if tensorouputdima.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnGetConvolutionNdForwardOutputDim").entered();
            let result = ffi::cudnnGetConvolutionNdForwardOutputDim(self.handle, inputtensordesc, filterdesc, nbdims, tensorouputdima);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

pub fn get_convolution_forward_algorithm_v_7(
    handle: ffi::cudnnHandle_t,
    srcdesc: ffi::cudnnTensorDescriptor_t,
    filterdesc: ffi::cudnnFilterDescriptor_t,
    convdesc: ffi::cudnnConvolutionDescriptor_t,
    destdesc: ffi::cudnnTensorDescriptor_t,
    requestedalgocount: :: core :: ffi :: c_int,
    returnedalgocount: *mut :: core :: ffi ::c_int,
    perfresults: *mutffi::cudnnConvolutionFwdAlgoPerf_t
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnGetConvolutionForwardAlgorithm_v7(handle, srcdesc, filterdesc, convdesc, destdesc, requestedalgocount, returnedalgocount, perfresults);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn find_convolution_forward_algorithm(
    handle: ffi::cudnnHandle_t,
    xdesc: ffi::cudnnTensorDescriptor_t,
    wdesc: ffi::cudnnFilterDescriptor_t,
    convdesc: ffi::cudnnConvolutionDescriptor_t,
    ydesc: ffi::cudnnTensorDescriptor_t,
    requestedalgocount: :: core :: ffi :: c_int,
    returnedalgocount: *mut :: core :: ffi ::c_int,
    perfresults: *mutffi::cudnnConvolutionFwdAlgoPerf_t
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnFindConvolutionForwardAlgorithm(handle, xdesc, wdesc, convdesc, ydesc, requestedalgocount, returnedalgocount, perfresults);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn find_convolution_forward_algorithm_ex(
    handle: ffi::cudnnHandle_t,
    xdesc: ffi::cudnnTensorDescriptor_t,
    x: *const :: core :: ffi ::c_void,
    wdesc: ffi::cudnnFilterDescriptor_t,
    w: *const :: core :: ffi ::c_void,
    convdesc: ffi::cudnnConvolutionDescriptor_t,
    ydesc: ffi::cudnnTensorDescriptor_t,
    y: *mut :: core :: ffi ::c_void,
    requestedalgocount: :: core :: ffi :: c_int,
    returnedalgocount: *mut :: core :: ffi ::c_int,
    perfresults: *mutffi::cudnnConvolutionFwdAlgoPerf_t,
    workspace: *mut :: core :: ffi ::c_void,
    workspacesizeinbytes: usize
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnFindConvolutionForwardAlgorithmEx(handle, xdesc, x, wdesc, w, convdesc, ydesc, y, requestedalgocount, returnedalgocount, perfresults, workspace, workspacesizeinbytes);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn im_2col(
    handle: ffi::cudnnHandle_t,
    xdesc: ffi::cudnnTensorDescriptor_t,
    x: *const :: core :: ffi ::c_void,
    wdesc: ffi::cudnnFilterDescriptor_t,
    convdesc: ffi::cudnnConvolutionDescriptor_t,
    colbuffer: *mut :: core :: ffi ::c_void
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnIm2Col(handle, xdesc, x, wdesc, convdesc, colbuffer);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn get_convolution_forward_workspace_size(
    handle: ffi::cudnnHandle_t,
    xdesc: ffi::cudnnTensorDescriptor_t,
    wdesc: ffi::cudnnFilterDescriptor_t,
    convdesc: ffi::cudnnConvolutionDescriptor_t,
    ydesc: ffi::cudnnTensorDescriptor_t,
    algo: ffi::cudnnConvolutionFwdAlgo_t,
    sizeinbytes: *mutusize
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnGetConvolutionForwardWorkspaceSize(handle, xdesc, wdesc, convdesc, ydesc, algo, sizeinbytes);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn convolution_forward(
    handle: ffi::cudnnHandle_t,
    alpha: *const :: core :: ffi ::c_void,
    xdesc: ffi::cudnnTensorDescriptor_t,
    x: *const :: core :: ffi ::c_void,
    wdesc: ffi::cudnnFilterDescriptor_t,
    w: *const :: core :: ffi ::c_void,
    convdesc: ffi::cudnnConvolutionDescriptor_t,
    algo: ffi::cudnnConvolutionFwdAlgo_t,
    workspace: *mut :: core :: ffi ::c_void,
    workspacesizeinbytes: usize,
    beta: *const :: core :: ffi ::c_void,
    ydesc: ffi::cudnnTensorDescriptor_t,
    y: *mut :: core :: ffi ::c_void
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnConvolutionForward(handle, alpha, xdesc, x, wdesc, w, convdesc, algo, workspace, workspacesizeinbytes, beta, ydesc, y);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn convolution_bias_activation_forward(
    handle: ffi::cudnnHandle_t,
    alpha1: *const :: core :: ffi ::c_void,
    xdesc: ffi::cudnnTensorDescriptor_t,
    x: *const :: core :: ffi ::c_void,
    wdesc: ffi::cudnnFilterDescriptor_t,
    w: *const :: core :: ffi ::c_void,
    convdesc: ffi::cudnnConvolutionDescriptor_t,
    algo: ffi::cudnnConvolutionFwdAlgo_t,
    workspace: *mut :: core :: ffi ::c_void,
    workspacesizeinbytes: usize,
    alpha2: *const :: core :: ffi ::c_void,
    zdesc: ffi::cudnnTensorDescriptor_t,
    z: *const :: core :: ffi ::c_void,
    biasdesc: ffi::cudnnTensorDescriptor_t,
    bias: *const :: core :: ffi ::c_void,
    activationdesc: ffi::cudnnActivationDescriptor_t,
    ydesc: ffi::cudnnTensorDescriptor_t,
    y: *mut :: core :: ffi ::c_void
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnConvolutionBiasActivationForward(handle, alpha1, xdesc, x, wdesc, w, convdesc, algo, workspace, workspacesizeinbytes, alpha2, zdesc, z, biasdesc, bias, activationdesc, ydesc, y);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn find_convolution_backward_data_algorithm(
    handle: ffi::cudnnHandle_t,
    wdesc: ffi::cudnnFilterDescriptor_t,
    dydesc: ffi::cudnnTensorDescriptor_t,
    convdesc: ffi::cudnnConvolutionDescriptor_t,
    dxdesc: ffi::cudnnTensorDescriptor_t,
    requestedalgocount: :: core :: ffi :: c_int,
    returnedalgocount: *mut :: core :: ffi ::c_int,
    perfresults: *mutffi::cudnnConvolutionBwdDataAlgoPerf_t
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnFindConvolutionBackwardDataAlgorithm(handle, wdesc, dydesc, convdesc, dxdesc, requestedalgocount, returnedalgocount, perfresults);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn find_convolution_backward_data_algorithm_ex(
    handle: ffi::cudnnHandle_t,
    wdesc: ffi::cudnnFilterDescriptor_t,
    w: *const :: core :: ffi ::c_void,
    dydesc: ffi::cudnnTensorDescriptor_t,
    dy: *const :: core :: ffi ::c_void,
    convdesc: ffi::cudnnConvolutionDescriptor_t,
    dxdesc: ffi::cudnnTensorDescriptor_t,
    dx: *mut :: core :: ffi ::c_void,
    requestedalgocount: :: core :: ffi :: c_int,
    returnedalgocount: *mut :: core :: ffi ::c_int,
    perfresults: *mutffi::cudnnConvolutionBwdDataAlgoPerf_t,
    workspace: *mut :: core :: ffi ::c_void,
    workspacesizeinbytes: usize
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnFindConvolutionBackwardDataAlgorithmEx(handle, wdesc, w, dydesc, dy, convdesc, dxdesc, dx, requestedalgocount, returnedalgocount, perfresults, workspace, workspacesizeinbytes);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn get_convolution_backward_data_algorithm_v_7(
    handle: ffi::cudnnHandle_t,
    filterdesc: ffi::cudnnFilterDescriptor_t,
    diffdesc: ffi::cudnnTensorDescriptor_t,
    convdesc: ffi::cudnnConvolutionDescriptor_t,
    graddesc: ffi::cudnnTensorDescriptor_t,
    requestedalgocount: :: core :: ffi :: c_int,
    returnedalgocount: *mut :: core :: ffi ::c_int,
    perfresults: *mutffi::cudnnConvolutionBwdDataAlgoPerf_t
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnGetConvolutionBackwardDataAlgorithm_v7(handle, filterdesc, diffdesc, convdesc, graddesc, requestedalgocount, returnedalgocount, perfresults);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn get_convolution_backward_data_workspace_size(
    handle: ffi::cudnnHandle_t,
    wdesc: ffi::cudnnFilterDescriptor_t,
    dydesc: ffi::cudnnTensorDescriptor_t,
    convdesc: ffi::cudnnConvolutionDescriptor_t,
    dxdesc: ffi::cudnnTensorDescriptor_t,
    algo: ffi::cudnnConvolutionBwdDataAlgo_t,
    sizeinbytes: *mutusize
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnGetConvolutionBackwardDataWorkspaceSize(handle, wdesc, dydesc, convdesc, dxdesc, algo, sizeinbytes);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn convolution_backward_data(
    handle: ffi::cudnnHandle_t,
    alpha: *const :: core :: ffi ::c_void,
    wdesc: ffi::cudnnFilterDescriptor_t,
    w: *const :: core :: ffi ::c_void,
    dydesc: ffi::cudnnTensorDescriptor_t,
    dy: *const :: core :: ffi ::c_void,
    convdesc: ffi::cudnnConvolutionDescriptor_t,
    algo: ffi::cudnnConvolutionBwdDataAlgo_t,
    workspace: *mut :: core :: ffi ::c_void,
    workspacesizeinbytes: usize,
    beta: *const :: core :: ffi ::c_void,
    dxdesc: ffi::cudnnTensorDescriptor_t,
    dx: *mut :: core :: ffi ::c_void
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnConvolutionBackwardData(handle, alpha, wdesc, w, dydesc, dy, convdesc, algo, workspace, workspacesizeinbytes, beta, dxdesc, dx);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn get_folded_conv_backward_data_descriptors(
    handle: ffi::cudnnHandle_t,
    filterdesc: ffi::cudnnFilterDescriptor_t,
    diffdesc: ffi::cudnnTensorDescriptor_t,
    convdesc: ffi::cudnnConvolutionDescriptor_t,
    graddesc: ffi::cudnnTensorDescriptor_t,
    transformformat: ffi::cudnnTensorFormat_t,
    foldedfilterdesc: ffi::cudnnFilterDescriptor_t,
    paddeddiffdesc: ffi::cudnnTensorDescriptor_t,
    foldedconvdesc: ffi::cudnnConvolutionDescriptor_t,
    foldedgraddesc: ffi::cudnnTensorDescriptor_t,
    filterfoldtransdesc: ffi::cudnnTensorTransformDescriptor_t,
    diffpadtransdesc: ffi::cudnnTensorTransformDescriptor_t,
    gradfoldtransdesc: ffi::cudnnTensorTransformDescriptor_t,
    gradunfoldtransdesc: ffi::cudnnTensorTransformDescriptor_t
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnGetFoldedConvBackwardDataDescriptors(handle, filterdesc, diffdesc, convdesc, graddesc, transformformat, foldedfilterdesc, paddeddiffdesc, foldedconvdesc, foldedgraddesc, filterfoldtransdesc, diffpadtransdesc, gradfoldtransdesc, gradunfoldtransdesc);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn find_convolution_backward_filter_algorithm(
    handle: ffi::cudnnHandle_t,
    xdesc: ffi::cudnnTensorDescriptor_t,
    dydesc: ffi::cudnnTensorDescriptor_t,
    convdesc: ffi::cudnnConvolutionDescriptor_t,
    dwdesc: ffi::cudnnFilterDescriptor_t,
    requestedalgocount: :: core :: ffi :: c_int,
    returnedalgocount: *mut :: core :: ffi ::c_int,
    perfresults: *mutffi::cudnnConvolutionBwdFilterAlgoPerf_t
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnFindConvolutionBackwardFilterAlgorithm(handle, xdesc, dydesc, convdesc, dwdesc, requestedalgocount, returnedalgocount, perfresults);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn find_convolution_backward_filter_algorithm_ex(
    handle: ffi::cudnnHandle_t,
    xdesc: ffi::cudnnTensorDescriptor_t,
    x: *const :: core :: ffi ::c_void,
    dydesc: ffi::cudnnTensorDescriptor_t,
    y: *const :: core :: ffi ::c_void,
    convdesc: ffi::cudnnConvolutionDescriptor_t,
    dwdesc: ffi::cudnnFilterDescriptor_t,
    dw: *mut :: core :: ffi ::c_void,
    requestedalgocount: :: core :: ffi :: c_int,
    returnedalgocount: *mut :: core :: ffi ::c_int,
    perfresults: *mutffi::cudnnConvolutionBwdFilterAlgoPerf_t,
    workspace: *mut :: core :: ffi ::c_void,
    workspacesizeinbytes: usize
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnFindConvolutionBackwardFilterAlgorithmEx(handle, xdesc, x, dydesc, y, convdesc, dwdesc, dw, requestedalgocount, returnedalgocount, perfresults, workspace, workspacesizeinbytes);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn get_convolution_backward_filter_algorithm_v_7(
    handle: ffi::cudnnHandle_t,
    srcdesc: ffi::cudnnTensorDescriptor_t,
    diffdesc: ffi::cudnnTensorDescriptor_t,
    convdesc: ffi::cudnnConvolutionDescriptor_t,
    graddesc: ffi::cudnnFilterDescriptor_t,
    requestedalgocount: :: core :: ffi :: c_int,
    returnedalgocount: *mut :: core :: ffi ::c_int,
    perfresults: *mutffi::cudnnConvolutionBwdFilterAlgoPerf_t
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnGetConvolutionBackwardFilterAlgorithm_v7(handle, srcdesc, diffdesc, convdesc, graddesc, requestedalgocount, returnedalgocount, perfresults);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn get_convolution_backward_filter_workspace_size(
    handle: ffi::cudnnHandle_t,
    xdesc: ffi::cudnnTensorDescriptor_t,
    dydesc: ffi::cudnnTensorDescriptor_t,
    convdesc: ffi::cudnnConvolutionDescriptor_t,
    graddesc: ffi::cudnnFilterDescriptor_t,
    algo: ffi::cudnnConvolutionBwdFilterAlgo_t,
    sizeinbytes: *mutusize
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnGetConvolutionBackwardFilterWorkspaceSize(handle, xdesc, dydesc, convdesc, graddesc, algo, sizeinbytes);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn convolution_backward_filter(
    handle: ffi::cudnnHandle_t,
    alpha: *const :: core :: ffi ::c_void,
    xdesc: ffi::cudnnTensorDescriptor_t,
    x: *const :: core :: ffi ::c_void,
    dydesc: ffi::cudnnTensorDescriptor_t,
    dy: *const :: core :: ffi ::c_void,
    convdesc: ffi::cudnnConvolutionDescriptor_t,
    algo: ffi::cudnnConvolutionBwdFilterAlgo_t,
    workspace: *mut :: core :: ffi ::c_void,
    workspacesizeinbytes: usize,
    beta: *const :: core :: ffi ::c_void,
    dwdesc: ffi::cudnnFilterDescriptor_t,
    dw: *mut :: core :: ffi ::c_void
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnConvolutionBackwardFilter(handle, alpha, xdesc, x, dydesc, dy, convdesc, algo, workspace, workspacesizeinbytes, beta, dwdesc, dw);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

}

// Additional methods for cudnnHandle_t
impl CudnnHandle {
    #[inline]
    pub fn query_runtime_error(
        &mut self,
        rstatus: *mutffi::cudnnStatus_t,
        mode: ffi::cudnnErrQueryMode_t,
        tag: *mutffi::cudnnRuntimeTag_t,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if rstatus.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if rstatus.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if tag.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if tag.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnQueryRuntimeError").entered();
            let result = ffi::cudnnQueryRuntimeError(self.handle, rstatus, mode, tag);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn set_stream(
        &mut self,
        streamid: ffi::cudaStream_t,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnSetStream").entered();
            let result = ffi::cudnnSetStream(self.handle, streamid);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn get_stream(
        &mut self,
        streamid: *mutffi::cudaStream_t,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if streamid.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if streamid.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnGetStream").entered();
            let result = ffi::cudnnGetStream(self.handle, streamid);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn backend_execute(
        &mut self,
        executionplan: ffi::cudnnBackendDescriptor_t,
        variantpack: ffi::cudnnBackendDescriptor_t,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnBackendExecute").entered();
            let result = ffi::cudnnBackendExecute(self.handle, executionplan, variantpack);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn backend_populate_cuda_graph(
        &mut self,
        executionplan: ffi::cudnnBackendDescriptor_t,
        variantpack: ffi::cudnnBackendDescriptor_t,
        graph: ffi::cudaGraph_t,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnBackendPopulateCudaGraph").entered();
            let result = ffi::cudnnBackendPopulateCudaGraph(self.handle, executionplan, variantpack, graph);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn backend_update_cuda_graph(
        &mut self,
        executionplan: ffi::cudnnBackendDescriptor_t,
        variantpack: ffi::cudnnBackendDescriptor_t,
        graph: ffi::cudaGraph_t,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnBackendUpdateCudaGraph").entered();
            let result = ffi::cudnnBackendUpdateCudaGraph(self.handle, executionplan, variantpack, graph);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn transform_tensor(
        &mut self,
        alpha: *const :: core :: ffi ::c_void,
        xdesc: ffi::cudnnTensorDescriptor_t,
        x: *const :: core :: ffi ::c_void,
        beta: *const :: core :: ffi ::c_void,
        ydesc: ffi::cudnnTensorDescriptor_t,
        y: *mut :: core :: ffi ::c_void,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if alpha.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if alpha.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if x.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if x.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if beta.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if beta.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if y.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if y.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnTransformTensor").entered();
            let result = ffi::cudnnTransformTensor(self.handle, alpha, xdesc, x, beta, ydesc, y);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn transform_tensor_ex(
        &mut self,
        transdesc: ffi::cudnnTensorTransformDescriptor_t,
        alpha: *const :: core :: ffi ::c_void,
        srcdesc: ffi::cudnnTensorDescriptor_t,
        srcdata: *const :: core :: ffi ::c_void,
        beta: *const :: core :: ffi ::c_void,
        destdesc: ffi::cudnnTensorDescriptor_t,
        destdata: *mut :: core :: ffi ::c_void,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if alpha.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if alpha.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if srcdata.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if srcdata.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if beta.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if beta.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if destdata.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if destdata.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnTransformTensorEx").entered();
            let result = ffi::cudnnTransformTensorEx(self.handle, transdesc, alpha, srcdesc, srcdata, beta, destdesc, destdata);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn add_tensor(
        &mut self,
        alpha: *const :: core :: ffi ::c_void,
        adesc: ffi::cudnnTensorDescriptor_t,
        a: *const :: core :: ffi ::c_void,
        beta: *const :: core :: ffi ::c_void,
        cdesc: ffi::cudnnTensorDescriptor_t,
        c: *mut :: core :: ffi ::c_void,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if alpha.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if alpha.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if a.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if a.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if beta.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if beta.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if c.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if c.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnAddTensor").entered();
            let result = ffi::cudnnAddTensor(self.handle, alpha, adesc, a, beta, cdesc, c);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn op_tensor(
        &mut self,
        optensordesc: ffi::cudnnOpTensorDescriptor_t,
        alpha1: *const :: core :: ffi ::c_void,
        adesc: ffi::cudnnTensorDescriptor_t,
        a: *const :: core :: ffi ::c_void,
        alpha2: *const :: core :: ffi ::c_void,
        bdesc: ffi::cudnnTensorDescriptor_t,
        b: *const :: core :: ffi ::c_void,
        beta: *const :: core :: ffi ::c_void,
        cdesc: ffi::cudnnTensorDescriptor_t,
        c: *mut :: core :: ffi ::c_void,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if alpha1.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if alpha1.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if a.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if a.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if alpha2.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if alpha2.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if b.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if b.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if beta.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if beta.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if c.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if c.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnOpTensor").entered();
            let result = ffi::cudnnOpTensor(self.handle, optensordesc, alpha1, adesc, a, alpha2, bdesc, b, beta, cdesc, c);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn get_reduction_indices_size(
        &mut self,
        reducetensordesc: ffi::cudnnReduceTensorDescriptor_t,
        adesc: ffi::cudnnTensorDescriptor_t,
        cdesc: ffi::cudnnTensorDescriptor_t,
        sizeinbytes: *mutusize,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if sizeinbytes.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if sizeinbytes.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnGetReductionIndicesSize").entered();
            let result = ffi::cudnnGetReductionIndicesSize(self.handle, reducetensordesc, adesc, cdesc, sizeinbytes);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn get_reduction_workspace_size(
        &mut self,
        reducetensordesc: ffi::cudnnReduceTensorDescriptor_t,
        adesc: ffi::cudnnTensorDescriptor_t,
        cdesc: ffi::cudnnTensorDescriptor_t,
        sizeinbytes: *mutusize,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if sizeinbytes.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if sizeinbytes.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnGetReductionWorkspaceSize").entered();
            let result = ffi::cudnnGetReductionWorkspaceSize(self.handle, reducetensordesc, adesc, cdesc, sizeinbytes);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn reduce_tensor(
        &mut self,
        reducetensordesc: ffi::cudnnReduceTensorDescriptor_t,
        indices: *mut :: core :: ffi ::c_void,
        indicessizeinbytes: usize,
        workspace: *mut :: core :: ffi ::c_void,
        workspacesizeinbytes: usize,
        alpha: *const :: core :: ffi ::c_void,
        adesc: ffi::cudnnTensorDescriptor_t,
        a: *const :: core :: ffi ::c_void,
        beta: *const :: core :: ffi ::c_void,
        cdesc: ffi::cudnnTensorDescriptor_t,
        c: *mut :: core :: ffi ::c_void,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if indices.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if indices.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if workspace.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if workspace.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if alpha.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if alpha.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if a.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if a.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if beta.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if beta.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if c.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if c.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnReduceTensor").entered();
            let result = ffi::cudnnReduceTensor(self.handle, reducetensordesc, indices, indicessizeinbytes, workspace, workspacesizeinbytes, alpha, adesc, a, beta, cdesc, c);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn set_tensor(
        &mut self,
        ydesc: ffi::cudnnTensorDescriptor_t,
        y: *mut :: core :: ffi ::c_void,
        valueptr: *const :: core :: ffi ::c_void,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if y.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if y.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if valueptr.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if valueptr.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnSetTensor").entered();
            let result = ffi::cudnnSetTensor(self.handle, ydesc, y, valueptr);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn scale_tensor(
        &mut self,
        ydesc: ffi::cudnnTensorDescriptor_t,
        y: *mut :: core :: ffi ::c_void,
        alpha: *const :: core :: ffi ::c_void,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if y.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if y.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if alpha.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if alpha.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnScaleTensor").entered();
            let result = ffi::cudnnScaleTensor(self.handle, ydesc, y, alpha);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn transform_filter(
        &mut self,
        transdesc: ffi::cudnnTensorTransformDescriptor_t,
        alpha: *const :: core :: ffi ::c_void,
        srcdesc: ffi::cudnnFilterDescriptor_t,
        srcdata: *const :: core :: ffi ::c_void,
        beta: *const :: core :: ffi ::c_void,
        destdesc: ffi::cudnnFilterDescriptor_t,
        destdata: *mut :: core :: ffi ::c_void,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if alpha.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if alpha.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if srcdata.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if srcdata.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if beta.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if beta.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if destdata.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if destdata.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnTransformFilter").entered();
            let result = ffi::cudnnTransformFilter(self.handle, transdesc, alpha, srcdesc, srcdata, beta, destdesc, destdata);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn softmax_forward(
        &mut self,
        algo: ffi::cudnnSoftmaxAlgorithm_t,
        mode: ffi::cudnnSoftmaxMode_t,
        alpha: *const :: core :: ffi ::c_void,
        xdesc: ffi::cudnnTensorDescriptor_t,
        x: *const :: core :: ffi ::c_void,
        beta: *const :: core :: ffi ::c_void,
        ydesc: ffi::cudnnTensorDescriptor_t,
        y: *mut :: core :: ffi ::c_void,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if alpha.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if alpha.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if x.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if x.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if beta.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if beta.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if y.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if y.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnSoftmaxForward").entered();
            let result = ffi::cudnnSoftmaxForward(self.handle, algo, mode, alpha, xdesc, x, beta, ydesc, y);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn pooling_forward(
        &mut self,
        poolingdesc: ffi::cudnnPoolingDescriptor_t,
        alpha: *const :: core :: ffi ::c_void,
        xdesc: ffi::cudnnTensorDescriptor_t,
        x: *const :: core :: ffi ::c_void,
        beta: *const :: core :: ffi ::c_void,
        ydesc: ffi::cudnnTensorDescriptor_t,
        y: *mut :: core :: ffi ::c_void,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if alpha.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if alpha.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if x.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if x.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if beta.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if beta.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if y.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if y.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnPoolingForward").entered();
            let result = ffi::cudnnPoolingForward(self.handle, poolingdesc, alpha, xdesc, x, beta, ydesc, y);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn activation_forward(
        &mut self,
        activationdesc: ffi::cudnnActivationDescriptor_t,
        alpha: *const :: core :: ffi ::c_void,
        xdesc: ffi::cudnnTensorDescriptor_t,
        x: *const :: core :: ffi ::c_void,
        beta: *const :: core :: ffi ::c_void,
        ydesc: ffi::cudnnTensorDescriptor_t,
        y: *mut :: core :: ffi ::c_void,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if alpha.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if alpha.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if x.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if x.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if beta.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if beta.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if y.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if y.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnActivationForward").entered();
            let result = ffi::cudnnActivationForward(self.handle, activationdesc, alpha, xdesc, x, beta, ydesc, y);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn lrn_cross_channel_forward(
        &mut self,
        normdesc: ffi::cudnnLRNDescriptor_t,
        lrnmode: ffi::cudnnLRNMode_t,
        alpha: *const :: core :: ffi ::c_void,
        xdesc: ffi::cudnnTensorDescriptor_t,
        x: *const :: core :: ffi ::c_void,
        beta: *const :: core :: ffi ::c_void,
        ydesc: ffi::cudnnTensorDescriptor_t,
        y: *mut :: core :: ffi ::c_void,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if alpha.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if alpha.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if x.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if x.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if beta.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if beta.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if y.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if y.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnLRNCrossChannelForward").entered();
            let result = ffi::cudnnLRNCrossChannelForward(self.handle, normdesc, lrnmode, alpha, xdesc, x, beta, ydesc, y);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn divisive_normalization_forward(
        &mut self,
        normdesc: ffi::cudnnLRNDescriptor_t,
        mode: ffi::cudnnDivNormMode_t,
        alpha: *const :: core :: ffi ::c_void,
        xdesc: ffi::cudnnTensorDescriptor_t,
        x: *const :: core :: ffi ::c_void,
        means: *const :: core :: ffi ::c_void,
        temp: *mut :: core :: ffi ::c_void,
        temp2: *mut :: core :: ffi ::c_void,
        beta: *const :: core :: ffi ::c_void,
        ydesc: ffi::cudnnTensorDescriptor_t,
        y: *mut :: core :: ffi ::c_void,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if alpha.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if alpha.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if x.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if x.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if means.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if means.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if temp.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if temp.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if temp2.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if temp2.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if beta.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if beta.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if y.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if y.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnDivisiveNormalizationForward").entered();
            let result = ffi::cudnnDivisiveNormalizationForward(self.handle, normdesc, mode, alpha, xdesc, x, means, temp, temp2, beta, ydesc, y);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn batch_normalization_forward_inference(
        &mut self,
        mode: ffi::cudnnBatchNormMode_t,
        alpha: *const :: core :: ffi ::c_void,
        beta: *const :: core :: ffi ::c_void,
        xdesc: ffi::cudnnTensorDescriptor_t,
        x: *const :: core :: ffi ::c_void,
        ydesc: ffi::cudnnTensorDescriptor_t,
        y: *mut :: core :: ffi ::c_void,
        bnscalebiasmeanvardesc: ffi::cudnnTensorDescriptor_t,
        bnscale: *const :: core :: ffi ::c_void,
        bnbias: *const :: core :: ffi ::c_void,
        estimatedmean: *const :: core :: ffi ::c_void,
        estimatedvariance: *const :: core :: ffi ::c_void,
        epsilon: f64,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if alpha.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if alpha.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if beta.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if beta.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if x.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if x.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if y.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if y.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if bnscale.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if bnscale.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if bnbias.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if bnbias.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if estimatedmean.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if estimatedmean.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if estimatedvariance.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if estimatedvariance.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnBatchNormalizationForwardInference").entered();
            let result = ffi::cudnnBatchNormalizationForwardInference(self.handle, mode, alpha, beta, xdesc, x, ydesc, y, bnscalebiasmeanvardesc, bnscale, bnbias, estimatedmean, estimatedvariance, epsilon);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn normalization_forward_inference(
        &mut self,
        mode: ffi::cudnnNormMode_t,
        normops: ffi::cudnnNormOps_t,
        algo: ffi::cudnnNormAlgo_t,
        alpha: *const :: core :: ffi ::c_void,
        beta: *const :: core :: ffi ::c_void,
        xdesc: ffi::cudnnTensorDescriptor_t,
        x: *const :: core :: ffi ::c_void,
        normscalebiasdesc: ffi::cudnnTensorDescriptor_t,
        normscale: *const :: core :: ffi ::c_void,
        normbias: *const :: core :: ffi ::c_void,
        normmeanvardesc: ffi::cudnnTensorDescriptor_t,
        estimatedmean: *const :: core :: ffi ::c_void,
        estimatedvariance: *const :: core :: ffi ::c_void,
        zdesc: ffi::cudnnTensorDescriptor_t,
        z: *const :: core :: ffi ::c_void,
        activationdesc: ffi::cudnnActivationDescriptor_t,
        ydesc: ffi::cudnnTensorDescriptor_t,
        y: *mut :: core :: ffi ::c_void,
        epsilon: f64,
        groupcnt: :: core :: ffi :: c_int,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if alpha.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if alpha.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if beta.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if beta.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if x.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if x.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if normscale.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if normscale.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if normbias.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if normbias.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if estimatedmean.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if estimatedmean.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if estimatedvariance.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if estimatedvariance.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if z.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if z.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if y.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if y.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnNormalizationForwardInference").entered();
            let result = ffi::cudnnNormalizationForwardInference(self.handle, mode, normops, algo, alpha, beta, xdesc, x, normscalebiasdesc, normscale, normbias, normmeanvardesc, estimatedmean, estimatedvariance, zdesc, z, activationdesc, ydesc, y, epsilon, groupcnt);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn spatial_tf_grid_generator_forward(
        &mut self,
        stdesc: ffi::cudnnSpatialTransformerDescriptor_t,
        theta: *const :: core :: ffi ::c_void,
        grid: *mut :: core :: ffi ::c_void,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if theta.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if theta.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if grid.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if grid.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnSpatialTfGridGeneratorForward").entered();
            let result = ffi::cudnnSpatialTfGridGeneratorForward(self.handle, stdesc, theta, grid);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn spatial_tf_sampler_forward(
        &mut self,
        stdesc: ffi::cudnnSpatialTransformerDescriptor_t,
        alpha: *const :: core :: ffi ::c_void,
        xdesc: ffi::cudnnTensorDescriptor_t,
        x: *const :: core :: ffi ::c_void,
        grid: *const :: core :: ffi ::c_void,
        beta: *const :: core :: ffi ::c_void,
        ydesc: ffi::cudnnTensorDescriptor_t,
        y: *mut :: core :: ffi ::c_void,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if alpha.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if alpha.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if x.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if x.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if grid.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if grid.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if beta.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if beta.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if y.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if y.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnSpatialTfSamplerForward").entered();
            let result = ffi::cudnnSpatialTfSamplerForward(self.handle, stdesc, alpha, xdesc, x, grid, beta, ydesc, y);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn dropout_get_states_size(
        &mut self,
        sizeinbytes: *mutusize,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if sizeinbytes.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if sizeinbytes.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnDropoutGetStatesSize").entered();
            let result = ffi::cudnnDropoutGetStatesSize(self.handle, sizeinbytes);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

pub fn set_dropout_descriptor(
    dropoutdesc: ffi::cudnnDropoutDescriptor_t,
    handle: ffi::cudnnHandle_t,
    dropout: f32,
    states: *mut :: core :: ffi ::c_void,
    statesizeinbytes: usize,
    seed: :: core :: ffi :: c_ulonglong
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnSetDropoutDescriptor(dropoutdesc, handle, dropout, states, statesizeinbytes, seed);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn restore_dropout_descriptor(
    dropoutdesc: ffi::cudnnDropoutDescriptor_t,
    handle: ffi::cudnnHandle_t,
    dropout: f32,
    states: *mut :: core :: ffi ::c_void,
    statesizeinbytes: usize,
    seed: :: core :: ffi :: c_ulonglong
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnRestoreDropoutDescriptor(dropoutdesc, handle, dropout, states, statesizeinbytes, seed);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn get_dropout_descriptor(
    dropoutdesc: ffi::cudnnDropoutDescriptor_t,
    handle: ffi::cudnnHandle_t,
    dropout: *mutf32,
    states: *mut *mut :: core :: ffi ::c_void,
    seed: *mut :: core :: ffi ::c_ulonglong
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnGetDropoutDescriptor(dropoutdesc, handle, dropout, states, seed);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

    #[inline]
    pub fn dropout_forward(
        &mut self,
        dropoutdesc: ffi::cudnnDropoutDescriptor_t,
        xdesc: ffi::cudnnTensorDescriptor_t,
        x: *const :: core :: ffi ::c_void,
        ydesc: ffi::cudnnTensorDescriptor_t,
        y: *mut :: core :: ffi ::c_void,
        reservespace: *mut :: core :: ffi ::c_void,
        reservespacesizeinbytes: usize,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if x.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if x.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if y.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if y.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if reservespace.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if reservespace.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnDropoutForward").entered();
            let result = ffi::cudnnDropoutForward(self.handle, dropoutdesc, xdesc, x, ydesc, y, reservespace, reservespacesizeinbytes);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn softmax_backward(
        &mut self,
        algo: ffi::cudnnSoftmaxAlgorithm_t,
        mode: ffi::cudnnSoftmaxMode_t,
        alpha: *const :: core :: ffi ::c_void,
        ydesc: ffi::cudnnTensorDescriptor_t,
        y: *const :: core :: ffi ::c_void,
        dydesc: ffi::cudnnTensorDescriptor_t,
        dy: *const :: core :: ffi ::c_void,
        beta: *const :: core :: ffi ::c_void,
        dxdesc: ffi::cudnnTensorDescriptor_t,
        dx: *mut :: core :: ffi ::c_void,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if alpha.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if alpha.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if y.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if y.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if dy.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if dy.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if beta.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if beta.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if dx.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if dx.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnSoftmaxBackward").entered();
            let result = ffi::cudnnSoftmaxBackward(self.handle, algo, mode, alpha, ydesc, y, dydesc, dy, beta, dxdesc, dx);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn pooling_backward(
        &mut self,
        poolingdesc: ffi::cudnnPoolingDescriptor_t,
        alpha: *const :: core :: ffi ::c_void,
        ydesc: ffi::cudnnTensorDescriptor_t,
        y: *const :: core :: ffi ::c_void,
        dydesc: ffi::cudnnTensorDescriptor_t,
        dy: *const :: core :: ffi ::c_void,
        xdesc: ffi::cudnnTensorDescriptor_t,
        x: *const :: core :: ffi ::c_void,
        beta: *const :: core :: ffi ::c_void,
        dxdesc: ffi::cudnnTensorDescriptor_t,
        dx: *mut :: core :: ffi ::c_void,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if alpha.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if alpha.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if y.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if y.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if dy.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if dy.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if x.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if x.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if beta.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if beta.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if dx.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if dx.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnPoolingBackward").entered();
            let result = ffi::cudnnPoolingBackward(self.handle, poolingdesc, alpha, ydesc, y, dydesc, dy, xdesc, x, beta, dxdesc, dx);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn activation_backward(
        &mut self,
        activationdesc: ffi::cudnnActivationDescriptor_t,
        alpha: *const :: core :: ffi ::c_void,
        ydesc: ffi::cudnnTensorDescriptor_t,
        y: *const :: core :: ffi ::c_void,
        dydesc: ffi::cudnnTensorDescriptor_t,
        dy: *const :: core :: ffi ::c_void,
        xdesc: ffi::cudnnTensorDescriptor_t,
        x: *const :: core :: ffi ::c_void,
        beta: *const :: core :: ffi ::c_void,
        dxdesc: ffi::cudnnTensorDescriptor_t,
        dx: *mut :: core :: ffi ::c_void,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if alpha.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if alpha.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if y.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if y.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if dy.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if dy.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if x.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if x.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if beta.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if beta.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if dx.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if dx.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnActivationBackward").entered();
            let result = ffi::cudnnActivationBackward(self.handle, activationdesc, alpha, ydesc, y, dydesc, dy, xdesc, x, beta, dxdesc, dx);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn lrn_cross_channel_backward(
        &mut self,
        normdesc: ffi::cudnnLRNDescriptor_t,
        lrnmode: ffi::cudnnLRNMode_t,
        alpha: *const :: core :: ffi ::c_void,
        ydesc: ffi::cudnnTensorDescriptor_t,
        y: *const :: core :: ffi ::c_void,
        dydesc: ffi::cudnnTensorDescriptor_t,
        dy: *const :: core :: ffi ::c_void,
        xdesc: ffi::cudnnTensorDescriptor_t,
        x: *const :: core :: ffi ::c_void,
        beta: *const :: core :: ffi ::c_void,
        dxdesc: ffi::cudnnTensorDescriptor_t,
        dx: *mut :: core :: ffi ::c_void,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if alpha.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if alpha.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if y.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if y.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if dy.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if dy.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if x.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if x.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if beta.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if beta.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if dx.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if dx.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnLRNCrossChannelBackward").entered();
            let result = ffi::cudnnLRNCrossChannelBackward(self.handle, normdesc, lrnmode, alpha, ydesc, y, dydesc, dy, xdesc, x, beta, dxdesc, dx);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn divisive_normalization_backward(
        &mut self,
        normdesc: ffi::cudnnLRNDescriptor_t,
        mode: ffi::cudnnDivNormMode_t,
        alpha: *const :: core :: ffi ::c_void,
        xdesc: ffi::cudnnTensorDescriptor_t,
        x: *const :: core :: ffi ::c_void,
        means: *const :: core :: ffi ::c_void,
        dy: *const :: core :: ffi ::c_void,
        temp: *mut :: core :: ffi ::c_void,
        temp2: *mut :: core :: ffi ::c_void,
        beta: *const :: core :: ffi ::c_void,
        dxdmeansdesc: ffi::cudnnTensorDescriptor_t,
        dx: *mut :: core :: ffi ::c_void,
        dmeans: *mut :: core :: ffi ::c_void,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if alpha.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if alpha.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if x.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if x.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if means.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if means.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if dy.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if dy.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if temp.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if temp.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if temp2.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if temp2.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if beta.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if beta.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if dx.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if dx.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if dmeans.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if dmeans.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnDivisiveNormalizationBackward").entered();
            let result = ffi::cudnnDivisiveNormalizationBackward(self.handle, normdesc, mode, alpha, xdesc, x, means, dy, temp, temp2, beta, dxdmeansdesc, dx, dmeans);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn get_batch_normalization_forward_training_ex_workspace_size(
        &mut self,
        mode: ffi::cudnnBatchNormMode_t,
        bnops: ffi::cudnnBatchNormOps_t,
        xdesc: ffi::cudnnTensorDescriptor_t,
        zdesc: ffi::cudnnTensorDescriptor_t,
        ydesc: ffi::cudnnTensorDescriptor_t,
        bnscalebiasmeanvardesc: ffi::cudnnTensorDescriptor_t,
        activationdesc: ffi::cudnnActivationDescriptor_t,
        sizeinbytes: *mutusize,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if sizeinbytes.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if sizeinbytes.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnGetBatchNormalizationForwardTrainingExWorkspaceSize").entered();
            let result = ffi::cudnnGetBatchNormalizationForwardTrainingExWorkspaceSize(self.handle, mode, bnops, xdesc, zdesc, ydesc, bnscalebiasmeanvardesc, activationdesc, sizeinbytes);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn get_batch_normalization_backward_ex_workspace_size(
        &mut self,
        mode: ffi::cudnnBatchNormMode_t,
        bnops: ffi::cudnnBatchNormOps_t,
        xdesc: ffi::cudnnTensorDescriptor_t,
        ydesc: ffi::cudnnTensorDescriptor_t,
        dydesc: ffi::cudnnTensorDescriptor_t,
        dzdesc: ffi::cudnnTensorDescriptor_t,
        dxdesc: ffi::cudnnTensorDescriptor_t,
        dbnscalebiasdesc: ffi::cudnnTensorDescriptor_t,
        activationdesc: ffi::cudnnActivationDescriptor_t,
        sizeinbytes: *mutusize,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if sizeinbytes.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if sizeinbytes.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnGetBatchNormalizationBackwardExWorkspaceSize").entered();
            let result = ffi::cudnnGetBatchNormalizationBackwardExWorkspaceSize(self.handle, mode, bnops, xdesc, ydesc, dydesc, dzdesc, dxdesc, dbnscalebiasdesc, activationdesc, sizeinbytes);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn get_batch_normalization_training_ex_reserve_space_size(
        &mut self,
        mode: ffi::cudnnBatchNormMode_t,
        bnops: ffi::cudnnBatchNormOps_t,
        activationdesc: ffi::cudnnActivationDescriptor_t,
        xdesc: ffi::cudnnTensorDescriptor_t,
        sizeinbytes: *mutusize,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if sizeinbytes.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if sizeinbytes.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnGetBatchNormalizationTrainingExReserveSpaceSize").entered();
            let result = ffi::cudnnGetBatchNormalizationTrainingExReserveSpaceSize(self.handle, mode, bnops, activationdesc, xdesc, sizeinbytes);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn batch_normalization_forward_training(
        &mut self,
        mode: ffi::cudnnBatchNormMode_t,
        alpha: *const :: core :: ffi ::c_void,
        beta: *const :: core :: ffi ::c_void,
        xdesc: ffi::cudnnTensorDescriptor_t,
        x: *const :: core :: ffi ::c_void,
        ydesc: ffi::cudnnTensorDescriptor_t,
        y: *mut :: core :: ffi ::c_void,
        bnscalebiasmeanvardesc: ffi::cudnnTensorDescriptor_t,
        bnscale: *const :: core :: ffi ::c_void,
        bnbias: *const :: core :: ffi ::c_void,
        exponentialaveragefactor: f64,
        resultrunningmean: *mut :: core :: ffi ::c_void,
        resultrunningvariance: *mut :: core :: ffi ::c_void,
        epsilon: f64,
        resultsavemean: *mut :: core :: ffi ::c_void,
        resultsaveinvvariance: *mut :: core :: ffi ::c_void,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if alpha.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if alpha.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if beta.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if beta.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if x.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if x.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if y.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if y.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if bnscale.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if bnscale.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if bnbias.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if bnbias.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if resultrunningmean.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if resultrunningmean.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if resultrunningvariance.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if resultrunningvariance.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if resultsavemean.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if resultsavemean.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if resultsaveinvvariance.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if resultsaveinvvariance.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnBatchNormalizationForwardTraining").entered();
            let result = ffi::cudnnBatchNormalizationForwardTraining(self.handle, mode, alpha, beta, xdesc, x, ydesc, y, bnscalebiasmeanvardesc, bnscale, bnbias, exponentialaveragefactor, resultrunningmean, resultrunningvariance, epsilon, resultsavemean, resultsaveinvvariance);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn batch_normalization_forward_training_ex(
        &mut self,
        mode: ffi::cudnnBatchNormMode_t,
        bnops: ffi::cudnnBatchNormOps_t,
        alpha: *const :: core :: ffi ::c_void,
        beta: *const :: core :: ffi ::c_void,
        xdesc: ffi::cudnnTensorDescriptor_t,
        xdata: *const :: core :: ffi ::c_void,
        zdesc: ffi::cudnnTensorDescriptor_t,
        zdata: *const :: core :: ffi ::c_void,
        ydesc: ffi::cudnnTensorDescriptor_t,
        ydata: *mut :: core :: ffi ::c_void,
        bnscalebiasmeanvardesc: ffi::cudnnTensorDescriptor_t,
        bnscale: *const :: core :: ffi ::c_void,
        bnbias: *const :: core :: ffi ::c_void,
        exponentialaveragefactor: f64,
        resultrunningmean: *mut :: core :: ffi ::c_void,
        resultrunningvariance: *mut :: core :: ffi ::c_void,
        epsilon: f64,
        resultsavemean: *mut :: core :: ffi ::c_void,
        resultsaveinvvariance: *mut :: core :: ffi ::c_void,
        activationdesc: ffi::cudnnActivationDescriptor_t,
        workspace: *mut :: core :: ffi ::c_void,
        workspacesizeinbytes: usize,
        reservespace: *mut :: core :: ffi ::c_void,
        reservespacesizeinbytes: usize,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if alpha.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if alpha.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if beta.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if beta.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if xdata.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if xdata.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if zdata.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if zdata.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if ydata.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if ydata.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if bnscale.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if bnscale.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if bnbias.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if bnbias.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if resultrunningmean.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if resultrunningmean.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if resultrunningvariance.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if resultrunningvariance.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if resultsavemean.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if resultsavemean.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if resultsaveinvvariance.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if resultsaveinvvariance.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if workspace.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if workspace.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if reservespace.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if reservespace.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnBatchNormalizationForwardTrainingEx").entered();
            let result = ffi::cudnnBatchNormalizationForwardTrainingEx(self.handle, mode, bnops, alpha, beta, xdesc, xdata, zdesc, zdata, ydesc, ydata, bnscalebiasmeanvardesc, bnscale, bnbias, exponentialaveragefactor, resultrunningmean, resultrunningvariance, epsilon, resultsavemean, resultsaveinvvariance, activationdesc, workspace, workspacesizeinbytes, reservespace, reservespacesizeinbytes);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn batch_normalization_backward(
        &mut self,
        mode: ffi::cudnnBatchNormMode_t,
        alphadatadiff: *const :: core :: ffi ::c_void,
        betadatadiff: *const :: core :: ffi ::c_void,
        alphaparamdiff: *const :: core :: ffi ::c_void,
        betaparamdiff: *const :: core :: ffi ::c_void,
        xdesc: ffi::cudnnTensorDescriptor_t,
        x: *const :: core :: ffi ::c_void,
        dydesc: ffi::cudnnTensorDescriptor_t,
        dy: *const :: core :: ffi ::c_void,
        dxdesc: ffi::cudnnTensorDescriptor_t,
        dx: *mut :: core :: ffi ::c_void,
        dbnscalebiasdesc: ffi::cudnnTensorDescriptor_t,
        bnscale: *const :: core :: ffi ::c_void,
        dbnscaleresult: *mut :: core :: ffi ::c_void,
        dbnbiasresult: *mut :: core :: ffi ::c_void,
        epsilon: f64,
        savedmean: *const :: core :: ffi ::c_void,
        savedinvvariance: *const :: core :: ffi ::c_void,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if alphadatadiff.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if alphadatadiff.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if betadatadiff.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if betadatadiff.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if alphaparamdiff.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if alphaparamdiff.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if betaparamdiff.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if betaparamdiff.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if x.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if x.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if dy.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if dy.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if dx.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if dx.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if bnscale.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if bnscale.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if dbnscaleresult.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if dbnscaleresult.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if dbnbiasresult.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if dbnbiasresult.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if savedmean.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if savedmean.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if savedinvvariance.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if savedinvvariance.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnBatchNormalizationBackward").entered();
            let result = ffi::cudnnBatchNormalizationBackward(self.handle, mode, alphadatadiff, betadatadiff, alphaparamdiff, betaparamdiff, xdesc, x, dydesc, dy, dxdesc, dx, dbnscalebiasdesc, bnscale, dbnscaleresult, dbnbiasresult, epsilon, savedmean, savedinvvariance);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn batch_normalization_backward_ex(
        &mut self,
        mode: ffi::cudnnBatchNormMode_t,
        bnops: ffi::cudnnBatchNormOps_t,
        alphadatadiff: *const :: core :: ffi ::c_void,
        betadatadiff: *const :: core :: ffi ::c_void,
        alphaparamdiff: *const :: core :: ffi ::c_void,
        betaparamdiff: *const :: core :: ffi ::c_void,
        xdesc: ffi::cudnnTensorDescriptor_t,
        xdata: *const :: core :: ffi ::c_void,
        ydesc: ffi::cudnnTensorDescriptor_t,
        ydata: *const :: core :: ffi ::c_void,
        dydesc: ffi::cudnnTensorDescriptor_t,
        dydata: *const :: core :: ffi ::c_void,
        dzdesc: ffi::cudnnTensorDescriptor_t,
        dzdata: *mut :: core :: ffi ::c_void,
        dxdesc: ffi::cudnnTensorDescriptor_t,
        dxdata: *mut :: core :: ffi ::c_void,
        dbnscalebiasdesc: ffi::cudnnTensorDescriptor_t,
        bnscaledata: *const :: core :: ffi ::c_void,
        bnbiasdata: *const :: core :: ffi ::c_void,
        dbnscaledata: *mut :: core :: ffi ::c_void,
        dbnbiasdata: *mut :: core :: ffi ::c_void,
        epsilon: f64,
        savedmean: *const :: core :: ffi ::c_void,
        savedinvvariance: *const :: core :: ffi ::c_void,
        activationdesc: ffi::cudnnActivationDescriptor_t,
        workspace: *mut :: core :: ffi ::c_void,
        workspacesizeinbytes: usize,
        reservespace: *mut :: core :: ffi ::c_void,
        reservespacesizeinbytes: usize,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if alphadatadiff.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if alphadatadiff.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if betadatadiff.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if betadatadiff.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if alphaparamdiff.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if alphaparamdiff.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if betaparamdiff.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if betaparamdiff.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if xdata.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if xdata.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if ydata.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if ydata.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if dydata.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if dydata.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if dzdata.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if dzdata.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if dxdata.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if dxdata.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if bnscaledata.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if bnscaledata.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if bnbiasdata.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if bnbiasdata.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if dbnscaledata.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if dbnscaledata.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if dbnbiasdata.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if dbnbiasdata.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if savedmean.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if savedmean.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if savedinvvariance.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if savedinvvariance.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if workspace.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if workspace.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if reservespace.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if reservespace.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnBatchNormalizationBackwardEx").entered();
            let result = ffi::cudnnBatchNormalizationBackwardEx(self.handle, mode, bnops, alphadatadiff, betadatadiff, alphaparamdiff, betaparamdiff, xdesc, xdata, ydesc, ydata, dydesc, dydata, dzdesc, dzdata, dxdesc, dxdata, dbnscalebiasdesc, bnscaledata, bnbiasdata, dbnscaledata, dbnbiasdata, epsilon, savedmean, savedinvvariance, activationdesc, workspace, workspacesizeinbytes, reservespace, reservespacesizeinbytes);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn get_normalization_forward_training_workspace_size(
        &mut self,
        mode: ffi::cudnnNormMode_t,
        normops: ffi::cudnnNormOps_t,
        algo: ffi::cudnnNormAlgo_t,
        xdesc: ffi::cudnnTensorDescriptor_t,
        zdesc: ffi::cudnnTensorDescriptor_t,
        ydesc: ffi::cudnnTensorDescriptor_t,
        normscalebiasdesc: ffi::cudnnTensorDescriptor_t,
        activationdesc: ffi::cudnnActivationDescriptor_t,
        normmeanvardesc: ffi::cudnnTensorDescriptor_t,
        sizeinbytes: *mutusize,
        groupcnt: :: core :: ffi :: c_int,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if sizeinbytes.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if sizeinbytes.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnGetNormalizationForwardTrainingWorkspaceSize").entered();
            let result = ffi::cudnnGetNormalizationForwardTrainingWorkspaceSize(self.handle, mode, normops, algo, xdesc, zdesc, ydesc, normscalebiasdesc, activationdesc, normmeanvardesc, sizeinbytes, groupcnt);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn get_normalization_backward_workspace_size(
        &mut self,
        mode: ffi::cudnnNormMode_t,
        normops: ffi::cudnnNormOps_t,
        algo: ffi::cudnnNormAlgo_t,
        xdesc: ffi::cudnnTensorDescriptor_t,
        ydesc: ffi::cudnnTensorDescriptor_t,
        dydesc: ffi::cudnnTensorDescriptor_t,
        dzdesc: ffi::cudnnTensorDescriptor_t,
        dxdesc: ffi::cudnnTensorDescriptor_t,
        dnormscalebiasdesc: ffi::cudnnTensorDescriptor_t,
        activationdesc: ffi::cudnnActivationDescriptor_t,
        normmeanvardesc: ffi::cudnnTensorDescriptor_t,
        sizeinbytes: *mutusize,
        groupcnt: :: core :: ffi :: c_int,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if sizeinbytes.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if sizeinbytes.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnGetNormalizationBackwardWorkspaceSize").entered();
            let result = ffi::cudnnGetNormalizationBackwardWorkspaceSize(self.handle, mode, normops, algo, xdesc, ydesc, dydesc, dzdesc, dxdesc, dnormscalebiasdesc, activationdesc, normmeanvardesc, sizeinbytes, groupcnt);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn get_normalization_training_reserve_space_size(
        &mut self,
        mode: ffi::cudnnNormMode_t,
        normops: ffi::cudnnNormOps_t,
        algo: ffi::cudnnNormAlgo_t,
        activationdesc: ffi::cudnnActivationDescriptor_t,
        xdesc: ffi::cudnnTensorDescriptor_t,
        sizeinbytes: *mutusize,
        groupcnt: :: core :: ffi :: c_int,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if sizeinbytes.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if sizeinbytes.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnGetNormalizationTrainingReserveSpaceSize").entered();
            let result = ffi::cudnnGetNormalizationTrainingReserveSpaceSize(self.handle, mode, normops, algo, activationdesc, xdesc, sizeinbytes, groupcnt);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn normalization_forward_training(
        &mut self,
        mode: ffi::cudnnNormMode_t,
        normops: ffi::cudnnNormOps_t,
        algo: ffi::cudnnNormAlgo_t,
        alpha: *const :: core :: ffi ::c_void,
        beta: *const :: core :: ffi ::c_void,
        xdesc: ffi::cudnnTensorDescriptor_t,
        xdata: *const :: core :: ffi ::c_void,
        normscalebiasdesc: ffi::cudnnTensorDescriptor_t,
        normscale: *const :: core :: ffi ::c_void,
        normbias: *const :: core :: ffi ::c_void,
        exponentialaveragefactor: f64,
        normmeanvardesc: ffi::cudnnTensorDescriptor_t,
        resultrunningmean: *mut :: core :: ffi ::c_void,
        resultrunningvariance: *mut :: core :: ffi ::c_void,
        epsilon: f64,
        resultsavemean: *mut :: core :: ffi ::c_void,
        resultsaveinvvariance: *mut :: core :: ffi ::c_void,
        activationdesc: ffi::cudnnActivationDescriptor_t,
        zdesc: ffi::cudnnTensorDescriptor_t,
        zdata: *const :: core :: ffi ::c_void,
        ydesc: ffi::cudnnTensorDescriptor_t,
        ydata: *mut :: core :: ffi ::c_void,
        workspace: *mut :: core :: ffi ::c_void,
        workspacesizeinbytes: usize,
        reservespace: *mut :: core :: ffi ::c_void,
        reservespacesizeinbytes: usize,
        groupcnt: :: core :: ffi :: c_int,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if alpha.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if alpha.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if beta.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if beta.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if xdata.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if xdata.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if normscale.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if normscale.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if normbias.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if normbias.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if resultrunningmean.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if resultrunningmean.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if resultrunningvariance.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if resultrunningvariance.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if resultsavemean.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if resultsavemean.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if resultsaveinvvariance.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if resultsaveinvvariance.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if zdata.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if zdata.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if ydata.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if ydata.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if workspace.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if workspace.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if reservespace.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if reservespace.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnNormalizationForwardTraining").entered();
            let result = ffi::cudnnNormalizationForwardTraining(self.handle, mode, normops, algo, alpha, beta, xdesc, xdata, normscalebiasdesc, normscale, normbias, exponentialaveragefactor, normmeanvardesc, resultrunningmean, resultrunningvariance, epsilon, resultsavemean, resultsaveinvvariance, activationdesc, zdesc, zdata, ydesc, ydata, workspace, workspacesizeinbytes, reservespace, reservespacesizeinbytes, groupcnt);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn normalization_backward(
        &mut self,
        mode: ffi::cudnnNormMode_t,
        normops: ffi::cudnnNormOps_t,
        algo: ffi::cudnnNormAlgo_t,
        alphadatadiff: *const :: core :: ffi ::c_void,
        betadatadiff: *const :: core :: ffi ::c_void,
        alphaparamdiff: *const :: core :: ffi ::c_void,
        betaparamdiff: *const :: core :: ffi ::c_void,
        xdesc: ffi::cudnnTensorDescriptor_t,
        xdata: *const :: core :: ffi ::c_void,
        ydesc: ffi::cudnnTensorDescriptor_t,
        ydata: *const :: core :: ffi ::c_void,
        dydesc: ffi::cudnnTensorDescriptor_t,
        dydata: *const :: core :: ffi ::c_void,
        dzdesc: ffi::cudnnTensorDescriptor_t,
        dzdata: *mut :: core :: ffi ::c_void,
        dxdesc: ffi::cudnnTensorDescriptor_t,
        dxdata: *mut :: core :: ffi ::c_void,
        dnormscalebiasdesc: ffi::cudnnTensorDescriptor_t,
        normscaledata: *const :: core :: ffi ::c_void,
        normbiasdata: *const :: core :: ffi ::c_void,
        dnormscaledata: *mut :: core :: ffi ::c_void,
        dnormbiasdata: *mut :: core :: ffi ::c_void,
        epsilon: f64,
        normmeanvardesc: ffi::cudnnTensorDescriptor_t,
        savedmean: *const :: core :: ffi ::c_void,
        savedinvvariance: *const :: core :: ffi ::c_void,
        activationdesc: ffi::cudnnActivationDescriptor_t,
        workspace: *mut :: core :: ffi ::c_void,
        workspacesizeinbytes: usize,
        reservespace: *mut :: core :: ffi ::c_void,
        reservespacesizeinbytes: usize,
        groupcnt: :: core :: ffi :: c_int,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if alphadatadiff.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if alphadatadiff.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if betadatadiff.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if betadatadiff.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if alphaparamdiff.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if alphaparamdiff.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if betaparamdiff.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if betaparamdiff.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if xdata.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if xdata.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if ydata.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if ydata.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if dydata.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if dydata.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if dzdata.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if dzdata.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if dxdata.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if dxdata.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if normscaledata.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if normscaledata.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if normbiasdata.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if normbiasdata.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if dnormscaledata.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if dnormscaledata.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if dnormbiasdata.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if dnormbiasdata.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if savedmean.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if savedmean.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if savedinvvariance.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if savedinvvariance.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if workspace.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if workspace.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if reservespace.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if reservespace.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnNormalizationBackward").entered();
            let result = ffi::cudnnNormalizationBackward(self.handle, mode, normops, algo, alphadatadiff, betadatadiff, alphaparamdiff, betaparamdiff, xdesc, xdata, ydesc, ydata, dydesc, dydata, dzdesc, dzdata, dxdesc, dxdata, dnormscalebiasdesc, normscaledata, normbiasdata, dnormscaledata, dnormbiasdata, epsilon, normmeanvardesc, savedmean, savedinvvariance, activationdesc, workspace, workspacesizeinbytes, reservespace, reservespacesizeinbytes, groupcnt);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn spatial_tf_grid_generator_backward(
        &mut self,
        stdesc: ffi::cudnnSpatialTransformerDescriptor_t,
        dgrid: *const :: core :: ffi ::c_void,
        dtheta: *mut :: core :: ffi ::c_void,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if dgrid.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if dgrid.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if dtheta.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if dtheta.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnSpatialTfGridGeneratorBackward").entered();
            let result = ffi::cudnnSpatialTfGridGeneratorBackward(self.handle, stdesc, dgrid, dtheta);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn spatial_tf_sampler_backward(
        &mut self,
        stdesc: ffi::cudnnSpatialTransformerDescriptor_t,
        alpha: *const :: core :: ffi ::c_void,
        xdesc: ffi::cudnnTensorDescriptor_t,
        x: *const :: core :: ffi ::c_void,
        beta: *const :: core :: ffi ::c_void,
        dxdesc: ffi::cudnnTensorDescriptor_t,
        dx: *mut :: core :: ffi ::c_void,
        alphadgrid: *const :: core :: ffi ::c_void,
        dydesc: ffi::cudnnTensorDescriptor_t,
        dy: *const :: core :: ffi ::c_void,
        grid: *const :: core :: ffi ::c_void,
        betadgrid: *const :: core :: ffi ::c_void,
        dgrid: *mut :: core :: ffi ::c_void,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if alpha.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if alpha.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if x.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if x.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if beta.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if beta.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if dx.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if dx.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if alphadgrid.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if alphadgrid.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if dy.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if dy.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if grid.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if grid.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if betadgrid.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if betadgrid.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if dgrid.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if dgrid.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnSpatialTfSamplerBackward").entered();
            let result = ffi::cudnnSpatialTfSamplerBackward(self.handle, stdesc, alpha, xdesc, x, beta, dxdesc, dx, alphadgrid, dydesc, dy, grid, betadgrid, dgrid);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn dropout_backward(
        &mut self,
        dropoutdesc: ffi::cudnnDropoutDescriptor_t,
        dydesc: ffi::cudnnTensorDescriptor_t,
        dy: *const :: core :: ffi ::c_void,
        dxdesc: ffi::cudnnTensorDescriptor_t,
        dx: *mut :: core :: ffi ::c_void,
        reservespace: *mut :: core :: ffi ::c_void,
        reservespacesizeinbytes: usize,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if dy.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if dy.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if dx.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if dx.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if reservespace.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if reservespace.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnDropoutBackward").entered();
            let result = ffi::cudnnDropoutBackward(self.handle, dropoutdesc, dydesc, dy, dxdesc, dx, reservespace, reservespacesizeinbytes);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn build_rnn_dynamic(
        &mut self,
        rnndesc: ffi::cudnnRNNDescriptor_t,
        minibatch: :: core :: ffi :: c_int,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnBuildRNNDynamic").entered();
            let result = ffi::cudnnBuildRNNDynamic(self.handle, rnndesc, minibatch);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn get_rnn_temp_space_sizes(
        &mut self,
        rnndesc: ffi::cudnnRNNDescriptor_t,
        fwdmode: ffi::cudnnForwardMode_t,
        xdesc: ffi::cudnnRNNDataDescriptor_t,
        workspacesize: *mutusize,
        reservespacesize: *mutusize,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if workspacesize.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if workspacesize.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if reservespacesize.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if reservespacesize.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnGetRNNTempSpaceSizes").entered();
            let result = ffi::cudnnGetRNNTempSpaceSizes(self.handle, rnndesc, fwdmode, xdesc, workspacesize, reservespacesize);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn get_rnn_weight_space_size(
        &mut self,
        rnndesc: ffi::cudnnRNNDescriptor_t,
        weightspacesize: *mutusize,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if weightspacesize.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if weightspacesize.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnGetRNNWeightSpaceSize").entered();
            let result = ffi::cudnnGetRNNWeightSpaceSize(self.handle, rnndesc, weightspacesize);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn get_rnn_weight_params(
        &mut self,
        rnndesc: ffi::cudnnRNNDescriptor_t,
        pseudolayer: i32,
        weightspacesize: usize,
        weightspace: *const :: core :: ffi ::c_void,
        linlayerid: i32,
        mdesc: ffi::cudnnTensorDescriptor_t,
        maddr: *mut *mut :: core :: ffi ::c_void,
        bdesc: ffi::cudnnTensorDescriptor_t,
        baddr: *mut *mut :: core :: ffi ::c_void,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if weightspace.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if weightspace.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if maddr.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if maddr.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if baddr.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if baddr.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnGetRNNWeightParams").entered();
            let result = ffi::cudnnGetRNNWeightParams(self.handle, rnndesc, pseudolayer, weightspacesize, weightspace, linlayerid, mdesc, maddr, bdesc, baddr);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn rnn_forward(
        &mut self,
        rnndesc: ffi::cudnnRNNDescriptor_t,
        fwdmode: ffi::cudnnForwardMode_t,
        devseqlengths: *consti32,
        xdesc: ffi::cudnnRNNDataDescriptor_t,
        x: *const :: core :: ffi ::c_void,
        ydesc: ffi::cudnnRNNDataDescriptor_t,
        y: *mut :: core :: ffi ::c_void,
        hdesc: ffi::cudnnTensorDescriptor_t,
        hx: *const :: core :: ffi ::c_void,
        hy: *mut :: core :: ffi ::c_void,
        cdesc: ffi::cudnnTensorDescriptor_t,
        cx: *const :: core :: ffi ::c_void,
        cy: *mut :: core :: ffi ::c_void,
        weightspacesize: usize,
        weightspace: *const :: core :: ffi ::c_void,
        workspacesize: usize,
        workspace: *mut :: core :: ffi ::c_void,
        reservespacesize: usize,
        reservespace: *mut :: core :: ffi ::c_void,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if devseqlengths.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if devseqlengths.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if x.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if x.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if y.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if y.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if hx.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if hx.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if hy.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if hy.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if cx.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if cx.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if cy.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if cy.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if weightspace.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if weightspace.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if workspace.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if workspace.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if reservespace.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if reservespace.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnRNNForward").entered();
            let result = ffi::cudnnRNNForward(self.handle, rnndesc, fwdmode, devseqlengths, xdesc, x, ydesc, y, hdesc, hx, hy, cdesc, cx, cy, weightspacesize, weightspace, workspacesize, workspace, reservespacesize, reservespace);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn get_multi_head_attn_buffers(
        &mut self,
        attndesc: ffi::cudnnAttnDescriptor_t,
        weightsizeinbytes: *mutusize,
        workspacesizeinbytes: *mutusize,
        reservespacesizeinbytes: *mutusize,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if weightsizeinbytes.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if weightsizeinbytes.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if workspacesizeinbytes.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if workspacesizeinbytes.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if reservespacesizeinbytes.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if reservespacesizeinbytes.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnGetMultiHeadAttnBuffers").entered();
            let result = ffi::cudnnGetMultiHeadAttnBuffers(self.handle, attndesc, weightsizeinbytes, workspacesizeinbytes, reservespacesizeinbytes);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn get_multi_head_attn_weights(
        &mut self,
        attndesc: ffi::cudnnAttnDescriptor_t,
        wkind: ffi::cudnnMultiHeadAttnWeightKind_t,
        weightsizeinbytes: usize,
        weights: *const :: core :: ffi ::c_void,
        wdesc: ffi::cudnnTensorDescriptor_t,
        waddr: *mut *mut :: core :: ffi ::c_void,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if weights.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if weights.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if waddr.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if waddr.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnGetMultiHeadAttnWeights").entered();
            let result = ffi::cudnnGetMultiHeadAttnWeights(self.handle, attndesc, wkind, weightsizeinbytes, weights, wdesc, waddr);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn multi_head_attn_forward(
        &mut self,
        attndesc: ffi::cudnnAttnDescriptor_t,
        curridx: :: core :: ffi :: c_int,
        lowinidx: *const :: core :: ffi ::c_int,
        hiwinidx: *const :: core :: ffi ::c_int,
        devseqlengthsqo: *const :: core :: ffi ::c_int,
        devseqlengthskv: *const :: core :: ffi ::c_int,
        qdesc: ffi::cudnnSeqDataDescriptor_t,
        queries: *const :: core :: ffi ::c_void,
        residuals: *const :: core :: ffi ::c_void,
        kdesc: ffi::cudnnSeqDataDescriptor_t,
        keys: *const :: core :: ffi ::c_void,
        vdesc: ffi::cudnnSeqDataDescriptor_t,
        values: *const :: core :: ffi ::c_void,
        odesc: ffi::cudnnSeqDataDescriptor_t,
        out: *mut :: core :: ffi ::c_void,
        weightsizeinbytes: usize,
        weights: *const :: core :: ffi ::c_void,
        workspacesizeinbytes: usize,
        workspace: *mut :: core :: ffi ::c_void,
        reservespacesizeinbytes: usize,
        reservespace: *mut :: core :: ffi ::c_void,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if lowinidx.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if lowinidx.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if hiwinidx.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if hiwinidx.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if devseqlengthsqo.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if devseqlengthsqo.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if devseqlengthskv.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if devseqlengthskv.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if queries.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if queries.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if residuals.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if residuals.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if keys.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if keys.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if values.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if values.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if out.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if out.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if weights.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if weights.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if workspace.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if workspace.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if reservespace.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if reservespace.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnMultiHeadAttnForward").entered();
            let result = ffi::cudnnMultiHeadAttnForward(self.handle, attndesc, curridx, lowinidx, hiwinidx, devseqlengthsqo, devseqlengthskv, qdesc, queries, residuals, kdesc, keys, vdesc, values, odesc, out, weightsizeinbytes, weights, workspacesizeinbytes, workspace, reservespacesizeinbytes, reservespace);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn rnn_backward_data_v_8(
        &mut self,
        rnndesc: ffi::cudnnRNNDescriptor_t,
        devseqlengths: *consti32,
        ydesc: ffi::cudnnRNNDataDescriptor_t,
        y: *const :: core :: ffi ::c_void,
        dy: *const :: core :: ffi ::c_void,
        xdesc: ffi::cudnnRNNDataDescriptor_t,
        dx: *mut :: core :: ffi ::c_void,
        hdesc: ffi::cudnnTensorDescriptor_t,
        hx: *const :: core :: ffi ::c_void,
        dhy: *const :: core :: ffi ::c_void,
        dhx: *mut :: core :: ffi ::c_void,
        cdesc: ffi::cudnnTensorDescriptor_t,
        cx: *const :: core :: ffi ::c_void,
        dcy: *const :: core :: ffi ::c_void,
        dcx: *mut :: core :: ffi ::c_void,
        weightspacesize: usize,
        weightspace: *const :: core :: ffi ::c_void,
        workspacesize: usize,
        workspace: *mut :: core :: ffi ::c_void,
        reservespacesize: usize,
        reservespace: *mut :: core :: ffi ::c_void,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if devseqlengths.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if devseqlengths.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if y.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if y.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if dy.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if dy.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if dx.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if dx.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if hx.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if hx.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if dhy.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if dhy.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if dhx.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if dhx.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if cx.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if cx.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if dcy.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if dcy.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if dcx.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if dcx.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if weightspace.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if weightspace.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if workspace.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if workspace.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if reservespace.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if reservespace.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnRNNBackwardData_v8").entered();
            let result = ffi::cudnnRNNBackwardData_v8(self.handle, rnndesc, devseqlengths, ydesc, y, dy, xdesc, dx, hdesc, hx, dhy, dhx, cdesc, cx, dcy, dcx, weightspacesize, weightspace, workspacesize, workspace, reservespacesize, reservespace);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn rnn_backward_weights_v_8(
        &mut self,
        rnndesc: ffi::cudnnRNNDescriptor_t,
        addgrad: ffi::cudnnWgradMode_t,
        devseqlengths: *consti32,
        xdesc: ffi::cudnnRNNDataDescriptor_t,
        x: *const :: core :: ffi ::c_void,
        hdesc: ffi::cudnnTensorDescriptor_t,
        hx: *const :: core :: ffi ::c_void,
        ydesc: ffi::cudnnRNNDataDescriptor_t,
        y: *const :: core :: ffi ::c_void,
        weightspacesize: usize,
        dweightspace: *mut :: core :: ffi ::c_void,
        workspacesize: usize,
        workspace: *mut :: core :: ffi ::c_void,
        reservespacesize: usize,
        reservespace: *mut :: core :: ffi ::c_void,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if devseqlengths.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if devseqlengths.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if x.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if x.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if hx.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if hx.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if y.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if y.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if dweightspace.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if dweightspace.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if workspace.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if workspace.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if reservespace.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if reservespace.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnRNNBackwardWeights_v8").entered();
            let result = ffi::cudnnRNNBackwardWeights_v8(self.handle, rnndesc, addgrad, devseqlengths, xdesc, x, hdesc, hx, ydesc, y, weightspacesize, dweightspace, workspacesize, workspace, reservespacesize, reservespace);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn multi_head_attn_backward_data(
        &mut self,
        attndesc: ffi::cudnnAttnDescriptor_t,
        lowinidx: *const :: core :: ffi ::c_int,
        hiwinidx: *const :: core :: ffi ::c_int,
        devseqlengthsdqdo: *const :: core :: ffi ::c_int,
        devseqlengthsdkdv: *const :: core :: ffi ::c_int,
        dodesc: ffi::cudnnSeqDataDescriptor_t,
        dout: *const :: core :: ffi ::c_void,
        dqdesc: ffi::cudnnSeqDataDescriptor_t,
        dqueries: *mut :: core :: ffi ::c_void,
        queries: *const :: core :: ffi ::c_void,
        dkdesc: ffi::cudnnSeqDataDescriptor_t,
        dkeys: *mut :: core :: ffi ::c_void,
        keys: *const :: core :: ffi ::c_void,
        dvdesc: ffi::cudnnSeqDataDescriptor_t,
        dvalues: *mut :: core :: ffi ::c_void,
        values: *const :: core :: ffi ::c_void,
        weightsizeinbytes: usize,
        weights: *const :: core :: ffi ::c_void,
        workspacesizeinbytes: usize,
        workspace: *mut :: core :: ffi ::c_void,
        reservespacesizeinbytes: usize,
        reservespace: *mut :: core :: ffi ::c_void,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if lowinidx.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if lowinidx.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if hiwinidx.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if hiwinidx.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if devseqlengthsdqdo.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if devseqlengthsdqdo.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if devseqlengthsdkdv.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if devseqlengthsdkdv.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if dout.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if dout.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if dqueries.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if dqueries.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if queries.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if queries.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if dkeys.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if dkeys.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if keys.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if keys.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if dvalues.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if dvalues.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if values.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if values.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if weights.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if weights.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if workspace.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if workspace.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if reservespace.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if reservespace.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnMultiHeadAttnBackwardData").entered();
            let result = ffi::cudnnMultiHeadAttnBackwardData(self.handle, attndesc, lowinidx, hiwinidx, devseqlengthsdqdo, devseqlengthsdkdv, dodesc, dout, dqdesc, dqueries, queries, dkdesc, dkeys, keys, dvdesc, dvalues, values, weightsizeinbytes, weights, workspacesizeinbytes, workspace, reservespacesizeinbytes, reservespace);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn multi_head_attn_backward_weights(
        &mut self,
        attndesc: ffi::cudnnAttnDescriptor_t,
        addgrad: ffi::cudnnWgradMode_t,
        qdesc: ffi::cudnnSeqDataDescriptor_t,
        queries: *const :: core :: ffi ::c_void,
        kdesc: ffi::cudnnSeqDataDescriptor_t,
        keys: *const :: core :: ffi ::c_void,
        vdesc: ffi::cudnnSeqDataDescriptor_t,
        values: *const :: core :: ffi ::c_void,
        dodesc: ffi::cudnnSeqDataDescriptor_t,
        dout: *const :: core :: ffi ::c_void,
        weightsizeinbytes: usize,
        weights: *const :: core :: ffi ::c_void,
        dweights: *mut :: core :: ffi ::c_void,
        workspacesizeinbytes: usize,
        workspace: *mut :: core :: ffi ::c_void,
        reservespacesizeinbytes: usize,
        reservespace: *mut :: core :: ffi ::c_void,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if queries.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if queries.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if keys.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if keys.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if values.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if values.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if dout.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if dout.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if weights.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if weights.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if dweights.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if dweights.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if workspace.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if workspace.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if reservespace.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if reservespace.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnMultiHeadAttnBackwardWeights").entered();
            let result = ffi::cudnnMultiHeadAttnBackwardWeights(self.handle, attndesc, addgrad, qdesc, queries, kdesc, keys, vdesc, values, dodesc, dout, weightsizeinbytes, weights, dweights, workspacesizeinbytes, workspace, reservespacesizeinbytes, reservespace);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn ctc_loss(
        &mut self,
        probsdesc: ffi::cudnnTensorDescriptor_t,
        probs: *const :: core :: ffi ::c_void,
        hostlabels: *const :: core :: ffi ::c_int,
        hostlabellengths: *const :: core :: ffi ::c_int,
        hostinputlengths: *const :: core :: ffi ::c_int,
        costs: *mut :: core :: ffi ::c_void,
        gradientsdesc: ffi::cudnnTensorDescriptor_t,
        gradients: *mut :: core :: ffi ::c_void,
        algo: ffi::cudnnCTCLossAlgo_t,
        ctclossdesc: ffi::cudnnCTCLossDescriptor_t,
        workspace: *mut :: core :: ffi ::c_void,
        workspacesizeinbytes: usize,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if probs.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if probs.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if hostlabels.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if hostlabels.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if hostlabellengths.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if hostlabellengths.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if hostinputlengths.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if hostinputlengths.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if costs.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if costs.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if gradients.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if gradients.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if workspace.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if workspace.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnCTCLoss").entered();
            let result = ffi::cudnnCTCLoss(self.handle, probsdesc, probs, hostlabels, hostlabellengths, hostinputlengths, costs, gradientsdesc, gradients, algo, ctclossdesc, workspace, workspacesizeinbytes);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn ctc_loss_v_8(
        &mut self,
        algo: ffi::cudnnCTCLossAlgo_t,
        ctclossdesc: ffi::cudnnCTCLossDescriptor_t,
        probsdesc: ffi::cudnnTensorDescriptor_t,
        probs: *const :: core :: ffi ::c_void,
        labels: *const :: core :: ffi ::c_int,
        labellengths: *const :: core :: ffi ::c_int,
        inputlengths: *const :: core :: ffi ::c_int,
        costs: *mut :: core :: ffi ::c_void,
        gradientsdesc: ffi::cudnnTensorDescriptor_t,
        gradients: *mut :: core :: ffi ::c_void,
        workspacesizeinbytes: usize,
        workspace: *mut :: core :: ffi ::c_void,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if probs.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if probs.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if labels.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if labels.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if labellengths.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if labellengths.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if inputlengths.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if inputlengths.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if costs.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if costs.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if gradients.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if gradients.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if workspace.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if workspace.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnCTCLoss_v8").entered();
            let result = ffi::cudnnCTCLoss_v8(self.handle, algo, ctclossdesc, probsdesc, probs, labels, labellengths, inputlengths, costs, gradientsdesc, gradients, workspacesizeinbytes, workspace);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn get_ctc_loss_workspace_size(
        &mut self,
        probsdesc: ffi::cudnnTensorDescriptor_t,
        gradientsdesc: ffi::cudnnTensorDescriptor_t,
        labels: *const :: core :: ffi ::c_int,
        labellengths: *const :: core :: ffi ::c_int,
        inputlengths: *const :: core :: ffi ::c_int,
        algo: ffi::cudnnCTCLossAlgo_t,
        ctclossdesc: ffi::cudnnCTCLossDescriptor_t,
        sizeinbytes: *mutusize,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if labels.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if labels.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if labellengths.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if labellengths.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if inputlengths.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if inputlengths.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if sizeinbytes.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if sizeinbytes.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnGetCTCLossWorkspaceSize").entered();
            let result = ffi::cudnnGetCTCLossWorkspaceSize(self.handle, probsdesc, gradientsdesc, labels, labellengths, inputlengths, algo, ctclossdesc, sizeinbytes);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn get_ctc_loss_workspace_size_v_8(
        &mut self,
        algo: ffi::cudnnCTCLossAlgo_t,
        ctclossdesc: ffi::cudnnCTCLossDescriptor_t,
        probsdesc: ffi::cudnnTensorDescriptor_t,
        gradientsdesc: ffi::cudnnTensorDescriptor_t,
        sizeinbytes: *mutusize,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if sizeinbytes.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if sizeinbytes.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnGetCTCLossWorkspaceSize_v8").entered();
            let result = ffi::cudnnGetCTCLossWorkspaceSize_v8(self.handle, algo, ctclossdesc, probsdesc, gradientsdesc, sizeinbytes);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn get_convolution_forward_algorithm_max_count(
        &mut self,
        count: *mut :: core :: ffi ::c_int,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if count.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if count.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnGetConvolutionForwardAlgorithmMaxCount").entered();
            let result = ffi::cudnnGetConvolutionForwardAlgorithmMaxCount(self.handle, count);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn get_convolution_forward_algorithm_v_7(
        &mut self,
        srcdesc: ffi::cudnnTensorDescriptor_t,
        filterdesc: ffi::cudnnFilterDescriptor_t,
        convdesc: ffi::cudnnConvolutionDescriptor_t,
        destdesc: ffi::cudnnTensorDescriptor_t,
        requestedalgocount: :: core :: ffi :: c_int,
        returnedalgocount: *mut :: core :: ffi ::c_int,
        perfresults: *mutffi::cudnnConvolutionFwdAlgoPerf_t,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if returnedalgocount.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if returnedalgocount.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if perfresults.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if perfresults.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnGetConvolutionForwardAlgorithm_v7").entered();
            let result = ffi::cudnnGetConvolutionForwardAlgorithm_v7(self.handle, srcdesc, filterdesc, convdesc, destdesc, requestedalgocount, returnedalgocount, perfresults);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn find_convolution_forward_algorithm(
        &mut self,
        xdesc: ffi::cudnnTensorDescriptor_t,
        wdesc: ffi::cudnnFilterDescriptor_t,
        convdesc: ffi::cudnnConvolutionDescriptor_t,
        ydesc: ffi::cudnnTensorDescriptor_t,
        requestedalgocount: :: core :: ffi :: c_int,
        returnedalgocount: *mut :: core :: ffi ::c_int,
        perfresults: *mutffi::cudnnConvolutionFwdAlgoPerf_t,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if returnedalgocount.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if returnedalgocount.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if perfresults.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if perfresults.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnFindConvolutionForwardAlgorithm").entered();
            let result = ffi::cudnnFindConvolutionForwardAlgorithm(self.handle, xdesc, wdesc, convdesc, ydesc, requestedalgocount, returnedalgocount, perfresults);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn find_convolution_forward_algorithm_ex(
        &mut self,
        xdesc: ffi::cudnnTensorDescriptor_t,
        x: *const :: core :: ffi ::c_void,
        wdesc: ffi::cudnnFilterDescriptor_t,
        w: *const :: core :: ffi ::c_void,
        convdesc: ffi::cudnnConvolutionDescriptor_t,
        ydesc: ffi::cudnnTensorDescriptor_t,
        y: *mut :: core :: ffi ::c_void,
        requestedalgocount: :: core :: ffi :: c_int,
        returnedalgocount: *mut :: core :: ffi ::c_int,
        perfresults: *mutffi::cudnnConvolutionFwdAlgoPerf_t,
        workspace: *mut :: core :: ffi ::c_void,
        workspacesizeinbytes: usize,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if x.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if x.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if w.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if w.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if y.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if y.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if returnedalgocount.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if returnedalgocount.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if perfresults.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if perfresults.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if workspace.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if workspace.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnFindConvolutionForwardAlgorithmEx").entered();
            let result = ffi::cudnnFindConvolutionForwardAlgorithmEx(self.handle, xdesc, x, wdesc, w, convdesc, ydesc, y, requestedalgocount, returnedalgocount, perfresults, workspace, workspacesizeinbytes);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn im_2col(
        &mut self,
        xdesc: ffi::cudnnTensorDescriptor_t,
        x: *const :: core :: ffi ::c_void,
        wdesc: ffi::cudnnFilterDescriptor_t,
        convdesc: ffi::cudnnConvolutionDescriptor_t,
        colbuffer: *mut :: core :: ffi ::c_void,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if x.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if x.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if colbuffer.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if colbuffer.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnIm2Col").entered();
            let result = ffi::cudnnIm2Col(self.handle, xdesc, x, wdesc, convdesc, colbuffer);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn reorder_filter_and_bias(
        &mut self,
        filterdesc: ffi::cudnnFilterDescriptor_t,
        reordertype: ffi::cudnnReorderType_t,
        filterdata: *const :: core :: ffi ::c_void,
        reorderedfilterdata: *mut :: core :: ffi ::c_void,
        reorderbias: :: core :: ffi :: c_int,
        biasdata: *const :: core :: ffi ::c_void,
        reorderedbiasdata: *mut :: core :: ffi ::c_void,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if filterdata.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if filterdata.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if reorderedfilterdata.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if reorderedfilterdata.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if biasdata.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if biasdata.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if reorderedbiasdata.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if reorderedbiasdata.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnReorderFilterAndBias").entered();
            let result = ffi::cudnnReorderFilterAndBias(self.handle, filterdesc, reordertype, filterdata, reorderedfilterdata, reorderbias, biasdata, reorderedbiasdata);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn get_convolution_forward_workspace_size(
        &mut self,
        xdesc: ffi::cudnnTensorDescriptor_t,
        wdesc: ffi::cudnnFilterDescriptor_t,
        convdesc: ffi::cudnnConvolutionDescriptor_t,
        ydesc: ffi::cudnnTensorDescriptor_t,
        algo: ffi::cudnnConvolutionFwdAlgo_t,
        sizeinbytes: *mutusize,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if sizeinbytes.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if sizeinbytes.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnGetConvolutionForwardWorkspaceSize").entered();
            let result = ffi::cudnnGetConvolutionForwardWorkspaceSize(self.handle, xdesc, wdesc, convdesc, ydesc, algo, sizeinbytes);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn convolution_forward(
        &mut self,
        alpha: *const :: core :: ffi ::c_void,
        xdesc: ffi::cudnnTensorDescriptor_t,
        x: *const :: core :: ffi ::c_void,
        wdesc: ffi::cudnnFilterDescriptor_t,
        w: *const :: core :: ffi ::c_void,
        convdesc: ffi::cudnnConvolutionDescriptor_t,
        algo: ffi::cudnnConvolutionFwdAlgo_t,
        workspace: *mut :: core :: ffi ::c_void,
        workspacesizeinbytes: usize,
        beta: *const :: core :: ffi ::c_void,
        ydesc: ffi::cudnnTensorDescriptor_t,
        y: *mut :: core :: ffi ::c_void,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if alpha.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if alpha.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if x.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if x.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if w.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if w.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if workspace.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if workspace.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if beta.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if beta.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if y.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if y.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnConvolutionForward").entered();
            let result = ffi::cudnnConvolutionForward(self.handle, alpha, xdesc, x, wdesc, w, convdesc, algo, workspace, workspacesizeinbytes, beta, ydesc, y);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn convolution_bias_activation_forward(
        &mut self,
        alpha1: *const :: core :: ffi ::c_void,
        xdesc: ffi::cudnnTensorDescriptor_t,
        x: *const :: core :: ffi ::c_void,
        wdesc: ffi::cudnnFilterDescriptor_t,
        w: *const :: core :: ffi ::c_void,
        convdesc: ffi::cudnnConvolutionDescriptor_t,
        algo: ffi::cudnnConvolutionFwdAlgo_t,
        workspace: *mut :: core :: ffi ::c_void,
        workspacesizeinbytes: usize,
        alpha2: *const :: core :: ffi ::c_void,
        zdesc: ffi::cudnnTensorDescriptor_t,
        z: *const :: core :: ffi ::c_void,
        biasdesc: ffi::cudnnTensorDescriptor_t,
        bias: *const :: core :: ffi ::c_void,
        activationdesc: ffi::cudnnActivationDescriptor_t,
        ydesc: ffi::cudnnTensorDescriptor_t,
        y: *mut :: core :: ffi ::c_void,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if alpha1.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if alpha1.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if x.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if x.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if w.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if w.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if workspace.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if workspace.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if alpha2.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if alpha2.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if z.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if z.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if bias.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if bias.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if y.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if y.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnConvolutionBiasActivationForward").entered();
            let result = ffi::cudnnConvolutionBiasActivationForward(self.handle, alpha1, xdesc, x, wdesc, w, convdesc, algo, workspace, workspacesizeinbytes, alpha2, zdesc, z, biasdesc, bias, activationdesc, ydesc, y);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn get_convolution_backward_data_algorithm_max_count(
        &mut self,
        count: *mut :: core :: ffi ::c_int,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if count.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if count.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnGetConvolutionBackwardDataAlgorithmMaxCount").entered();
            let result = ffi::cudnnGetConvolutionBackwardDataAlgorithmMaxCount(self.handle, count);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn find_convolution_backward_data_algorithm(
        &mut self,
        wdesc: ffi::cudnnFilterDescriptor_t,
        dydesc: ffi::cudnnTensorDescriptor_t,
        convdesc: ffi::cudnnConvolutionDescriptor_t,
        dxdesc: ffi::cudnnTensorDescriptor_t,
        requestedalgocount: :: core :: ffi :: c_int,
        returnedalgocount: *mut :: core :: ffi ::c_int,
        perfresults: *mutffi::cudnnConvolutionBwdDataAlgoPerf_t,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if returnedalgocount.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if returnedalgocount.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if perfresults.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if perfresults.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnFindConvolutionBackwardDataAlgorithm").entered();
            let result = ffi::cudnnFindConvolutionBackwardDataAlgorithm(self.handle, wdesc, dydesc, convdesc, dxdesc, requestedalgocount, returnedalgocount, perfresults);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn find_convolution_backward_data_algorithm_ex(
        &mut self,
        wdesc: ffi::cudnnFilterDescriptor_t,
        w: *const :: core :: ffi ::c_void,
        dydesc: ffi::cudnnTensorDescriptor_t,
        dy: *const :: core :: ffi ::c_void,
        convdesc: ffi::cudnnConvolutionDescriptor_t,
        dxdesc: ffi::cudnnTensorDescriptor_t,
        dx: *mut :: core :: ffi ::c_void,
        requestedalgocount: :: core :: ffi :: c_int,
        returnedalgocount: *mut :: core :: ffi ::c_int,
        perfresults: *mutffi::cudnnConvolutionBwdDataAlgoPerf_t,
        workspace: *mut :: core :: ffi ::c_void,
        workspacesizeinbytes: usize,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if w.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if w.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if dy.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if dy.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if dx.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if dx.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if returnedalgocount.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if returnedalgocount.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if perfresults.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if perfresults.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if workspace.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if workspace.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnFindConvolutionBackwardDataAlgorithmEx").entered();
            let result = ffi::cudnnFindConvolutionBackwardDataAlgorithmEx(self.handle, wdesc, w, dydesc, dy, convdesc, dxdesc, dx, requestedalgocount, returnedalgocount, perfresults, workspace, workspacesizeinbytes);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn get_convolution_backward_data_algorithm_v_7(
        &mut self,
        filterdesc: ffi::cudnnFilterDescriptor_t,
        diffdesc: ffi::cudnnTensorDescriptor_t,
        convdesc: ffi::cudnnConvolutionDescriptor_t,
        graddesc: ffi::cudnnTensorDescriptor_t,
        requestedalgocount: :: core :: ffi :: c_int,
        returnedalgocount: *mut :: core :: ffi ::c_int,
        perfresults: *mutffi::cudnnConvolutionBwdDataAlgoPerf_t,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if returnedalgocount.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if returnedalgocount.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if perfresults.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if perfresults.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnGetConvolutionBackwardDataAlgorithm_v7").entered();
            let result = ffi::cudnnGetConvolutionBackwardDataAlgorithm_v7(self.handle, filterdesc, diffdesc, convdesc, graddesc, requestedalgocount, returnedalgocount, perfresults);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn get_convolution_backward_data_workspace_size(
        &mut self,
        wdesc: ffi::cudnnFilterDescriptor_t,
        dydesc: ffi::cudnnTensorDescriptor_t,
        convdesc: ffi::cudnnConvolutionDescriptor_t,
        dxdesc: ffi::cudnnTensorDescriptor_t,
        algo: ffi::cudnnConvolutionBwdDataAlgo_t,
        sizeinbytes: *mutusize,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if sizeinbytes.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if sizeinbytes.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnGetConvolutionBackwardDataWorkspaceSize").entered();
            let result = ffi::cudnnGetConvolutionBackwardDataWorkspaceSize(self.handle, wdesc, dydesc, convdesc, dxdesc, algo, sizeinbytes);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn convolution_backward_data(
        &mut self,
        alpha: *const :: core :: ffi ::c_void,
        wdesc: ffi::cudnnFilterDescriptor_t,
        w: *const :: core :: ffi ::c_void,
        dydesc: ffi::cudnnTensorDescriptor_t,
        dy: *const :: core :: ffi ::c_void,
        convdesc: ffi::cudnnConvolutionDescriptor_t,
        algo: ffi::cudnnConvolutionBwdDataAlgo_t,
        workspace: *mut :: core :: ffi ::c_void,
        workspacesizeinbytes: usize,
        beta: *const :: core :: ffi ::c_void,
        dxdesc: ffi::cudnnTensorDescriptor_t,
        dx: *mut :: core :: ffi ::c_void,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if alpha.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if alpha.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if w.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if w.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if dy.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if dy.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if workspace.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if workspace.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if beta.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if beta.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if dx.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if dx.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnConvolutionBackwardData").entered();
            let result = ffi::cudnnConvolutionBackwardData(self.handle, alpha, wdesc, w, dydesc, dy, convdesc, algo, workspace, workspacesizeinbytes, beta, dxdesc, dx);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn get_folded_conv_backward_data_descriptors(
        &mut self,
        filterdesc: ffi::cudnnFilterDescriptor_t,
        diffdesc: ffi::cudnnTensorDescriptor_t,
        convdesc: ffi::cudnnConvolutionDescriptor_t,
        graddesc: ffi::cudnnTensorDescriptor_t,
        transformformat: ffi::cudnnTensorFormat_t,
        foldedfilterdesc: ffi::cudnnFilterDescriptor_t,
        paddeddiffdesc: ffi::cudnnTensorDescriptor_t,
        foldedconvdesc: ffi::cudnnConvolutionDescriptor_t,
        foldedgraddesc: ffi::cudnnTensorDescriptor_t,
        filterfoldtransdesc: ffi::cudnnTensorTransformDescriptor_t,
        diffpadtransdesc: ffi::cudnnTensorTransformDescriptor_t,
        gradfoldtransdesc: ffi::cudnnTensorTransformDescriptor_t,
        gradunfoldtransdesc: ffi::cudnnTensorTransformDescriptor_t,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnGetFoldedConvBackwardDataDescriptors").entered();
            let result = ffi::cudnnGetFoldedConvBackwardDataDescriptors(self.handle, filterdesc, diffdesc, convdesc, graddesc, transformformat, foldedfilterdesc, paddeddiffdesc, foldedconvdesc, foldedgraddesc, filterfoldtransdesc, diffpadtransdesc, gradfoldtransdesc, gradunfoldtransdesc);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn get_convolution_backward_filter_algorithm_max_count(
        &mut self,
        count: *mut :: core :: ffi ::c_int,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if count.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if count.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnGetConvolutionBackwardFilterAlgorithmMaxCount").entered();
            let result = ffi::cudnnGetConvolutionBackwardFilterAlgorithmMaxCount(self.handle, count);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn find_convolution_backward_filter_algorithm(
        &mut self,
        xdesc: ffi::cudnnTensorDescriptor_t,
        dydesc: ffi::cudnnTensorDescriptor_t,
        convdesc: ffi::cudnnConvolutionDescriptor_t,
        dwdesc: ffi::cudnnFilterDescriptor_t,
        requestedalgocount: :: core :: ffi :: c_int,
        returnedalgocount: *mut :: core :: ffi ::c_int,
        perfresults: *mutffi::cudnnConvolutionBwdFilterAlgoPerf_t,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if returnedalgocount.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if returnedalgocount.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if perfresults.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if perfresults.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnFindConvolutionBackwardFilterAlgorithm").entered();
            let result = ffi::cudnnFindConvolutionBackwardFilterAlgorithm(self.handle, xdesc, dydesc, convdesc, dwdesc, requestedalgocount, returnedalgocount, perfresults);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn find_convolution_backward_filter_algorithm_ex(
        &mut self,
        xdesc: ffi::cudnnTensorDescriptor_t,
        x: *const :: core :: ffi ::c_void,
        dydesc: ffi::cudnnTensorDescriptor_t,
        y: *const :: core :: ffi ::c_void,
        convdesc: ffi::cudnnConvolutionDescriptor_t,
        dwdesc: ffi::cudnnFilterDescriptor_t,
        dw: *mut :: core :: ffi ::c_void,
        requestedalgocount: :: core :: ffi :: c_int,
        returnedalgocount: *mut :: core :: ffi ::c_int,
        perfresults: *mutffi::cudnnConvolutionBwdFilterAlgoPerf_t,
        workspace: *mut :: core :: ffi ::c_void,
        workspacesizeinbytes: usize,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if x.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if x.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if y.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if y.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if dw.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if dw.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if returnedalgocount.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if returnedalgocount.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if perfresults.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if perfresults.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if workspace.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if workspace.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnFindConvolutionBackwardFilterAlgorithmEx").entered();
            let result = ffi::cudnnFindConvolutionBackwardFilterAlgorithmEx(self.handle, xdesc, x, dydesc, y, convdesc, dwdesc, dw, requestedalgocount, returnedalgocount, perfresults, workspace, workspacesizeinbytes);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn get_convolution_backward_filter_algorithm_v_7(
        &mut self,
        srcdesc: ffi::cudnnTensorDescriptor_t,
        diffdesc: ffi::cudnnTensorDescriptor_t,
        convdesc: ffi::cudnnConvolutionDescriptor_t,
        graddesc: ffi::cudnnFilterDescriptor_t,
        requestedalgocount: :: core :: ffi :: c_int,
        returnedalgocount: *mut :: core :: ffi ::c_int,
        perfresults: *mutffi::cudnnConvolutionBwdFilterAlgoPerf_t,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if returnedalgocount.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if returnedalgocount.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if perfresults.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if perfresults.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnGetConvolutionBackwardFilterAlgorithm_v7").entered();
            let result = ffi::cudnnGetConvolutionBackwardFilterAlgorithm_v7(self.handle, srcdesc, diffdesc, convdesc, graddesc, requestedalgocount, returnedalgocount, perfresults);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn get_convolution_backward_filter_workspace_size(
        &mut self,
        xdesc: ffi::cudnnTensorDescriptor_t,
        dydesc: ffi::cudnnTensorDescriptor_t,
        convdesc: ffi::cudnnConvolutionDescriptor_t,
        graddesc: ffi::cudnnFilterDescriptor_t,
        algo: ffi::cudnnConvolutionBwdFilterAlgo_t,
        sizeinbytes: *mutusize,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if sizeinbytes.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if sizeinbytes.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnGetConvolutionBackwardFilterWorkspaceSize").entered();
            let result = ffi::cudnnGetConvolutionBackwardFilterWorkspaceSize(self.handle, xdesc, dydesc, convdesc, graddesc, algo, sizeinbytes);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn convolution_backward_filter(
        &mut self,
        alpha: *const :: core :: ffi ::c_void,
        xdesc: ffi::cudnnTensorDescriptor_t,
        x: *const :: core :: ffi ::c_void,
        dydesc: ffi::cudnnTensorDescriptor_t,
        dy: *const :: core :: ffi ::c_void,
        convdesc: ffi::cudnnConvolutionDescriptor_t,
        algo: ffi::cudnnConvolutionBwdFilterAlgo_t,
        workspace: *mut :: core :: ffi ::c_void,
        workspacesizeinbytes: usize,
        beta: *const :: core :: ffi ::c_void,
        dwdesc: ffi::cudnnFilterDescriptor_t,
        dw: *mut :: core :: ffi ::c_void,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if alpha.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if alpha.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if x.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if x.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if dy.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if dy.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if workspace.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if workspace.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if beta.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if beta.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if dw.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if dw.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnConvolutionBackwardFilter").entered();
            let result = ffi::cudnnConvolutionBackwardFilter(self.handle, alpha, xdesc, x, dydesc, dy, convdesc, algo, workspace, workspacesizeinbytes, beta, dwdesc, dw);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn convolution_backward_bias(
        &mut self,
        alpha: *const :: core :: ffi ::c_void,
        dydesc: ffi::cudnnTensorDescriptor_t,
        dy: *const :: core :: ffi ::c_void,
        beta: *const :: core :: ffi ::c_void,
        dbdesc: ffi::cudnnTensorDescriptor_t,
        db: *mut :: core :: ffi ::c_void,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if alpha.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if alpha.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if dy.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if dy.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if beta.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if beta.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if db.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if db.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnConvolutionBackwardBias").entered();
            let result = ffi::cudnnConvolutionBackwardBias(self.handle, alpha, dydesc, dy, beta, dbdesc, db);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn make_fused_ops_plan(
        &mut self,
        plan: ffi::cudnnFusedOpsPlan_t,
        constpack: ffi::cudnnFusedOpsConstParamPack_t,
        workspacesizeinbytes: *mutusize,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if workspacesizeinbytes.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if workspacesizeinbytes.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnMakeFusedOpsPlan").entered();
            let result = ffi::cudnnMakeFusedOpsPlan(self.handle, plan, constpack, workspacesizeinbytes);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn fused_ops_execute(
        &mut self,
        plan: ffi::cudnnFusedOpsPlan_t,
        varpack: ffi::cudnnFusedOpsVariantParamPack_t,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnFusedOpsExecute").entered();
            let result = ffi::cudnnFusedOpsExecute(self.handle, plan, varpack);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

}

// Additional methods for cudaMipmappedArray_const_t
impl CudaMipmappedArrayConst {
pub fn get_mipmapped_array_level(
    levelarray: *mutffi::cudaArray_t,
    mipmappedarray: ffi::cudaMipmappedArray_const_t,
    level: :: core :: ffi :: c_uint
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaGetMipmappedArrayLevel(levelarray, mipmappedarray, level);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

}

// Additional methods for cudnnAttnDescriptor_t
impl CudnnAttnDescriptor {
    #[inline]
    pub fn set_attn_descriptor(
        &mut self,
        attnmode: :: core :: ffi :: c_uint,
        nheads: :: core :: ffi :: c_int,
        smscaler: f64,
        datatype: ffi::cudnnDataType_t,
        computeprec: ffi::cudnnDataType_t,
        mathtype: ffi::cudnnMathType_t,
        attndropoutdesc: ffi::cudnnDropoutDescriptor_t,
        postdropoutdesc: ffi::cudnnDropoutDescriptor_t,
        qsize: :: core :: ffi :: c_int,
        ksize: :: core :: ffi :: c_int,
        vsize: :: core :: ffi :: c_int,
        qprojsize: :: core :: ffi :: c_int,
        kprojsize: :: core :: ffi :: c_int,
        vprojsize: :: core :: ffi :: c_int,
        oprojsize: :: core :: ffi :: c_int,
        qomaxseqlength: :: core :: ffi :: c_int,
        kvmaxseqlength: :: core :: ffi :: c_int,
        maxbatchsize: :: core :: ffi :: c_int,
        maxbeamsize: :: core :: ffi :: c_int,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnSetAttnDescriptor").entered();
            let result = ffi::cudnnSetAttnDescriptor(self.handle, attnmode, nheads, smscaler, datatype, computeprec, mathtype, attndropoutdesc, postdropoutdesc, qsize, ksize, vsize, qprojsize, kprojsize, vprojsize, oprojsize, qomaxseqlength, kvmaxseqlength, maxbatchsize, maxbeamsize);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn get_attn_descriptor(
        &mut self,
        attnmode: *mut :: core :: ffi ::c_uint,
        nheads: *mut :: core :: ffi ::c_int,
        smscaler: *mutf64,
        datatype: *mutffi::cudnnDataType_t,
        computeprec: *mutffi::cudnnDataType_t,
        mathtype: *mutffi::cudnnMathType_t,
        attndropoutdesc: *mutffi::cudnnDropoutDescriptor_t,
        postdropoutdesc: *mutffi::cudnnDropoutDescriptor_t,
        qsize: *mut :: core :: ffi ::c_int,
        ksize: *mut :: core :: ffi ::c_int,
        vsize: *mut :: core :: ffi ::c_int,
        qprojsize: *mut :: core :: ffi ::c_int,
        kprojsize: *mut :: core :: ffi ::c_int,
        vprojsize: *mut :: core :: ffi ::c_int,
        oprojsize: *mut :: core :: ffi ::c_int,
        qomaxseqlength: *mut :: core :: ffi ::c_int,
        kvmaxseqlength: *mut :: core :: ffi ::c_int,
        maxbatchsize: *mut :: core :: ffi ::c_int,
        maxbeamsize: *mut :: core :: ffi ::c_int,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if attnmode.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if attnmode.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if nheads.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if nheads.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if smscaler.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if smscaler.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if datatype.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if datatype.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if computeprec.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if computeprec.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if mathtype.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if mathtype.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if attndropoutdesc.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if attndropoutdesc.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if postdropoutdesc.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if postdropoutdesc.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if qsize.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if qsize.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if ksize.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if ksize.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if vsize.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if vsize.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if qprojsize.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if qprojsize.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if kprojsize.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if kprojsize.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if vprojsize.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if vprojsize.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if oprojsize.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if oprojsize.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if qomaxseqlength.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if qomaxseqlength.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if kvmaxseqlength.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if kvmaxseqlength.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if maxbatchsize.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if maxbatchsize.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if maxbeamsize.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if maxbeamsize.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnGetAttnDescriptor").entered();
            let result = ffi::cudnnGetAttnDescriptor(self.handle, attnmode, nheads, smscaler, datatype, computeprec, mathtype, attndropoutdesc, postdropoutdesc, qsize, ksize, vsize, qprojsize, kprojsize, vprojsize, oprojsize, qomaxseqlength, kvmaxseqlength, maxbatchsize, maxbeamsize);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

pub fn get_multi_head_attn_buffers(
    handle: ffi::cudnnHandle_t,
    attndesc: ffi::cudnnAttnDescriptor_t,
    weightsizeinbytes: *mutusize,
    workspacesizeinbytes: *mutusize,
    reservespacesizeinbytes: *mutusize
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnGetMultiHeadAttnBuffers(handle, attndesc, weightsizeinbytes, workspacesizeinbytes, reservespacesizeinbytes);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn get_multi_head_attn_weights(
    handle: ffi::cudnnHandle_t,
    attndesc: ffi::cudnnAttnDescriptor_t,
    wkind: ffi::cudnnMultiHeadAttnWeightKind_t,
    weightsizeinbytes: usize,
    weights: *const :: core :: ffi ::c_void,
    wdesc: ffi::cudnnTensorDescriptor_t,
    waddr: *mut *mut :: core :: ffi ::c_void
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnGetMultiHeadAttnWeights(handle, attndesc, wkind, weightsizeinbytes, weights, wdesc, waddr);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn multi_head_attn_forward(
    handle: ffi::cudnnHandle_t,
    attndesc: ffi::cudnnAttnDescriptor_t,
    curridx: :: core :: ffi :: c_int,
    lowinidx: *const :: core :: ffi ::c_int,
    hiwinidx: *const :: core :: ffi ::c_int,
    devseqlengthsqo: *const :: core :: ffi ::c_int,
    devseqlengthskv: *const :: core :: ffi ::c_int,
    qdesc: ffi::cudnnSeqDataDescriptor_t,
    queries: *const :: core :: ffi ::c_void,
    residuals: *const :: core :: ffi ::c_void,
    kdesc: ffi::cudnnSeqDataDescriptor_t,
    keys: *const :: core :: ffi ::c_void,
    vdesc: ffi::cudnnSeqDataDescriptor_t,
    values: *const :: core :: ffi ::c_void,
    odesc: ffi::cudnnSeqDataDescriptor_t,
    out: *mut :: core :: ffi ::c_void,
    weightsizeinbytes: usize,
    weights: *const :: core :: ffi ::c_void,
    workspacesizeinbytes: usize,
    workspace: *mut :: core :: ffi ::c_void,
    reservespacesizeinbytes: usize,
    reservespace: *mut :: core :: ffi ::c_void
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnMultiHeadAttnForward(handle, attndesc, curridx, lowinidx, hiwinidx, devseqlengthsqo, devseqlengthskv, qdesc, queries, residuals, kdesc, keys, vdesc, values, odesc, out, weightsizeinbytes, weights, workspacesizeinbytes, workspace, reservespacesizeinbytes, reservespace);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn multi_head_attn_backward_data(
    handle: ffi::cudnnHandle_t,
    attndesc: ffi::cudnnAttnDescriptor_t,
    lowinidx: *const :: core :: ffi ::c_int,
    hiwinidx: *const :: core :: ffi ::c_int,
    devseqlengthsdqdo: *const :: core :: ffi ::c_int,
    devseqlengthsdkdv: *const :: core :: ffi ::c_int,
    dodesc: ffi::cudnnSeqDataDescriptor_t,
    dout: *const :: core :: ffi ::c_void,
    dqdesc: ffi::cudnnSeqDataDescriptor_t,
    dqueries: *mut :: core :: ffi ::c_void,
    queries: *const :: core :: ffi ::c_void,
    dkdesc: ffi::cudnnSeqDataDescriptor_t,
    dkeys: *mut :: core :: ffi ::c_void,
    keys: *const :: core :: ffi ::c_void,
    dvdesc: ffi::cudnnSeqDataDescriptor_t,
    dvalues: *mut :: core :: ffi ::c_void,
    values: *const :: core :: ffi ::c_void,
    weightsizeinbytes: usize,
    weights: *const :: core :: ffi ::c_void,
    workspacesizeinbytes: usize,
    workspace: *mut :: core :: ffi ::c_void,
    reservespacesizeinbytes: usize,
    reservespace: *mut :: core :: ffi ::c_void
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnMultiHeadAttnBackwardData(handle, attndesc, lowinidx, hiwinidx, devseqlengthsdqdo, devseqlengthsdkdv, dodesc, dout, dqdesc, dqueries, queries, dkdesc, dkeys, keys, dvdesc, dvalues, values, weightsizeinbytes, weights, workspacesizeinbytes, workspace, reservespacesizeinbytes, reservespace);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn multi_head_attn_backward_weights(
    handle: ffi::cudnnHandle_t,
    attndesc: ffi::cudnnAttnDescriptor_t,
    addgrad: ffi::cudnnWgradMode_t,
    qdesc: ffi::cudnnSeqDataDescriptor_t,
    queries: *const :: core :: ffi ::c_void,
    kdesc: ffi::cudnnSeqDataDescriptor_t,
    keys: *const :: core :: ffi ::c_void,
    vdesc: ffi::cudnnSeqDataDescriptor_t,
    values: *const :: core :: ffi ::c_void,
    dodesc: ffi::cudnnSeqDataDescriptor_t,
    dout: *const :: core :: ffi ::c_void,
    weightsizeinbytes: usize,
    weights: *const :: core :: ffi ::c_void,
    dweights: *mut :: core :: ffi ::c_void,
    workspacesizeinbytes: usize,
    workspace: *mut :: core :: ffi ::c_void,
    reservespacesizeinbytes: usize,
    reservespace: *mut :: core :: ffi ::c_void
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnMultiHeadAttnBackwardWeights(handle, attndesc, addgrad, qdesc, queries, kdesc, keys, vdesc, values, dodesc, dout, weightsizeinbytes, weights, dweights, workspacesizeinbytes, workspace, reservespacesizeinbytes, reservespace);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

}

// Additional methods for cudnnBackendDescriptor_t
impl CudnnBackendDescriptor {
pub fn backend_create_descriptor(
    descriptortype: ffi::cudnnBackendDescriptorType_t,
    descriptor: *mutffi::cudnnBackendDescriptor_t
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnBackendCreateDescriptor(descriptortype, descriptor);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

    #[inline]
    pub fn backend_destroy_descriptor(
        &mut self,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnBackendDestroyDescriptor").entered();
            let result = ffi::cudnnBackendDestroyDescriptor(self.handle);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn backend_initialize(
        &mut self,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnBackendInitialize").entered();
            let result = ffi::cudnnBackendInitialize(self.handle);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn backend_finalize(
        &mut self,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnBackendFinalize").entered();
            let result = ffi::cudnnBackendFinalize(self.handle);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn backend_set_attribute(
        &mut self,
        attributename: ffi::cudnnBackendAttributeName_t,
        attributetype: ffi::cudnnBackendAttributeType_t,
        elementcount: i64,
        arrayofelements: *const :: core :: ffi ::c_void,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if arrayofelements.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if arrayofelements.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnBackendSetAttribute").entered();
            let result = ffi::cudnnBackendSetAttribute(self.handle, attributename, attributetype, elementcount, arrayofelements);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn backend_get_attribute(
        &mut self,
        attributename: ffi::cudnnBackendAttributeName_t,
        attributetype: ffi::cudnnBackendAttributeType_t,
        requestedelementcount: i64,
        elementcount: *muti64,
        arrayofelements: *mut :: core :: ffi ::c_void,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if elementcount.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if elementcount.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if arrayofelements.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if arrayofelements.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnBackendGetAttribute").entered();
            let result = ffi::cudnnBackendGetAttribute(self.handle, attributename, attributetype, requestedelementcount, elementcount, arrayofelements);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

pub fn backend_execute(
    handle: ffi::cudnnHandle_t,
    executionplan: ffi::cudnnBackendDescriptor_t,
    variantpack: ffi::cudnnBackendDescriptor_t
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnBackendExecute(handle, executionplan, variantpack);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn backend_populate_cuda_graph(
    handle: ffi::cudnnHandle_t,
    executionplan: ffi::cudnnBackendDescriptor_t,
    variantpack: ffi::cudnnBackendDescriptor_t,
    graph: ffi::cudaGraph_t
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnBackendPopulateCudaGraph(handle, executionplan, variantpack, graph);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn backend_update_cuda_graph(
    handle: ffi::cudnnHandle_t,
    executionplan: ffi::cudnnBackendDescriptor_t,
    variantpack: ffi::cudnnBackendDescriptor_t,
    graph: ffi::cudaGraph_t
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnBackendUpdateCudaGraph(handle, executionplan, variantpack, graph);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

}

// Additional methods for cudaGraphExec_t
impl CudaGraphExec {
    #[inline]
    pub fn graph_exec_get_flags(
        &mut self,
        flags: *mut :: core :: ffi ::c_ulonglong,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if flags.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if flags.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudaGraphExecGetFlags").entered();
            let result = ffi::cudaGraphExecGetFlags(self.handle, flags);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn graph_exec_kernel_node_set_params(
        &mut self,
        node: ffi::cudaGraphNode_t,
        pnodeparams: *constffi::cudaKernelNodeParams,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if pnodeparams.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if pnodeparams.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudaGraphExecKernelNodeSetParams").entered();
            let result = ffi::cudaGraphExecKernelNodeSetParams(self.handle, node, pnodeparams);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn graph_exec_memcpy_node_set_params(
        &mut self,
        node: ffi::cudaGraphNode_t,
        pnodeparams: *constffi::cudaMemcpy3DParms,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if pnodeparams.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if pnodeparams.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudaGraphExecMemcpyNodeSetParams").entered();
            let result = ffi::cudaGraphExecMemcpyNodeSetParams(self.handle, node, pnodeparams);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn graph_exec_memcpy_node_set_params_to_symbol(
        &mut self,
        node: ffi::cudaGraphNode_t,
        symbol: *const :: core :: ffi ::c_void,
        src: *const :: core :: ffi ::c_void,
        count: usize,
        offset: usize,
        kind: ffi::cudaMemcpyKind,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if symbol.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if symbol.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if src.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if src.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudaGraphExecMemcpyNodeSetParamsToSymbol").entered();
            let result = ffi::cudaGraphExecMemcpyNodeSetParamsToSymbol(self.handle, node, symbol, src, count, offset, kind);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn graph_exec_memcpy_node_set_params_from_symbol(
        &mut self,
        node: ffi::cudaGraphNode_t,
        dst: *mut :: core :: ffi ::c_void,
        symbol: *const :: core :: ffi ::c_void,
        count: usize,
        offset: usize,
        kind: ffi::cudaMemcpyKind,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if dst.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if dst.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if symbol.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if symbol.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudaGraphExecMemcpyNodeSetParamsFromSymbol").entered();
            let result = ffi::cudaGraphExecMemcpyNodeSetParamsFromSymbol(self.handle, node, dst, symbol, count, offset, kind);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn graph_exec_memcpy_node_set_params_1d(
        &mut self,
        node: ffi::cudaGraphNode_t,
        dst: *mut :: core :: ffi ::c_void,
        src: *const :: core :: ffi ::c_void,
        count: usize,
        kind: ffi::cudaMemcpyKind,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if dst.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if dst.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if src.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if src.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudaGraphExecMemcpyNodeSetParams1D").entered();
            let result = ffi::cudaGraphExecMemcpyNodeSetParams1D(self.handle, node, dst, src, count, kind);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn graph_exec_memset_node_set_params(
        &mut self,
        node: ffi::cudaGraphNode_t,
        pnodeparams: *constffi::cudaMemsetParams,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if pnodeparams.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if pnodeparams.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudaGraphExecMemsetNodeSetParams").entered();
            let result = ffi::cudaGraphExecMemsetNodeSetParams(self.handle, node, pnodeparams);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn graph_exec_host_node_set_params(
        &mut self,
        node: ffi::cudaGraphNode_t,
        pnodeparams: *constffi::cudaHostNodeParams,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if pnodeparams.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if pnodeparams.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudaGraphExecHostNodeSetParams").entered();
            let result = ffi::cudaGraphExecHostNodeSetParams(self.handle, node, pnodeparams);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn graph_exec_child_graph_node_set_params(
        &mut self,
        node: ffi::cudaGraphNode_t,
        childgraph: ffi::cudaGraph_t,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudaGraphExecChildGraphNodeSetParams").entered();
            let result = ffi::cudaGraphExecChildGraphNodeSetParams(self.handle, node, childgraph);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn graph_exec_event_record_node_set_event(
        &mut self,
        hnode: ffi::cudaGraphNode_t,
        event: ffi::cudaEvent_t,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudaGraphExecEventRecordNodeSetEvent").entered();
            let result = ffi::cudaGraphExecEventRecordNodeSetEvent(self.handle, hnode, event);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn graph_exec_event_wait_node_set_event(
        &mut self,
        hnode: ffi::cudaGraphNode_t,
        event: ffi::cudaEvent_t,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudaGraphExecEventWaitNodeSetEvent").entered();
            let result = ffi::cudaGraphExecEventWaitNodeSetEvent(self.handle, hnode, event);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn graph_exec_external_semaphores_signal_node_set_params(
        &mut self,
        hnode: ffi::cudaGraphNode_t,
        nodeparams: *constffi::cudaExternalSemaphoreSignalNodeParams,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if nodeparams.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if nodeparams.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudaGraphExecExternalSemaphoresSignalNodeSetParams").entered();
            let result = ffi::cudaGraphExecExternalSemaphoresSignalNodeSetParams(self.handle, hnode, nodeparams);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn graph_exec_external_semaphores_wait_node_set_params(
        &mut self,
        hnode: ffi::cudaGraphNode_t,
        nodeparams: *constffi::cudaExternalSemaphoreWaitNodeParams,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if nodeparams.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if nodeparams.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudaGraphExecExternalSemaphoresWaitNodeSetParams").entered();
            let result = ffi::cudaGraphExecExternalSemaphoresWaitNodeSetParams(self.handle, hnode, nodeparams);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn graph_node_set_enabled(
        &mut self,
        hnode: ffi::cudaGraphNode_t,
        isenabled: :: core :: ffi :: c_uint,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudaGraphNodeSetEnabled").entered();
            let result = ffi::cudaGraphNodeSetEnabled(self.handle, hnode, isenabled);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn graph_node_get_enabled(
        &mut self,
        hnode: ffi::cudaGraphNode_t,
        isenabled: *mut :: core :: ffi ::c_uint,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if isenabled.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if isenabled.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudaGraphNodeGetEnabled").entered();
            let result = ffi::cudaGraphNodeGetEnabled(self.handle, hnode, isenabled);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn graph_exec_update(
        &mut self,
        hgraph: ffi::cudaGraph_t,
        resultinfo: *mutffi::cudaGraphExecUpdateResultInfo,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if resultinfo.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if resultinfo.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudaGraphExecUpdate").entered();
            let result = ffi::cudaGraphExecUpdate(self.handle, hgraph, resultinfo);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn graph_upload(
        &mut self,
        stream: ffi::cudaStream_t,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudaGraphUpload").entered();
            let result = ffi::cudaGraphUpload(self.handle, stream);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn graph_launch(
        &mut self,
        stream: ffi::cudaStream_t,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudaGraphLaunch").entered();
            let result = ffi::cudaGraphLaunch(self.handle, stream);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn graph_exec_destroy(
        &mut self,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudaGraphExecDestroy").entered();
            let result = ffi::cudaGraphExecDestroy(self.handle);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn graph_exec_node_set_params(
        &mut self,
        node: ffi::cudaGraphNode_t,
        nodeparams: *mutffi::cudaGraphNodeParams,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if nodeparams.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if nodeparams.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudaGraphExecNodeSetParams").entered();
            let result = ffi::cudaGraphExecNodeSetParams(self.handle, node, nodeparams);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

}

// Additional methods for cudaGraph_t
impl CudaGraph {
pub fn stream_begin_capture_to_graph(
    stream: ffi::cudaStream_t,
    graph: ffi::cudaGraph_t,
    dependencies: *constffi::cudaGraphNode_t,
    dependencydata: *constffi::cudaGraphEdgeData,
    numdependencies: usize,
    mode: ffi::cudaStreamCaptureMode
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaStreamBeginCaptureToGraph(stream, graph, dependencies, dependencydata, numdependencies, mode);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn stream_end_capture(
    stream: ffi::cudaStream_t,
    pgraph: *mutffi::cudaGraph_t
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaStreamEndCapture(stream, pgraph);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn stream_get_capture_info(
    stream: ffi::cudaStream_t,
    capturestatus_out: *mutffi::cudaStreamCaptureStatus,
    id_out: *mut :: core :: ffi ::c_ulonglong,
    graph_out: *mutffi::cudaGraph_t,
    dependencies_out: *mut *constffi::cudaGraphNode_t,
    edgedata_out: *mut *constffi::cudaGraphEdgeData,
    numdependencies_out: *mutusize
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaStreamGetCaptureInfo(stream, capturestatus_out, id_out, graph_out, dependencies_out, edgedata_out, numdependencies_out);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn graph_add_kernel_node(
    pgraphnode: *mutffi::cudaGraphNode_t,
    graph: ffi::cudaGraph_t,
    pdependencies: *constffi::cudaGraphNode_t,
    numdependencies: usize,
    pnodeparams: *constffi::cudaKernelNodeParams
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaGraphAddKernelNode(pgraphnode, graph, pdependencies, numdependencies, pnodeparams);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn graph_add_memcpy_node(
    pgraphnode: *mutffi::cudaGraphNode_t,
    graph: ffi::cudaGraph_t,
    pdependencies: *constffi::cudaGraphNode_t,
    numdependencies: usize,
    pcopyparams: *constffi::cudaMemcpy3DParms
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaGraphAddMemcpyNode(pgraphnode, graph, pdependencies, numdependencies, pcopyparams);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn graph_add_memcpy_node_to_symbol(
    pgraphnode: *mutffi::cudaGraphNode_t,
    graph: ffi::cudaGraph_t,
    pdependencies: *constffi::cudaGraphNode_t,
    numdependencies: usize,
    symbol: *const :: core :: ffi ::c_void,
    src: *const :: core :: ffi ::c_void,
    count: usize,
    offset: usize,
    kind: ffi::cudaMemcpyKind
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaGraphAddMemcpyNodeToSymbol(pgraphnode, graph, pdependencies, numdependencies, symbol, src, count, offset, kind);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn graph_add_memcpy_node_from_symbol(
    pgraphnode: *mutffi::cudaGraphNode_t,
    graph: ffi::cudaGraph_t,
    pdependencies: *constffi::cudaGraphNode_t,
    numdependencies: usize,
    dst: *mut :: core :: ffi ::c_void,
    symbol: *const :: core :: ffi ::c_void,
    count: usize,
    offset: usize,
    kind: ffi::cudaMemcpyKind
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaGraphAddMemcpyNodeFromSymbol(pgraphnode, graph, pdependencies, numdependencies, dst, symbol, count, offset, kind);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn graph_add_memcpy_node_1d(
    pgraphnode: *mutffi::cudaGraphNode_t,
    graph: ffi::cudaGraph_t,
    pdependencies: *constffi::cudaGraphNode_t,
    numdependencies: usize,
    dst: *mut :: core :: ffi ::c_void,
    src: *const :: core :: ffi ::c_void,
    count: usize,
    kind: ffi::cudaMemcpyKind
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaGraphAddMemcpyNode1D(pgraphnode, graph, pdependencies, numdependencies, dst, src, count, kind);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn graph_add_memset_node(
    pgraphnode: *mutffi::cudaGraphNode_t,
    graph: ffi::cudaGraph_t,
    pdependencies: *constffi::cudaGraphNode_t,
    numdependencies: usize,
    pmemsetparams: *constffi::cudaMemsetParams
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaGraphAddMemsetNode(pgraphnode, graph, pdependencies, numdependencies, pmemsetparams);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn graph_add_host_node(
    pgraphnode: *mutffi::cudaGraphNode_t,
    graph: ffi::cudaGraph_t,
    pdependencies: *constffi::cudaGraphNode_t,
    numdependencies: usize,
    pnodeparams: *constffi::cudaHostNodeParams
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaGraphAddHostNode(pgraphnode, graph, pdependencies, numdependencies, pnodeparams);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn graph_add_child_graph_node(
    pgraphnode: *mutffi::cudaGraphNode_t,
    graph: ffi::cudaGraph_t,
    pdependencies: *constffi::cudaGraphNode_t,
    numdependencies: usize,
    childgraph: ffi::cudaGraph_t
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaGraphAddChildGraphNode(pgraphnode, graph, pdependencies, numdependencies, childgraph);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn graph_child_graph_node_get_graph(
    node: ffi::cudaGraphNode_t,
    pgraph: *mutffi::cudaGraph_t
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaGraphChildGraphNodeGetGraph(node, pgraph);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn graph_add_empty_node(
    pgraphnode: *mutffi::cudaGraphNode_t,
    graph: ffi::cudaGraph_t,
    pdependencies: *constffi::cudaGraphNode_t,
    numdependencies: usize
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaGraphAddEmptyNode(pgraphnode, graph, pdependencies, numdependencies);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn graph_add_event_record_node(
    pgraphnode: *mutffi::cudaGraphNode_t,
    graph: ffi::cudaGraph_t,
    pdependencies: *constffi::cudaGraphNode_t,
    numdependencies: usize,
    event: ffi::cudaEvent_t
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaGraphAddEventRecordNode(pgraphnode, graph, pdependencies, numdependencies, event);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn graph_add_event_wait_node(
    pgraphnode: *mutffi::cudaGraphNode_t,
    graph: ffi::cudaGraph_t,
    pdependencies: *constffi::cudaGraphNode_t,
    numdependencies: usize,
    event: ffi::cudaEvent_t
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaGraphAddEventWaitNode(pgraphnode, graph, pdependencies, numdependencies, event);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn graph_add_external_semaphores_signal_node(
    pgraphnode: *mutffi::cudaGraphNode_t,
    graph: ffi::cudaGraph_t,
    pdependencies: *constffi::cudaGraphNode_t,
    numdependencies: usize,
    nodeparams: *constffi::cudaExternalSemaphoreSignalNodeParams
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaGraphAddExternalSemaphoresSignalNode(pgraphnode, graph, pdependencies, numdependencies, nodeparams);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn graph_add_external_semaphores_wait_node(
    pgraphnode: *mutffi::cudaGraphNode_t,
    graph: ffi::cudaGraph_t,
    pdependencies: *constffi::cudaGraphNode_t,
    numdependencies: usize,
    nodeparams: *constffi::cudaExternalSemaphoreWaitNodeParams
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaGraphAddExternalSemaphoresWaitNode(pgraphnode, graph, pdependencies, numdependencies, nodeparams);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn graph_add_mem_alloc_node(
    pgraphnode: *mutffi::cudaGraphNode_t,
    graph: ffi::cudaGraph_t,
    pdependencies: *constffi::cudaGraphNode_t,
    numdependencies: usize,
    nodeparams: *mutffi::cudaMemAllocNodeParams
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaGraphAddMemAllocNode(pgraphnode, graph, pdependencies, numdependencies, nodeparams);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn graph_add_mem_free_node(
    pgraphnode: *mutffi::cudaGraphNode_t,
    graph: ffi::cudaGraph_t,
    pdependencies: *constffi::cudaGraphNode_t,
    numdependencies: usize,
    dptr: *mut :: core :: ffi ::c_void
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaGraphAddMemFreeNode(pgraphnode, graph, pdependencies, numdependencies, dptr);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn graph_node_find_in_clone(
    pnode: *mutffi::cudaGraphNode_t,
    originalnode: ffi::cudaGraphNode_t,
    clonedgraph: ffi::cudaGraph_t
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaGraphNodeFindInClone(pnode, originalnode, clonedgraph);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

    #[inline]
    pub fn graph_get_nodes(
        &mut self,
        nodes: *mutffi::cudaGraphNode_t,
        numnodes: *mutusize,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if nodes.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if nodes.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if numnodes.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if numnodes.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudaGraphGetNodes").entered();
            let result = ffi::cudaGraphGetNodes(self.handle, nodes, numnodes);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn graph_get_root_nodes(
        &mut self,
        prootnodes: *mutffi::cudaGraphNode_t,
        pnumrootnodes: *mutusize,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if prootnodes.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if prootnodes.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if pnumrootnodes.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if pnumrootnodes.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudaGraphGetRootNodes").entered();
            let result = ffi::cudaGraphGetRootNodes(self.handle, prootnodes, pnumrootnodes);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn graph_get_edges(
        &mut self,
        from: *mutffi::cudaGraphNode_t,
        to: *mutffi::cudaGraphNode_t,
        edgedata: *mutffi::cudaGraphEdgeData,
        numedges: *mutusize,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if from.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if from.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if to.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if to.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if edgedata.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if edgedata.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if numedges.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if numedges.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudaGraphGetEdges").entered();
            let result = ffi::cudaGraphGetEdges(self.handle, from, to, edgedata, numedges);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn graph_add_dependencies(
        &mut self,
        from: *constffi::cudaGraphNode_t,
        to: *constffi::cudaGraphNode_t,
        edgedata: *constffi::cudaGraphEdgeData,
        numdependencies: usize,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if from.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if from.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if to.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if to.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if edgedata.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if edgedata.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudaGraphAddDependencies").entered();
            let result = ffi::cudaGraphAddDependencies(self.handle, from, to, edgedata, numdependencies);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn graph_remove_dependencies(
        &mut self,
        from: *constffi::cudaGraphNode_t,
        to: *constffi::cudaGraphNode_t,
        edgedata: *constffi::cudaGraphEdgeData,
        numdependencies: usize,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if from.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if from.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if to.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if to.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if edgedata.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if edgedata.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudaGraphRemoveDependencies").entered();
            let result = ffi::cudaGraphRemoveDependencies(self.handle, from, to, edgedata, numdependencies);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

pub fn graph_instantiate(
    pgraphexec: *mutffi::cudaGraphExec_t,
    graph: ffi::cudaGraph_t,
    flags: :: core :: ffi :: c_ulonglong
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaGraphInstantiate(pgraphexec, graph, flags);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn graph_instantiate_with_flags(
    pgraphexec: *mutffi::cudaGraphExec_t,
    graph: ffi::cudaGraph_t,
    flags: :: core :: ffi :: c_ulonglong
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaGraphInstantiateWithFlags(pgraphexec, graph, flags);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn graph_instantiate_with_params(
    pgraphexec: *mutffi::cudaGraphExec_t,
    graph: ffi::cudaGraph_t,
    instantiateparams: *mutffi::cudaGraphInstantiateParams
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaGraphInstantiateWithParams(pgraphexec, graph, instantiateparams);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn graph_exec_child_graph_node_set_params(
    hgraphexec: ffi::cudaGraphExec_t,
    node: ffi::cudaGraphNode_t,
    childgraph: ffi::cudaGraph_t
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaGraphExecChildGraphNodeSetParams(hgraphexec, node, childgraph);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn graph_exec_update(
    hgraphexec: ffi::cudaGraphExec_t,
    hgraph: ffi::cudaGraph_t,
    resultinfo: *mutffi::cudaGraphExecUpdateResultInfo
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaGraphExecUpdate(hgraphexec, hgraph, resultinfo);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

    #[inline]
    pub fn graph_debug_dot_print(
        &mut self,
        path: &str,
        flags: :: core :: ffi :: c_uint,
) -> Result<(), Error> {
        unsafe {
            let path_cstr = std::ffi::CString::new(path).map_err(|_| Error::NullPointer)?;
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudaGraphDebugDotPrint").entered();
            let result = ffi::cudaGraphDebugDotPrint(self.handle, path_cstr.as_ptr(), flags);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn graph_retain_user_object(
        &mut self,
        object: ffi::cudaUserObject_t,
        count: :: core :: ffi :: c_uint,
        flags: :: core :: ffi :: c_uint,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudaGraphRetainUserObject").entered();
            let result = ffi::cudaGraphRetainUserObject(self.handle, object, count, flags);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

pub fn graph_add_node(
    pgraphnode: *mutffi::cudaGraphNode_t,
    graph: ffi::cudaGraph_t,
    pdependencies: *constffi::cudaGraphNode_t,
    dependencydata: *constffi::cudaGraphEdgeData,
    numdependencies: usize,
    nodeparams: *mutffi::cudaGraphNodeParams
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaGraphAddNode(pgraphnode, graph, pdependencies, dependencydata, numdependencies, nodeparams);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn graph_conditional_handle_create(
    phandle_out: *mutffi::cudaGraphConditionalHandle,
    graph: ffi::cudaGraph_t,
    defaultlaunchvalue: :: core :: ffi :: c_uint,
    flags: :: core :: ffi :: c_uint
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaGraphConditionalHandleCreate(phandle_out, graph, defaultlaunchvalue, flags);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn backend_populate_cuda_graph(
    handle: ffi::cudnnHandle_t,
    executionplan: ffi::cudnnBackendDescriptor_t,
    variantpack: ffi::cudnnBackendDescriptor_t,
    graph: ffi::cudaGraph_t
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnBackendPopulateCudaGraph(handle, executionplan, variantpack, graph);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn backend_update_cuda_graph(
    handle: ffi::cudnnHandle_t,
    executionplan: ffi::cudnnBackendDescriptor_t,
    variantpack: ffi::cudnnBackendDescriptor_t,
    graph: ffi::cudaGraph_t
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnBackendUpdateCudaGraph(handle, executionplan, variantpack, graph);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

}

// Additional methods for cudaExternalMemory_t
impl CudaExternalMemory {
pub fn external_memory_get_mapped_buffer(
    devptr: *mut *mut :: core :: ffi ::c_void,
    extmem: ffi::cudaExternalMemory_t,
    bufferdesc: *constffi::cudaExternalMemoryBufferDesc
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaExternalMemoryGetMappedBuffer(devptr, extmem, bufferdesc);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn external_memory_get_mapped_mipmapped_array(
    mipmap: *mutffi::cudaMipmappedArray_t,
    extmem: ffi::cudaExternalMemory_t,
    mipmapdesc: *constffi::cudaExternalMemoryMipmappedArrayDesc
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaExternalMemoryGetMappedMipmappedArray(mipmap, extmem, mipmapdesc);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

    #[inline]
    pub fn destroy_external_memory(
        &mut self,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudaDestroyExternalMemory").entered();
            let result = ffi::cudaDestroyExternalMemory(self.handle);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

}

// Additional methods for cudnnFilterDescriptor_t
impl CudnnFilterDescriptor {
    #[inline]
    pub fn set_filter_4d_descriptor(
        &mut self,
        datatype: ffi::cudnnDataType_t,
        format: ffi::cudnnTensorFormat_t,
        k: :: core :: ffi :: c_int,
        c: :: core :: ffi :: c_int,
        h: :: core :: ffi :: c_int,
        w: :: core :: ffi :: c_int,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnSetFilter4dDescriptor").entered();
            let result = ffi::cudnnSetFilter4dDescriptor(self.handle, datatype, format, k, c, h, w);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn get_filter_4d_descriptor(
        &mut self,
        datatype: *mutffi::cudnnDataType_t,
        format: *mutffi::cudnnTensorFormat_t,
        k: *mut :: core :: ffi ::c_int,
        c: *mut :: core :: ffi ::c_int,
        h: *mut :: core :: ffi ::c_int,
        w: *mut :: core :: ffi ::c_int,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if datatype.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if datatype.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if format.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if format.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if k.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if k.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if c.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if c.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if h.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if h.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if w.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if w.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnGetFilter4dDescriptor").entered();
            let result = ffi::cudnnGetFilter4dDescriptor(self.handle, datatype, format, k, c, h, w);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn set_filter_nd_descriptor(
        &mut self,
        datatype: ffi::cudnnDataType_t,
        format: ffi::cudnnTensorFormat_t,
        nbdims: :: core :: ffi :: c_int,
        filterdima: *const :: core :: ffi ::c_int,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if filterdima.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if filterdima.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnSetFilterNdDescriptor").entered();
            let result = ffi::cudnnSetFilterNdDescriptor(self.handle, datatype, format, nbdims, filterdima);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn get_filter_nd_descriptor(
        &mut self,
        nbdimsrequested: :: core :: ffi :: c_int,
        datatype: *mutffi::cudnnDataType_t,
        format: *mutffi::cudnnTensorFormat_t,
        nbdims: *mut :: core :: ffi ::c_int,
        filterdima: *mut :: core :: ffi ::c_int,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if datatype.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if datatype.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if format.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if format.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if nbdims.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if nbdims.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if filterdima.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if filterdima.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnGetFilterNdDescriptor").entered();
            let result = ffi::cudnnGetFilterNdDescriptor(self.handle, nbdimsrequested, datatype, format, nbdims, filterdima);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn get_filter_size_in_bytes(
        &mut self,
        size: *mutusize,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if size.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if size.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnGetFilterSizeInBytes").entered();
            let result = ffi::cudnnGetFilterSizeInBytes(self.handle, size);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

pub fn transform_filter(
    handle: ffi::cudnnHandle_t,
    transdesc: ffi::cudnnTensorTransformDescriptor_t,
    alpha: *const :: core :: ffi ::c_void,
    srcdesc: ffi::cudnnFilterDescriptor_t,
    srcdata: *const :: core :: ffi ::c_void,
    beta: *const :: core :: ffi ::c_void,
    destdesc: ffi::cudnnFilterDescriptor_t,
    destdata: *mut :: core :: ffi ::c_void
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnTransformFilter(handle, transdesc, alpha, srcdesc, srcdata, beta, destdesc, destdata);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn get_convolution_2d_forward_output_dim(
    convdesc: ffi::cudnnConvolutionDescriptor_t,
    inputtensordesc: ffi::cudnnTensorDescriptor_t,
    filterdesc: ffi::cudnnFilterDescriptor_t,
    n: *mut :: core :: ffi ::c_int,
    c: *mut :: core :: ffi ::c_int,
    h: *mut :: core :: ffi ::c_int,
    w: *mut :: core :: ffi ::c_int
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnGetConvolution2dForwardOutputDim(convdesc, inputtensordesc, filterdesc, n, c, h, w);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn get_convolution_nd_forward_output_dim(
    convdesc: ffi::cudnnConvolutionDescriptor_t,
    inputtensordesc: ffi::cudnnTensorDescriptor_t,
    filterdesc: ffi::cudnnFilterDescriptor_t,
    nbdims: :: core :: ffi :: c_int,
    tensorouputdima: *mut :: core :: ffi ::c_int
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnGetConvolutionNdForwardOutputDim(convdesc, inputtensordesc, filterdesc, nbdims, tensorouputdima);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn get_convolution_forward_algorithm_v_7(
    handle: ffi::cudnnHandle_t,
    srcdesc: ffi::cudnnTensorDescriptor_t,
    filterdesc: ffi::cudnnFilterDescriptor_t,
    convdesc: ffi::cudnnConvolutionDescriptor_t,
    destdesc: ffi::cudnnTensorDescriptor_t,
    requestedalgocount: :: core :: ffi :: c_int,
    returnedalgocount: *mut :: core :: ffi ::c_int,
    perfresults: *mutffi::cudnnConvolutionFwdAlgoPerf_t
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnGetConvolutionForwardAlgorithm_v7(handle, srcdesc, filterdesc, convdesc, destdesc, requestedalgocount, returnedalgocount, perfresults);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn find_convolution_forward_algorithm(
    handle: ffi::cudnnHandle_t,
    xdesc: ffi::cudnnTensorDescriptor_t,
    wdesc: ffi::cudnnFilterDescriptor_t,
    convdesc: ffi::cudnnConvolutionDescriptor_t,
    ydesc: ffi::cudnnTensorDescriptor_t,
    requestedalgocount: :: core :: ffi :: c_int,
    returnedalgocount: *mut :: core :: ffi ::c_int,
    perfresults: *mutffi::cudnnConvolutionFwdAlgoPerf_t
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnFindConvolutionForwardAlgorithm(handle, xdesc, wdesc, convdesc, ydesc, requestedalgocount, returnedalgocount, perfresults);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn find_convolution_forward_algorithm_ex(
    handle: ffi::cudnnHandle_t,
    xdesc: ffi::cudnnTensorDescriptor_t,
    x: *const :: core :: ffi ::c_void,
    wdesc: ffi::cudnnFilterDescriptor_t,
    w: *const :: core :: ffi ::c_void,
    convdesc: ffi::cudnnConvolutionDescriptor_t,
    ydesc: ffi::cudnnTensorDescriptor_t,
    y: *mut :: core :: ffi ::c_void,
    requestedalgocount: :: core :: ffi :: c_int,
    returnedalgocount: *mut :: core :: ffi ::c_int,
    perfresults: *mutffi::cudnnConvolutionFwdAlgoPerf_t,
    workspace: *mut :: core :: ffi ::c_void,
    workspacesizeinbytes: usize
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnFindConvolutionForwardAlgorithmEx(handle, xdesc, x, wdesc, w, convdesc, ydesc, y, requestedalgocount, returnedalgocount, perfresults, workspace, workspacesizeinbytes);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn im_2col(
    handle: ffi::cudnnHandle_t,
    xdesc: ffi::cudnnTensorDescriptor_t,
    x: *const :: core :: ffi ::c_void,
    wdesc: ffi::cudnnFilterDescriptor_t,
    convdesc: ffi::cudnnConvolutionDescriptor_t,
    colbuffer: *mut :: core :: ffi ::c_void
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnIm2Col(handle, xdesc, x, wdesc, convdesc, colbuffer);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn reorder_filter_and_bias(
    handle: ffi::cudnnHandle_t,
    filterdesc: ffi::cudnnFilterDescriptor_t,
    reordertype: ffi::cudnnReorderType_t,
    filterdata: *const :: core :: ffi ::c_void,
    reorderedfilterdata: *mut :: core :: ffi ::c_void,
    reorderbias: :: core :: ffi :: c_int,
    biasdata: *const :: core :: ffi ::c_void,
    reorderedbiasdata: *mut :: core :: ffi ::c_void
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnReorderFilterAndBias(handle, filterdesc, reordertype, filterdata, reorderedfilterdata, reorderbias, biasdata, reorderedbiasdata);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn get_convolution_forward_workspace_size(
    handle: ffi::cudnnHandle_t,
    xdesc: ffi::cudnnTensorDescriptor_t,
    wdesc: ffi::cudnnFilterDescriptor_t,
    convdesc: ffi::cudnnConvolutionDescriptor_t,
    ydesc: ffi::cudnnTensorDescriptor_t,
    algo: ffi::cudnnConvolutionFwdAlgo_t,
    sizeinbytes: *mutusize
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnGetConvolutionForwardWorkspaceSize(handle, xdesc, wdesc, convdesc, ydesc, algo, sizeinbytes);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn convolution_forward(
    handle: ffi::cudnnHandle_t,
    alpha: *const :: core :: ffi ::c_void,
    xdesc: ffi::cudnnTensorDescriptor_t,
    x: *const :: core :: ffi ::c_void,
    wdesc: ffi::cudnnFilterDescriptor_t,
    w: *const :: core :: ffi ::c_void,
    convdesc: ffi::cudnnConvolutionDescriptor_t,
    algo: ffi::cudnnConvolutionFwdAlgo_t,
    workspace: *mut :: core :: ffi ::c_void,
    workspacesizeinbytes: usize,
    beta: *const :: core :: ffi ::c_void,
    ydesc: ffi::cudnnTensorDescriptor_t,
    y: *mut :: core :: ffi ::c_void
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnConvolutionForward(handle, alpha, xdesc, x, wdesc, w, convdesc, algo, workspace, workspacesizeinbytes, beta, ydesc, y);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn convolution_bias_activation_forward(
    handle: ffi::cudnnHandle_t,
    alpha1: *const :: core :: ffi ::c_void,
    xdesc: ffi::cudnnTensorDescriptor_t,
    x: *const :: core :: ffi ::c_void,
    wdesc: ffi::cudnnFilterDescriptor_t,
    w: *const :: core :: ffi ::c_void,
    convdesc: ffi::cudnnConvolutionDescriptor_t,
    algo: ffi::cudnnConvolutionFwdAlgo_t,
    workspace: *mut :: core :: ffi ::c_void,
    workspacesizeinbytes: usize,
    alpha2: *const :: core :: ffi ::c_void,
    zdesc: ffi::cudnnTensorDescriptor_t,
    z: *const :: core :: ffi ::c_void,
    biasdesc: ffi::cudnnTensorDescriptor_t,
    bias: *const :: core :: ffi ::c_void,
    activationdesc: ffi::cudnnActivationDescriptor_t,
    ydesc: ffi::cudnnTensorDescriptor_t,
    y: *mut :: core :: ffi ::c_void
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnConvolutionBiasActivationForward(handle, alpha1, xdesc, x, wdesc, w, convdesc, algo, workspace, workspacesizeinbytes, alpha2, zdesc, z, biasdesc, bias, activationdesc, ydesc, y);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn find_convolution_backward_data_algorithm(
    handle: ffi::cudnnHandle_t,
    wdesc: ffi::cudnnFilterDescriptor_t,
    dydesc: ffi::cudnnTensorDescriptor_t,
    convdesc: ffi::cudnnConvolutionDescriptor_t,
    dxdesc: ffi::cudnnTensorDescriptor_t,
    requestedalgocount: :: core :: ffi :: c_int,
    returnedalgocount: *mut :: core :: ffi ::c_int,
    perfresults: *mutffi::cudnnConvolutionBwdDataAlgoPerf_t
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnFindConvolutionBackwardDataAlgorithm(handle, wdesc, dydesc, convdesc, dxdesc, requestedalgocount, returnedalgocount, perfresults);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn find_convolution_backward_data_algorithm_ex(
    handle: ffi::cudnnHandle_t,
    wdesc: ffi::cudnnFilterDescriptor_t,
    w: *const :: core :: ffi ::c_void,
    dydesc: ffi::cudnnTensorDescriptor_t,
    dy: *const :: core :: ffi ::c_void,
    convdesc: ffi::cudnnConvolutionDescriptor_t,
    dxdesc: ffi::cudnnTensorDescriptor_t,
    dx: *mut :: core :: ffi ::c_void,
    requestedalgocount: :: core :: ffi :: c_int,
    returnedalgocount: *mut :: core :: ffi ::c_int,
    perfresults: *mutffi::cudnnConvolutionBwdDataAlgoPerf_t,
    workspace: *mut :: core :: ffi ::c_void,
    workspacesizeinbytes: usize
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnFindConvolutionBackwardDataAlgorithmEx(handle, wdesc, w, dydesc, dy, convdesc, dxdesc, dx, requestedalgocount, returnedalgocount, perfresults, workspace, workspacesizeinbytes);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn get_convolution_backward_data_algorithm_v_7(
    handle: ffi::cudnnHandle_t,
    filterdesc: ffi::cudnnFilterDescriptor_t,
    diffdesc: ffi::cudnnTensorDescriptor_t,
    convdesc: ffi::cudnnConvolutionDescriptor_t,
    graddesc: ffi::cudnnTensorDescriptor_t,
    requestedalgocount: :: core :: ffi :: c_int,
    returnedalgocount: *mut :: core :: ffi ::c_int,
    perfresults: *mutffi::cudnnConvolutionBwdDataAlgoPerf_t
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnGetConvolutionBackwardDataAlgorithm_v7(handle, filterdesc, diffdesc, convdesc, graddesc, requestedalgocount, returnedalgocount, perfresults);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn get_convolution_backward_data_workspace_size(
    handle: ffi::cudnnHandle_t,
    wdesc: ffi::cudnnFilterDescriptor_t,
    dydesc: ffi::cudnnTensorDescriptor_t,
    convdesc: ffi::cudnnConvolutionDescriptor_t,
    dxdesc: ffi::cudnnTensorDescriptor_t,
    algo: ffi::cudnnConvolutionBwdDataAlgo_t,
    sizeinbytes: *mutusize
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnGetConvolutionBackwardDataWorkspaceSize(handle, wdesc, dydesc, convdesc, dxdesc, algo, sizeinbytes);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn convolution_backward_data(
    handle: ffi::cudnnHandle_t,
    alpha: *const :: core :: ffi ::c_void,
    wdesc: ffi::cudnnFilterDescriptor_t,
    w: *const :: core :: ffi ::c_void,
    dydesc: ffi::cudnnTensorDescriptor_t,
    dy: *const :: core :: ffi ::c_void,
    convdesc: ffi::cudnnConvolutionDescriptor_t,
    algo: ffi::cudnnConvolutionBwdDataAlgo_t,
    workspace: *mut :: core :: ffi ::c_void,
    workspacesizeinbytes: usize,
    beta: *const :: core :: ffi ::c_void,
    dxdesc: ffi::cudnnTensorDescriptor_t,
    dx: *mut :: core :: ffi ::c_void
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnConvolutionBackwardData(handle, alpha, wdesc, w, dydesc, dy, convdesc, algo, workspace, workspacesizeinbytes, beta, dxdesc, dx);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn get_folded_conv_backward_data_descriptors(
    handle: ffi::cudnnHandle_t,
    filterdesc: ffi::cudnnFilterDescriptor_t,
    diffdesc: ffi::cudnnTensorDescriptor_t,
    convdesc: ffi::cudnnConvolutionDescriptor_t,
    graddesc: ffi::cudnnTensorDescriptor_t,
    transformformat: ffi::cudnnTensorFormat_t,
    foldedfilterdesc: ffi::cudnnFilterDescriptor_t,
    paddeddiffdesc: ffi::cudnnTensorDescriptor_t,
    foldedconvdesc: ffi::cudnnConvolutionDescriptor_t,
    foldedgraddesc: ffi::cudnnTensorDescriptor_t,
    filterfoldtransdesc: ffi::cudnnTensorTransformDescriptor_t,
    diffpadtransdesc: ffi::cudnnTensorTransformDescriptor_t,
    gradfoldtransdesc: ffi::cudnnTensorTransformDescriptor_t,
    gradunfoldtransdesc: ffi::cudnnTensorTransformDescriptor_t
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnGetFoldedConvBackwardDataDescriptors(handle, filterdesc, diffdesc, convdesc, graddesc, transformformat, foldedfilterdesc, paddeddiffdesc, foldedconvdesc, foldedgraddesc, filterfoldtransdesc, diffpadtransdesc, gradfoldtransdesc, gradunfoldtransdesc);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn find_convolution_backward_filter_algorithm(
    handle: ffi::cudnnHandle_t,
    xdesc: ffi::cudnnTensorDescriptor_t,
    dydesc: ffi::cudnnTensorDescriptor_t,
    convdesc: ffi::cudnnConvolutionDescriptor_t,
    dwdesc: ffi::cudnnFilterDescriptor_t,
    requestedalgocount: :: core :: ffi :: c_int,
    returnedalgocount: *mut :: core :: ffi ::c_int,
    perfresults: *mutffi::cudnnConvolutionBwdFilterAlgoPerf_t
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnFindConvolutionBackwardFilterAlgorithm(handle, xdesc, dydesc, convdesc, dwdesc, requestedalgocount, returnedalgocount, perfresults);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn find_convolution_backward_filter_algorithm_ex(
    handle: ffi::cudnnHandle_t,
    xdesc: ffi::cudnnTensorDescriptor_t,
    x: *const :: core :: ffi ::c_void,
    dydesc: ffi::cudnnTensorDescriptor_t,
    y: *const :: core :: ffi ::c_void,
    convdesc: ffi::cudnnConvolutionDescriptor_t,
    dwdesc: ffi::cudnnFilterDescriptor_t,
    dw: *mut :: core :: ffi ::c_void,
    requestedalgocount: :: core :: ffi :: c_int,
    returnedalgocount: *mut :: core :: ffi ::c_int,
    perfresults: *mutffi::cudnnConvolutionBwdFilterAlgoPerf_t,
    workspace: *mut :: core :: ffi ::c_void,
    workspacesizeinbytes: usize
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnFindConvolutionBackwardFilterAlgorithmEx(handle, xdesc, x, dydesc, y, convdesc, dwdesc, dw, requestedalgocount, returnedalgocount, perfresults, workspace, workspacesizeinbytes);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn get_convolution_backward_filter_algorithm_v_7(
    handle: ffi::cudnnHandle_t,
    srcdesc: ffi::cudnnTensorDescriptor_t,
    diffdesc: ffi::cudnnTensorDescriptor_t,
    convdesc: ffi::cudnnConvolutionDescriptor_t,
    graddesc: ffi::cudnnFilterDescriptor_t,
    requestedalgocount: :: core :: ffi :: c_int,
    returnedalgocount: *mut :: core :: ffi ::c_int,
    perfresults: *mutffi::cudnnConvolutionBwdFilterAlgoPerf_t
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnGetConvolutionBackwardFilterAlgorithm_v7(handle, srcdesc, diffdesc, convdesc, graddesc, requestedalgocount, returnedalgocount, perfresults);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn get_convolution_backward_filter_workspace_size(
    handle: ffi::cudnnHandle_t,
    xdesc: ffi::cudnnTensorDescriptor_t,
    dydesc: ffi::cudnnTensorDescriptor_t,
    convdesc: ffi::cudnnConvolutionDescriptor_t,
    graddesc: ffi::cudnnFilterDescriptor_t,
    algo: ffi::cudnnConvolutionBwdFilterAlgo_t,
    sizeinbytes: *mutusize
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnGetConvolutionBackwardFilterWorkspaceSize(handle, xdesc, dydesc, convdesc, graddesc, algo, sizeinbytes);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn convolution_backward_filter(
    handle: ffi::cudnnHandle_t,
    alpha: *const :: core :: ffi ::c_void,
    xdesc: ffi::cudnnTensorDescriptor_t,
    x: *const :: core :: ffi ::c_void,
    dydesc: ffi::cudnnTensorDescriptor_t,
    dy: *const :: core :: ffi ::c_void,
    convdesc: ffi::cudnnConvolutionDescriptor_t,
    algo: ffi::cudnnConvolutionBwdFilterAlgo_t,
    workspace: *mut :: core :: ffi ::c_void,
    workspacesizeinbytes: usize,
    beta: *const :: core :: ffi ::c_void,
    dwdesc: ffi::cudnnFilterDescriptor_t,
    dw: *mut :: core :: ffi ::c_void
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnConvolutionBackwardFilter(handle, alpha, xdesc, x, dydesc, dy, convdesc, algo, workspace, workspacesizeinbytes, beta, dwdesc, dw);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

}

// Additional methods for cudaHostFn_t
impl CudaHostFn {
pub fn launch_host_func(
    stream: ffi::cudaStream_t,
    fn_: ffi::cudaHostFn_t,
    userdata: *mut :: core :: ffi ::c_void
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaLaunchHostFunc(stream, fn_, userdata);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn user_object_create(
    object_out: *mutffi::cudaUserObject_t,
    ptr: *mut :: core :: ffi ::c_void,
    destroy: ffi::cudaHostFn_t,
    initialrefcount: :: core :: ffi :: c_uint,
    flags: :: core :: ffi :: c_uint
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaUserObjectCreate(object_out, ptr, destroy, initialrefcount, flags);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

}

// Additional methods for cudnnRNNDescriptor_t
impl CudnnRNNDescriptor {
    #[inline]
    pub fn set_rnn_descriptor_v_8(
        &mut self,
        algo: ffi::cudnnRNNAlgo_t,
        cellmode: ffi::cudnnRNNMode_t,
        biasmode: ffi::cudnnRNNBiasMode_t,
        dirmode: ffi::cudnnDirectionMode_t,
        inputmode: ffi::cudnnRNNInputMode_t,
        datatype: ffi::cudnnDataType_t,
        mathprec: ffi::cudnnDataType_t,
        mathtype: ffi::cudnnMathType_t,
        inputsize: i32,
        hiddensize: i32,
        projsize: i32,
        numlayers: i32,
        dropoutdesc: ffi::cudnnDropoutDescriptor_t,
        auxflags: u32,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnSetRNNDescriptor_v8").entered();
            let result = ffi::cudnnSetRNNDescriptor_v8(self.handle, algo, cellmode, biasmode, dirmode, inputmode, datatype, mathprec, mathtype, inputsize, hiddensize, projsize, numlayers, dropoutdesc, auxflags);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn get_rnn_descriptor_v_8(
        &mut self,
        algo: *mutffi::cudnnRNNAlgo_t,
        cellmode: *mutffi::cudnnRNNMode_t,
        biasmode: *mutffi::cudnnRNNBiasMode_t,
        dirmode: *mutffi::cudnnDirectionMode_t,
        inputmode: *mutffi::cudnnRNNInputMode_t,
        datatype: *mutffi::cudnnDataType_t,
        mathprec: *mutffi::cudnnDataType_t,
        mathtype: *mutffi::cudnnMathType_t,
        inputsize: *muti32,
        hiddensize: *muti32,
        projsize: *muti32,
        numlayers: *muti32,
        dropoutdesc: *mutffi::cudnnDropoutDescriptor_t,
        auxflags: *mutu32,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if algo.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if algo.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if cellmode.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if cellmode.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if biasmode.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if biasmode.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if dirmode.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if dirmode.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if inputmode.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if inputmode.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if datatype.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if datatype.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if mathprec.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if mathprec.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if mathtype.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if mathtype.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if inputsize.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if inputsize.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if hiddensize.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if hiddensize.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if projsize.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if projsize.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if numlayers.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if numlayers.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if dropoutdesc.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if dropoutdesc.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if auxflags.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if auxflags.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnGetRNNDescriptor_v8").entered();
            let result = ffi::cudnnGetRNNDescriptor_v8(self.handle, algo, cellmode, biasmode, dirmode, inputmode, datatype, mathprec, mathtype, inputsize, hiddensize, projsize, numlayers, dropoutdesc, auxflags);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn rnn_set_clip_v_8(
        &mut self,
        clipmode: ffi::cudnnRNNClipMode_t,
        clipnanopt: ffi::cudnnNanPropagation_t,
        lclip: f64,
        rclip: f64,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnRNNSetClip_v8").entered();
            let result = ffi::cudnnRNNSetClip_v8(self.handle, clipmode, clipnanopt, lclip, rclip);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn rnn_set_clip_v_9(
        &mut self,
        clipmode: ffi::cudnnRNNClipMode_t,
        lclip: f64,
        rclip: f64,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnRNNSetClip_v9").entered();
            let result = ffi::cudnnRNNSetClip_v9(self.handle, clipmode, lclip, rclip);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn rnn_get_clip_v_8(
        &mut self,
        clipmode: *mutffi::cudnnRNNClipMode_t,
        clipnanopt: *mutffi::cudnnNanPropagation_t,
        lclip: *mutf64,
        rclip: *mutf64,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if clipmode.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if clipmode.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if clipnanopt.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if lclip.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if lclip.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if rclip.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if rclip.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnRNNGetClip_v8").entered();
            let result = ffi::cudnnRNNGetClip_v8(self.handle, clipmode, clipnanopt, lclip, rclip);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn rnn_get_clip_v_9(
        &mut self,
        clipmode: *mutffi::cudnnRNNClipMode_t,
        lclip: *mutf64,
        rclip: *mutf64,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if clipmode.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if clipmode.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if lclip.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if lclip.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if rclip.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if rclip.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnRNNGetClip_v9").entered();
            let result = ffi::cudnnRNNGetClip_v9(self.handle, clipmode, lclip, rclip);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

pub fn build_rnn_dynamic(
    handle: ffi::cudnnHandle_t,
    rnndesc: ffi::cudnnRNNDescriptor_t,
    minibatch: :: core :: ffi :: c_int
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnBuildRNNDynamic(handle, rnndesc, minibatch);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn get_rnn_temp_space_sizes(
    handle: ffi::cudnnHandle_t,
    rnndesc: ffi::cudnnRNNDescriptor_t,
    fwdmode: ffi::cudnnForwardMode_t,
    xdesc: ffi::cudnnRNNDataDescriptor_t,
    workspacesize: *mutusize,
    reservespacesize: *mutusize
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnGetRNNTempSpaceSizes(handle, rnndesc, fwdmode, xdesc, workspacesize, reservespacesize);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn get_rnn_weight_space_size(
    handle: ffi::cudnnHandle_t,
    rnndesc: ffi::cudnnRNNDescriptor_t,
    weightspacesize: *mutusize
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnGetRNNWeightSpaceSize(handle, rnndesc, weightspacesize);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn get_rnn_weight_params(
    handle: ffi::cudnnHandle_t,
    rnndesc: ffi::cudnnRNNDescriptor_t,
    pseudolayer: i32,
    weightspacesize: usize,
    weightspace: *const :: core :: ffi ::c_void,
    linlayerid: i32,
    mdesc: ffi::cudnnTensorDescriptor_t,
    maddr: *mut *mut :: core :: ffi ::c_void,
    bdesc: ffi::cudnnTensorDescriptor_t,
    baddr: *mut *mut :: core :: ffi ::c_void
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnGetRNNWeightParams(handle, rnndesc, pseudolayer, weightspacesize, weightspace, linlayerid, mdesc, maddr, bdesc, baddr);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn rnn_forward(
    handle: ffi::cudnnHandle_t,
    rnndesc: ffi::cudnnRNNDescriptor_t,
    fwdmode: ffi::cudnnForwardMode_t,
    devseqlengths: *consti32,
    xdesc: ffi::cudnnRNNDataDescriptor_t,
    x: *const :: core :: ffi ::c_void,
    ydesc: ffi::cudnnRNNDataDescriptor_t,
    y: *mut :: core :: ffi ::c_void,
    hdesc: ffi::cudnnTensorDescriptor_t,
    hx: *const :: core :: ffi ::c_void,
    hy: *mut :: core :: ffi ::c_void,
    cdesc: ffi::cudnnTensorDescriptor_t,
    cx: *const :: core :: ffi ::c_void,
    cy: *mut :: core :: ffi ::c_void,
    weightspacesize: usize,
    weightspace: *const :: core :: ffi ::c_void,
    workspacesize: usize,
    workspace: *mut :: core :: ffi ::c_void,
    reservespacesize: usize,
    reservespace: *mut :: core :: ffi ::c_void
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnRNNForward(handle, rnndesc, fwdmode, devseqlengths, xdesc, x, ydesc, y, hdesc, hx, hy, cdesc, cx, cy, weightspacesize, weightspace, workspacesize, workspace, reservespacesize, reservespace);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn rnn_backward_data_v_8(
    handle: ffi::cudnnHandle_t,
    rnndesc: ffi::cudnnRNNDescriptor_t,
    devseqlengths: *consti32,
    ydesc: ffi::cudnnRNNDataDescriptor_t,
    y: *const :: core :: ffi ::c_void,
    dy: *const :: core :: ffi ::c_void,
    xdesc: ffi::cudnnRNNDataDescriptor_t,
    dx: *mut :: core :: ffi ::c_void,
    hdesc: ffi::cudnnTensorDescriptor_t,
    hx: *const :: core :: ffi ::c_void,
    dhy: *const :: core :: ffi ::c_void,
    dhx: *mut :: core :: ffi ::c_void,
    cdesc: ffi::cudnnTensorDescriptor_t,
    cx: *const :: core :: ffi ::c_void,
    dcy: *const :: core :: ffi ::c_void,
    dcx: *mut :: core :: ffi ::c_void,
    weightspacesize: usize,
    weightspace: *const :: core :: ffi ::c_void,
    workspacesize: usize,
    workspace: *mut :: core :: ffi ::c_void,
    reservespacesize: usize,
    reservespace: *mut :: core :: ffi ::c_void
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnRNNBackwardData_v8(handle, rnndesc, devseqlengths, ydesc, y, dy, xdesc, dx, hdesc, hx, dhy, dhx, cdesc, cx, dcy, dcx, weightspacesize, weightspace, workspacesize, workspace, reservespacesize, reservespace);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn rnn_backward_weights_v_8(
    handle: ffi::cudnnHandle_t,
    rnndesc: ffi::cudnnRNNDescriptor_t,
    addgrad: ffi::cudnnWgradMode_t,
    devseqlengths: *consti32,
    xdesc: ffi::cudnnRNNDataDescriptor_t,
    x: *const :: core :: ffi ::c_void,
    hdesc: ffi::cudnnTensorDescriptor_t,
    hx: *const :: core :: ffi ::c_void,
    ydesc: ffi::cudnnRNNDataDescriptor_t,
    y: *const :: core :: ffi ::c_void,
    weightspacesize: usize,
    dweightspace: *mut :: core :: ffi ::c_void,
    workspacesize: usize,
    workspace: *mut :: core :: ffi ::c_void,
    reservespacesize: usize,
    reservespace: *mut :: core :: ffi ::c_void
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnRNNBackwardWeights_v8(handle, rnndesc, addgrad, devseqlengths, xdesc, x, hdesc, hx, ydesc, y, weightspacesize, dweightspace, workspacesize, workspace, reservespacesize, reservespace);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

}

// Additional methods for cudnnPoolingDescriptor_t
impl CudnnPoolingDescriptor {
    #[inline]
    pub fn set_pooling_2d_descriptor(
        &mut self,
        mode: ffi::cudnnPoolingMode_t,
        maxpoolingnanopt: ffi::cudnnNanPropagation_t,
        windowheight: :: core :: ffi :: c_int,
        windowwidth: :: core :: ffi :: c_int,
        verticalpadding: :: core :: ffi :: c_int,
        horizontalpadding: :: core :: ffi :: c_int,
        verticalstride: :: core :: ffi :: c_int,
        horizontalstride: :: core :: ffi :: c_int,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnSetPooling2dDescriptor").entered();
            let result = ffi::cudnnSetPooling2dDescriptor(self.handle, mode, maxpoolingnanopt, windowheight, windowwidth, verticalpadding, horizontalpadding, verticalstride, horizontalstride);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn get_pooling_2d_descriptor(
        &mut self,
        mode: *mutffi::cudnnPoolingMode_t,
        maxpoolingnanopt: *mutffi::cudnnNanPropagation_t,
        windowheight: *mut :: core :: ffi ::c_int,
        windowwidth: *mut :: core :: ffi ::c_int,
        verticalpadding: *mut :: core :: ffi ::c_int,
        horizontalpadding: *mut :: core :: ffi ::c_int,
        verticalstride: *mut :: core :: ffi ::c_int,
        horizontalstride: *mut :: core :: ffi ::c_int,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if mode.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if mode.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if maxpoolingnanopt.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if windowheight.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if windowheight.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if windowwidth.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if windowwidth.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if verticalpadding.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if verticalpadding.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if horizontalpadding.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if horizontalpadding.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if verticalstride.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if verticalstride.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if horizontalstride.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if horizontalstride.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnGetPooling2dDescriptor").entered();
            let result = ffi::cudnnGetPooling2dDescriptor(self.handle, mode, maxpoolingnanopt, windowheight, windowwidth, verticalpadding, horizontalpadding, verticalstride, horizontalstride);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn set_pooling_nd_descriptor(
        &mut self,
        mode: ffi::cudnnPoolingMode_t,
        maxpoolingnanopt: ffi::cudnnNanPropagation_t,
        nbdims: :: core :: ffi :: c_int,
        windowdima: *const :: core :: ffi ::c_int,
        paddinga: *const :: core :: ffi ::c_int,
        stridea: *const :: core :: ffi ::c_int,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if windowdima.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if windowdima.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if paddinga.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if paddinga.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if stridea.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if stridea.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnSetPoolingNdDescriptor").entered();
            let result = ffi::cudnnSetPoolingNdDescriptor(self.handle, mode, maxpoolingnanopt, nbdims, windowdima, paddinga, stridea);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn get_pooling_nd_descriptor(
        &mut self,
        nbdimsrequested: :: core :: ffi :: c_int,
        mode: *mutffi::cudnnPoolingMode_t,
        maxpoolingnanopt: *mutffi::cudnnNanPropagation_t,
        nbdims: *mut :: core :: ffi ::c_int,
        windowdima: *mut :: core :: ffi ::c_int,
        paddinga: *mut :: core :: ffi ::c_int,
        stridea: *mut :: core :: ffi ::c_int,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if mode.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if mode.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if maxpoolingnanopt.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if nbdims.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if nbdims.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if windowdima.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if windowdima.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if paddinga.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if paddinga.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if stridea.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if stridea.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnGetPoolingNdDescriptor").entered();
            let result = ffi::cudnnGetPoolingNdDescriptor(self.handle, nbdimsrequested, mode, maxpoolingnanopt, nbdims, windowdima, paddinga, stridea);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn get_pooling_nd_forward_output_dim(
        &mut self,
        inputtensordesc: ffi::cudnnTensorDescriptor_t,
        nbdims: :: core :: ffi :: c_int,
        outputtensordima: *mut :: core :: ffi ::c_int,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if outputtensordima.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if outputtensordima.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnGetPoolingNdForwardOutputDim").entered();
            let result = ffi::cudnnGetPoolingNdForwardOutputDim(self.handle, inputtensordesc, nbdims, outputtensordima);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn get_pooling_2d_forward_output_dim(
        &mut self,
        inputtensordesc: ffi::cudnnTensorDescriptor_t,
        n: *mut :: core :: ffi ::c_int,
        c: *mut :: core :: ffi ::c_int,
        h: *mut :: core :: ffi ::c_int,
        w: *mut :: core :: ffi ::c_int,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if n.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if n.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if c.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if c.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if h.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if h.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if w.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if w.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnGetPooling2dForwardOutputDim").entered();
            let result = ffi::cudnnGetPooling2dForwardOutputDim(self.handle, inputtensordesc, n, c, h, w);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

pub fn pooling_forward(
    handle: ffi::cudnnHandle_t,
    poolingdesc: ffi::cudnnPoolingDescriptor_t,
    alpha: *const :: core :: ffi ::c_void,
    xdesc: ffi::cudnnTensorDescriptor_t,
    x: *const :: core :: ffi ::c_void,
    beta: *const :: core :: ffi ::c_void,
    ydesc: ffi::cudnnTensorDescriptor_t,
    y: *mut :: core :: ffi ::c_void
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnPoolingForward(handle, poolingdesc, alpha, xdesc, x, beta, ydesc, y);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn pooling_backward(
    handle: ffi::cudnnHandle_t,
    poolingdesc: ffi::cudnnPoolingDescriptor_t,
    alpha: *const :: core :: ffi ::c_void,
    ydesc: ffi::cudnnTensorDescriptor_t,
    y: *const :: core :: ffi ::c_void,
    dydesc: ffi::cudnnTensorDescriptor_t,
    dy: *const :: core :: ffi ::c_void,
    xdesc: ffi::cudnnTensorDescriptor_t,
    x: *const :: core :: ffi ::c_void,
    beta: *const :: core :: ffi ::c_void,
    dxdesc: ffi::cudnnTensorDescriptor_t,
    dx: *mut :: core :: ffi ::c_void
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnPoolingBackward(handle, poolingdesc, alpha, ydesc, y, dydesc, dy, xdesc, x, beta, dxdesc, dx);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

}

// Additional methods for cudaExternalSemaphore_t
impl CudaExternalSemaphore {
    #[inline]
    pub fn signal_external_semaphores_async(
        &mut self,
        paramsarray: *constffi::cudaExternalSemaphoreSignalParams,
        numextsems: :: core :: ffi :: c_uint,
        stream: ffi::cudaStream_t,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if paramsarray.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if paramsarray.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudaSignalExternalSemaphoresAsync").entered();
            let result = ffi::cudaSignalExternalSemaphoresAsync(&self.handle, paramsarray, numextsems, stream);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn wait_external_semaphores_async(
        &mut self,
        paramsarray: *constffi::cudaExternalSemaphoreWaitParams,
        numextsems: :: core :: ffi :: c_uint,
        stream: ffi::cudaStream_t,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if paramsarray.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if paramsarray.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudaWaitExternalSemaphoresAsync").entered();
            let result = ffi::cudaWaitExternalSemaphoresAsync(&self.handle, paramsarray, numextsems, stream);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn destroy_external_semaphore(
        &mut self,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudaDestroyExternalSemaphore").entered();
            let result = ffi::cudaDestroyExternalSemaphore(self.handle);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

}

// Additional methods for cudnnOpTensorDescriptor_t
impl CudnnOpTensorDescriptor {
    #[inline]
    pub fn set_op_tensor_descriptor(
        &mut self,
        optensorop: ffi::cudnnOpTensorOp_t,
        optensorcomptype: ffi::cudnnDataType_t,
        optensornanopt: ffi::cudnnNanPropagation_t,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnSetOpTensorDescriptor").entered();
            let result = ffi::cudnnSetOpTensorDescriptor(self.handle, optensorop, optensorcomptype, optensornanopt);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn get_op_tensor_descriptor(
        &mut self,
        optensorop: *mutffi::cudnnOpTensorOp_t,
        optensorcomptype: *mutffi::cudnnDataType_t,
        optensornanopt: *mutffi::cudnnNanPropagation_t,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if optensorop.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if optensorcomptype.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if optensornanopt.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnGetOpTensorDescriptor").entered();
            let result = ffi::cudnnGetOpTensorDescriptor(self.handle, optensorop, optensorcomptype, optensornanopt);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

pub fn op_tensor(
    handle: ffi::cudnnHandle_t,
    optensordesc: ffi::cudnnOpTensorDescriptor_t,
    alpha1: *const :: core :: ffi ::c_void,
    adesc: ffi::cudnnTensorDescriptor_t,
    a: *const :: core :: ffi ::c_void,
    alpha2: *const :: core :: ffi ::c_void,
    bdesc: ffi::cudnnTensorDescriptor_t,
    b: *const :: core :: ffi ::c_void,
    beta: *const :: core :: ffi ::c_void,
    cdesc: ffi::cudnnTensorDescriptor_t,
    c: *mut :: core :: ffi ::c_void
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnOpTensor(handle, optensordesc, alpha1, adesc, a, alpha2, bdesc, b, beta, cdesc, c);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

}

// Additional methods for cudnnActivationDescriptor_t
impl CudnnActivationDescriptor {
    #[inline]
    pub fn set_activation_descriptor(
        &mut self,
        mode: ffi::cudnnActivationMode_t,
        relunanopt: ffi::cudnnNanPropagation_t,
        coef: f64,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnSetActivationDescriptor").entered();
            let result = ffi::cudnnSetActivationDescriptor(self.handle, mode, relunanopt, coef);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn get_activation_descriptor(
        &mut self,
        mode: *mutffi::cudnnActivationMode_t,
        relunanopt: *mutffi::cudnnNanPropagation_t,
        coef: *mutf64,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if mode.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if mode.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if relunanopt.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if coef.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if coef.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnGetActivationDescriptor").entered();
            let result = ffi::cudnnGetActivationDescriptor(self.handle, mode, relunanopt, coef);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn set_activation_descriptor_swish_beta(
        &mut self,
        swish_beta: f64,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnSetActivationDescriptorSwishBeta").entered();
            let result = ffi::cudnnSetActivationDescriptorSwishBeta(self.handle, swish_beta);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn get_activation_descriptor_swish_beta(
        &mut self,
        swish_beta: *mutf64,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if swish_beta.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if swish_beta.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnGetActivationDescriptorSwishBeta").entered();
            let result = ffi::cudnnGetActivationDescriptorSwishBeta(self.handle, swish_beta);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

pub fn activation_forward(
    handle: ffi::cudnnHandle_t,
    activationdesc: ffi::cudnnActivationDescriptor_t,
    alpha: *const :: core :: ffi ::c_void,
    xdesc: ffi::cudnnTensorDescriptor_t,
    x: *const :: core :: ffi ::c_void,
    beta: *const :: core :: ffi ::c_void,
    ydesc: ffi::cudnnTensorDescriptor_t,
    y: *mut :: core :: ffi ::c_void
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnActivationForward(handle, activationdesc, alpha, xdesc, x, beta, ydesc, y);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn normalization_forward_inference(
    handle: ffi::cudnnHandle_t,
    mode: ffi::cudnnNormMode_t,
    normops: ffi::cudnnNormOps_t,
    algo: ffi::cudnnNormAlgo_t,
    alpha: *const :: core :: ffi ::c_void,
    beta: *const :: core :: ffi ::c_void,
    xdesc: ffi::cudnnTensorDescriptor_t,
    x: *const :: core :: ffi ::c_void,
    normscalebiasdesc: ffi::cudnnTensorDescriptor_t,
    normscale: *const :: core :: ffi ::c_void,
    normbias: *const :: core :: ffi ::c_void,
    normmeanvardesc: ffi::cudnnTensorDescriptor_t,
    estimatedmean: *const :: core :: ffi ::c_void,
    estimatedvariance: *const :: core :: ffi ::c_void,
    zdesc: ffi::cudnnTensorDescriptor_t,
    z: *const :: core :: ffi ::c_void,
    activationdesc: ffi::cudnnActivationDescriptor_t,
    ydesc: ffi::cudnnTensorDescriptor_t,
    y: *mut :: core :: ffi ::c_void,
    epsilon: f64,
    groupcnt: :: core :: ffi :: c_int
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnNormalizationForwardInference(handle, mode, normops, algo, alpha, beta, xdesc, x, normscalebiasdesc, normscale, normbias, normmeanvardesc, estimatedmean, estimatedvariance, zdesc, z, activationdesc, ydesc, y, epsilon, groupcnt);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn activation_backward(
    handle: ffi::cudnnHandle_t,
    activationdesc: ffi::cudnnActivationDescriptor_t,
    alpha: *const :: core :: ffi ::c_void,
    ydesc: ffi::cudnnTensorDescriptor_t,
    y: *const :: core :: ffi ::c_void,
    dydesc: ffi::cudnnTensorDescriptor_t,
    dy: *const :: core :: ffi ::c_void,
    xdesc: ffi::cudnnTensorDescriptor_t,
    x: *const :: core :: ffi ::c_void,
    beta: *const :: core :: ffi ::c_void,
    dxdesc: ffi::cudnnTensorDescriptor_t,
    dx: *mut :: core :: ffi ::c_void
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnActivationBackward(handle, activationdesc, alpha, ydesc, y, dydesc, dy, xdesc, x, beta, dxdesc, dx);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn get_batch_normalization_forward_training_ex_workspace_size(
    handle: ffi::cudnnHandle_t,
    mode: ffi::cudnnBatchNormMode_t,
    bnops: ffi::cudnnBatchNormOps_t,
    xdesc: ffi::cudnnTensorDescriptor_t,
    zdesc: ffi::cudnnTensorDescriptor_t,
    ydesc: ffi::cudnnTensorDescriptor_t,
    bnscalebiasmeanvardesc: ffi::cudnnTensorDescriptor_t,
    activationdesc: ffi::cudnnActivationDescriptor_t,
    sizeinbytes: *mutusize
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnGetBatchNormalizationForwardTrainingExWorkspaceSize(handle, mode, bnops, xdesc, zdesc, ydesc, bnscalebiasmeanvardesc, activationdesc, sizeinbytes);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn get_batch_normalization_backward_ex_workspace_size(
    handle: ffi::cudnnHandle_t,
    mode: ffi::cudnnBatchNormMode_t,
    bnops: ffi::cudnnBatchNormOps_t,
    xdesc: ffi::cudnnTensorDescriptor_t,
    ydesc: ffi::cudnnTensorDescriptor_t,
    dydesc: ffi::cudnnTensorDescriptor_t,
    dzdesc: ffi::cudnnTensorDescriptor_t,
    dxdesc: ffi::cudnnTensorDescriptor_t,
    dbnscalebiasdesc: ffi::cudnnTensorDescriptor_t,
    activationdesc: ffi::cudnnActivationDescriptor_t,
    sizeinbytes: *mutusize
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnGetBatchNormalizationBackwardExWorkspaceSize(handle, mode, bnops, xdesc, ydesc, dydesc, dzdesc, dxdesc, dbnscalebiasdesc, activationdesc, sizeinbytes);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn get_batch_normalization_training_ex_reserve_space_size(
    handle: ffi::cudnnHandle_t,
    mode: ffi::cudnnBatchNormMode_t,
    bnops: ffi::cudnnBatchNormOps_t,
    activationdesc: ffi::cudnnActivationDescriptor_t,
    xdesc: ffi::cudnnTensorDescriptor_t,
    sizeinbytes: *mutusize
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnGetBatchNormalizationTrainingExReserveSpaceSize(handle, mode, bnops, activationdesc, xdesc, sizeinbytes);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn batch_normalization_forward_training_ex(
    handle: ffi::cudnnHandle_t,
    mode: ffi::cudnnBatchNormMode_t,
    bnops: ffi::cudnnBatchNormOps_t,
    alpha: *const :: core :: ffi ::c_void,
    beta: *const :: core :: ffi ::c_void,
    xdesc: ffi::cudnnTensorDescriptor_t,
    xdata: *const :: core :: ffi ::c_void,
    zdesc: ffi::cudnnTensorDescriptor_t,
    zdata: *const :: core :: ffi ::c_void,
    ydesc: ffi::cudnnTensorDescriptor_t,
    ydata: *mut :: core :: ffi ::c_void,
    bnscalebiasmeanvardesc: ffi::cudnnTensorDescriptor_t,
    bnscale: *const :: core :: ffi ::c_void,
    bnbias: *const :: core :: ffi ::c_void,
    exponentialaveragefactor: f64,
    resultrunningmean: *mut :: core :: ffi ::c_void,
    resultrunningvariance: *mut :: core :: ffi ::c_void,
    epsilon: f64,
    resultsavemean: *mut :: core :: ffi ::c_void,
    resultsaveinvvariance: *mut :: core :: ffi ::c_void,
    activationdesc: ffi::cudnnActivationDescriptor_t,
    workspace: *mut :: core :: ffi ::c_void,
    workspacesizeinbytes: usize,
    reservespace: *mut :: core :: ffi ::c_void,
    reservespacesizeinbytes: usize
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnBatchNormalizationForwardTrainingEx(handle, mode, bnops, alpha, beta, xdesc, xdata, zdesc, zdata, ydesc, ydata, bnscalebiasmeanvardesc, bnscale, bnbias, exponentialaveragefactor, resultrunningmean, resultrunningvariance, epsilon, resultsavemean, resultsaveinvvariance, activationdesc, workspace, workspacesizeinbytes, reservespace, reservespacesizeinbytes);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn batch_normalization_backward_ex(
    handle: ffi::cudnnHandle_t,
    mode: ffi::cudnnBatchNormMode_t,
    bnops: ffi::cudnnBatchNormOps_t,
    alphadatadiff: *const :: core :: ffi ::c_void,
    betadatadiff: *const :: core :: ffi ::c_void,
    alphaparamdiff: *const :: core :: ffi ::c_void,
    betaparamdiff: *const :: core :: ffi ::c_void,
    xdesc: ffi::cudnnTensorDescriptor_t,
    xdata: *const :: core :: ffi ::c_void,
    ydesc: ffi::cudnnTensorDescriptor_t,
    ydata: *const :: core :: ffi ::c_void,
    dydesc: ffi::cudnnTensorDescriptor_t,
    dydata: *const :: core :: ffi ::c_void,
    dzdesc: ffi::cudnnTensorDescriptor_t,
    dzdata: *mut :: core :: ffi ::c_void,
    dxdesc: ffi::cudnnTensorDescriptor_t,
    dxdata: *mut :: core :: ffi ::c_void,
    dbnscalebiasdesc: ffi::cudnnTensorDescriptor_t,
    bnscaledata: *const :: core :: ffi ::c_void,
    bnbiasdata: *const :: core :: ffi ::c_void,
    dbnscaledata: *mut :: core :: ffi ::c_void,
    dbnbiasdata: *mut :: core :: ffi ::c_void,
    epsilon: f64,
    savedmean: *const :: core :: ffi ::c_void,
    savedinvvariance: *const :: core :: ffi ::c_void,
    activationdesc: ffi::cudnnActivationDescriptor_t,
    workspace: *mut :: core :: ffi ::c_void,
    workspacesizeinbytes: usize,
    reservespace: *mut :: core :: ffi ::c_void,
    reservespacesizeinbytes: usize
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnBatchNormalizationBackwardEx(handle, mode, bnops, alphadatadiff, betadatadiff, alphaparamdiff, betaparamdiff, xdesc, xdata, ydesc, ydata, dydesc, dydata, dzdesc, dzdata, dxdesc, dxdata, dbnscalebiasdesc, bnscaledata, bnbiasdata, dbnscaledata, dbnbiasdata, epsilon, savedmean, savedinvvariance, activationdesc, workspace, workspacesizeinbytes, reservespace, reservespacesizeinbytes);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn get_normalization_forward_training_workspace_size(
    handle: ffi::cudnnHandle_t,
    mode: ffi::cudnnNormMode_t,
    normops: ffi::cudnnNormOps_t,
    algo: ffi::cudnnNormAlgo_t,
    xdesc: ffi::cudnnTensorDescriptor_t,
    zdesc: ffi::cudnnTensorDescriptor_t,
    ydesc: ffi::cudnnTensorDescriptor_t,
    normscalebiasdesc: ffi::cudnnTensorDescriptor_t,
    activationdesc: ffi::cudnnActivationDescriptor_t,
    normmeanvardesc: ffi::cudnnTensorDescriptor_t,
    sizeinbytes: *mutusize,
    groupcnt: :: core :: ffi :: c_int
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnGetNormalizationForwardTrainingWorkspaceSize(handle, mode, normops, algo, xdesc, zdesc, ydesc, normscalebiasdesc, activationdesc, normmeanvardesc, sizeinbytes, groupcnt);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn get_normalization_backward_workspace_size(
    handle: ffi::cudnnHandle_t,
    mode: ffi::cudnnNormMode_t,
    normops: ffi::cudnnNormOps_t,
    algo: ffi::cudnnNormAlgo_t,
    xdesc: ffi::cudnnTensorDescriptor_t,
    ydesc: ffi::cudnnTensorDescriptor_t,
    dydesc: ffi::cudnnTensorDescriptor_t,
    dzdesc: ffi::cudnnTensorDescriptor_t,
    dxdesc: ffi::cudnnTensorDescriptor_t,
    dnormscalebiasdesc: ffi::cudnnTensorDescriptor_t,
    activationdesc: ffi::cudnnActivationDescriptor_t,
    normmeanvardesc: ffi::cudnnTensorDescriptor_t,
    sizeinbytes: *mutusize,
    groupcnt: :: core :: ffi :: c_int
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnGetNormalizationBackwardWorkspaceSize(handle, mode, normops, algo, xdesc, ydesc, dydesc, dzdesc, dxdesc, dnormscalebiasdesc, activationdesc, normmeanvardesc, sizeinbytes, groupcnt);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn get_normalization_training_reserve_space_size(
    handle: ffi::cudnnHandle_t,
    mode: ffi::cudnnNormMode_t,
    normops: ffi::cudnnNormOps_t,
    algo: ffi::cudnnNormAlgo_t,
    activationdesc: ffi::cudnnActivationDescriptor_t,
    xdesc: ffi::cudnnTensorDescriptor_t,
    sizeinbytes: *mutusize,
    groupcnt: :: core :: ffi :: c_int
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnGetNormalizationTrainingReserveSpaceSize(handle, mode, normops, algo, activationdesc, xdesc, sizeinbytes, groupcnt);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn normalization_forward_training(
    handle: ffi::cudnnHandle_t,
    mode: ffi::cudnnNormMode_t,
    normops: ffi::cudnnNormOps_t,
    algo: ffi::cudnnNormAlgo_t,
    alpha: *const :: core :: ffi ::c_void,
    beta: *const :: core :: ffi ::c_void,
    xdesc: ffi::cudnnTensorDescriptor_t,
    xdata: *const :: core :: ffi ::c_void,
    normscalebiasdesc: ffi::cudnnTensorDescriptor_t,
    normscale: *const :: core :: ffi ::c_void,
    normbias: *const :: core :: ffi ::c_void,
    exponentialaveragefactor: f64,
    normmeanvardesc: ffi::cudnnTensorDescriptor_t,
    resultrunningmean: *mut :: core :: ffi ::c_void,
    resultrunningvariance: *mut :: core :: ffi ::c_void,
    epsilon: f64,
    resultsavemean: *mut :: core :: ffi ::c_void,
    resultsaveinvvariance: *mut :: core :: ffi ::c_void,
    activationdesc: ffi::cudnnActivationDescriptor_t,
    zdesc: ffi::cudnnTensorDescriptor_t,
    zdata: *const :: core :: ffi ::c_void,
    ydesc: ffi::cudnnTensorDescriptor_t,
    ydata: *mut :: core :: ffi ::c_void,
    workspace: *mut :: core :: ffi ::c_void,
    workspacesizeinbytes: usize,
    reservespace: *mut :: core :: ffi ::c_void,
    reservespacesizeinbytes: usize,
    groupcnt: :: core :: ffi :: c_int
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnNormalizationForwardTraining(handle, mode, normops, algo, alpha, beta, xdesc, xdata, normscalebiasdesc, normscale, normbias, exponentialaveragefactor, normmeanvardesc, resultrunningmean, resultrunningvariance, epsilon, resultsavemean, resultsaveinvvariance, activationdesc, zdesc, zdata, ydesc, ydata, workspace, workspacesizeinbytes, reservespace, reservespacesizeinbytes, groupcnt);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn normalization_backward(
    handle: ffi::cudnnHandle_t,
    mode: ffi::cudnnNormMode_t,
    normops: ffi::cudnnNormOps_t,
    algo: ffi::cudnnNormAlgo_t,
    alphadatadiff: *const :: core :: ffi ::c_void,
    betadatadiff: *const :: core :: ffi ::c_void,
    alphaparamdiff: *const :: core :: ffi ::c_void,
    betaparamdiff: *const :: core :: ffi ::c_void,
    xdesc: ffi::cudnnTensorDescriptor_t,
    xdata: *const :: core :: ffi ::c_void,
    ydesc: ffi::cudnnTensorDescriptor_t,
    ydata: *const :: core :: ffi ::c_void,
    dydesc: ffi::cudnnTensorDescriptor_t,
    dydata: *const :: core :: ffi ::c_void,
    dzdesc: ffi::cudnnTensorDescriptor_t,
    dzdata: *mut :: core :: ffi ::c_void,
    dxdesc: ffi::cudnnTensorDescriptor_t,
    dxdata: *mut :: core :: ffi ::c_void,
    dnormscalebiasdesc: ffi::cudnnTensorDescriptor_t,
    normscaledata: *const :: core :: ffi ::c_void,
    normbiasdata: *const :: core :: ffi ::c_void,
    dnormscaledata: *mut :: core :: ffi ::c_void,
    dnormbiasdata: *mut :: core :: ffi ::c_void,
    epsilon: f64,
    normmeanvardesc: ffi::cudnnTensorDescriptor_t,
    savedmean: *const :: core :: ffi ::c_void,
    savedinvvariance: *const :: core :: ffi ::c_void,
    activationdesc: ffi::cudnnActivationDescriptor_t,
    workspace: *mut :: core :: ffi ::c_void,
    workspacesizeinbytes: usize,
    reservespace: *mut :: core :: ffi ::c_void,
    reservespacesizeinbytes: usize,
    groupcnt: :: core :: ffi :: c_int
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnNormalizationBackward(handle, mode, normops, algo, alphadatadiff, betadatadiff, alphaparamdiff, betaparamdiff, xdesc, xdata, ydesc, ydata, dydesc, dydata, dzdesc, dzdata, dxdesc, dxdata, dnormscalebiasdesc, normscaledata, normbiasdata, dnormscaledata, dnormbiasdata, epsilon, normmeanvardesc, savedmean, savedinvvariance, activationdesc, workspace, workspacesizeinbytes, reservespace, reservespacesizeinbytes, groupcnt);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn convolution_bias_activation_forward(
    handle: ffi::cudnnHandle_t,
    alpha1: *const :: core :: ffi ::c_void,
    xdesc: ffi::cudnnTensorDescriptor_t,
    x: *const :: core :: ffi ::c_void,
    wdesc: ffi::cudnnFilterDescriptor_t,
    w: *const :: core :: ffi ::c_void,
    convdesc: ffi::cudnnConvolutionDescriptor_t,
    algo: ffi::cudnnConvolutionFwdAlgo_t,
    workspace: *mut :: core :: ffi ::c_void,
    workspacesizeinbytes: usize,
    alpha2: *const :: core :: ffi ::c_void,
    zdesc: ffi::cudnnTensorDescriptor_t,
    z: *const :: core :: ffi ::c_void,
    biasdesc: ffi::cudnnTensorDescriptor_t,
    bias: *const :: core :: ffi ::c_void,
    activationdesc: ffi::cudnnActivationDescriptor_t,
    ydesc: ffi::cudnnTensorDescriptor_t,
    y: *mut :: core :: ffi ::c_void
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnConvolutionBiasActivationForward(handle, alpha1, xdesc, x, wdesc, w, convdesc, algo, workspace, workspacesizeinbytes, alpha2, zdesc, z, biasdesc, bias, activationdesc, ydesc, y);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

}

// Additional methods for cudnnSpatialTransformerDescriptor_t
impl CudnnSpatialTransformerDescriptor {
    #[inline]
    pub fn set_spatial_transformer_nd_descriptor(
        &mut self,
        samplertype: ffi::cudnnSamplerType_t,
        datatype: ffi::cudnnDataType_t,
        nbdims: :: core :: ffi :: c_int,
        dima: *const :: core :: ffi ::c_int,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if dima.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if dima.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnSetSpatialTransformerNdDescriptor").entered();
            let result = ffi::cudnnSetSpatialTransformerNdDescriptor(self.handle, samplertype, datatype, nbdims, dima);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

pub fn spatial_tf_grid_generator_forward(
    handle: ffi::cudnnHandle_t,
    stdesc: ffi::cudnnSpatialTransformerDescriptor_t,
    theta: *const :: core :: ffi ::c_void,
    grid: *mut :: core :: ffi ::c_void
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnSpatialTfGridGeneratorForward(handle, stdesc, theta, grid);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn spatial_tf_sampler_forward(
    handle: ffi::cudnnHandle_t,
    stdesc: ffi::cudnnSpatialTransformerDescriptor_t,
    alpha: *const :: core :: ffi ::c_void,
    xdesc: ffi::cudnnTensorDescriptor_t,
    x: *const :: core :: ffi ::c_void,
    grid: *const :: core :: ffi ::c_void,
    beta: *const :: core :: ffi ::c_void,
    ydesc: ffi::cudnnTensorDescriptor_t,
    y: *mut :: core :: ffi ::c_void
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnSpatialTfSamplerForward(handle, stdesc, alpha, xdesc, x, grid, beta, ydesc, y);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn spatial_tf_grid_generator_backward(
    handle: ffi::cudnnHandle_t,
    stdesc: ffi::cudnnSpatialTransformerDescriptor_t,
    dgrid: *const :: core :: ffi ::c_void,
    dtheta: *mut :: core :: ffi ::c_void
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnSpatialTfGridGeneratorBackward(handle, stdesc, dgrid, dtheta);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn spatial_tf_sampler_backward(
    handle: ffi::cudnnHandle_t,
    stdesc: ffi::cudnnSpatialTransformerDescriptor_t,
    alpha: *const :: core :: ffi ::c_void,
    xdesc: ffi::cudnnTensorDescriptor_t,
    x: *const :: core :: ffi ::c_void,
    beta: *const :: core :: ffi ::c_void,
    dxdesc: ffi::cudnnTensorDescriptor_t,
    dx: *mut :: core :: ffi ::c_void,
    alphadgrid: *const :: core :: ffi ::c_void,
    dydesc: ffi::cudnnTensorDescriptor_t,
    dy: *const :: core :: ffi ::c_void,
    grid: *const :: core :: ffi ::c_void,
    betadgrid: *const :: core :: ffi ::c_void,
    dgrid: *mut :: core :: ffi ::c_void
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnSpatialTfSamplerBackward(handle, stdesc, alpha, xdesc, x, beta, dxdesc, dx, alphadgrid, dydesc, dy, grid, betadgrid, dgrid);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

}

// Additional methods for cudnnRNNDataDescriptor_t
impl CudnnRNNDataDescriptor {
pub fn get_rnn_temp_space_sizes(
    handle: ffi::cudnnHandle_t,
    rnndesc: ffi::cudnnRNNDescriptor_t,
    fwdmode: ffi::cudnnForwardMode_t,
    xdesc: ffi::cudnnRNNDataDescriptor_t,
    workspacesize: *mutusize,
    reservespacesize: *mutusize
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnGetRNNTempSpaceSizes(handle, rnndesc, fwdmode, xdesc, workspacesize, reservespacesize);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

    #[inline]
    pub fn set_rnn_data_descriptor(
        &mut self,
        datatype: ffi::cudnnDataType_t,
        layout: ffi::cudnnRNNDataLayout_t,
        maxseqlength: :: core :: ffi :: c_int,
        batchsize: :: core :: ffi :: c_int,
        vectorsize: :: core :: ffi :: c_int,
        seqlengtharray: *const :: core :: ffi ::c_int,
        paddingfill: *mut :: core :: ffi ::c_void,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if seqlengtharray.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if seqlengtharray.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if paddingfill.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if paddingfill.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnSetRNNDataDescriptor").entered();
            let result = ffi::cudnnSetRNNDataDescriptor(self.handle, datatype, layout, maxseqlength, batchsize, vectorsize, seqlengtharray, paddingfill);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn get_rnn_data_descriptor(
        &mut self,
        datatype: *mutffi::cudnnDataType_t,
        layout: *mutffi::cudnnRNNDataLayout_t,
        maxseqlength: *mut :: core :: ffi ::c_int,
        batchsize: *mut :: core :: ffi ::c_int,
        vectorsize: *mut :: core :: ffi ::c_int,
        arraylengthrequested: :: core :: ffi :: c_int,
        seqlengtharray: *mut :: core :: ffi ::c_int,
        paddingfill: *mut :: core :: ffi ::c_void,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if datatype.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if datatype.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if layout.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if layout.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if maxseqlength.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if maxseqlength.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if batchsize.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if batchsize.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if vectorsize.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if vectorsize.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if seqlengtharray.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if seqlengtharray.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if paddingfill.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if paddingfill.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnGetRNNDataDescriptor").entered();
            let result = ffi::cudnnGetRNNDataDescriptor(self.handle, datatype, layout, maxseqlength, batchsize, vectorsize, arraylengthrequested, seqlengtharray, paddingfill);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

pub fn rnn_forward(
    handle: ffi::cudnnHandle_t,
    rnndesc: ffi::cudnnRNNDescriptor_t,
    fwdmode: ffi::cudnnForwardMode_t,
    devseqlengths: *consti32,
    xdesc: ffi::cudnnRNNDataDescriptor_t,
    x: *const :: core :: ffi ::c_void,
    ydesc: ffi::cudnnRNNDataDescriptor_t,
    y: *mut :: core :: ffi ::c_void,
    hdesc: ffi::cudnnTensorDescriptor_t,
    hx: *const :: core :: ffi ::c_void,
    hy: *mut :: core :: ffi ::c_void,
    cdesc: ffi::cudnnTensorDescriptor_t,
    cx: *const :: core :: ffi ::c_void,
    cy: *mut :: core :: ffi ::c_void,
    weightspacesize: usize,
    weightspace: *const :: core :: ffi ::c_void,
    workspacesize: usize,
    workspace: *mut :: core :: ffi ::c_void,
    reservespacesize: usize,
    reservespace: *mut :: core :: ffi ::c_void
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnRNNForward(handle, rnndesc, fwdmode, devseqlengths, xdesc, x, ydesc, y, hdesc, hx, hy, cdesc, cx, cy, weightspacesize, weightspace, workspacesize, workspace, reservespacesize, reservespace);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn rnn_backward_data_v_8(
    handle: ffi::cudnnHandle_t,
    rnndesc: ffi::cudnnRNNDescriptor_t,
    devseqlengths: *consti32,
    ydesc: ffi::cudnnRNNDataDescriptor_t,
    y: *const :: core :: ffi ::c_void,
    dy: *const :: core :: ffi ::c_void,
    xdesc: ffi::cudnnRNNDataDescriptor_t,
    dx: *mut :: core :: ffi ::c_void,
    hdesc: ffi::cudnnTensorDescriptor_t,
    hx: *const :: core :: ffi ::c_void,
    dhy: *const :: core :: ffi ::c_void,
    dhx: *mut :: core :: ffi ::c_void,
    cdesc: ffi::cudnnTensorDescriptor_t,
    cx: *const :: core :: ffi ::c_void,
    dcy: *const :: core :: ffi ::c_void,
    dcx: *mut :: core :: ffi ::c_void,
    weightspacesize: usize,
    weightspace: *const :: core :: ffi ::c_void,
    workspacesize: usize,
    workspace: *mut :: core :: ffi ::c_void,
    reservespacesize: usize,
    reservespace: *mut :: core :: ffi ::c_void
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnRNNBackwardData_v8(handle, rnndesc, devseqlengths, ydesc, y, dy, xdesc, dx, hdesc, hx, dhy, dhx, cdesc, cx, dcy, dcx, weightspacesize, weightspace, workspacesize, workspace, reservespacesize, reservespace);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn rnn_backward_weights_v_8(
    handle: ffi::cudnnHandle_t,
    rnndesc: ffi::cudnnRNNDescriptor_t,
    addgrad: ffi::cudnnWgradMode_t,
    devseqlengths: *consti32,
    xdesc: ffi::cudnnRNNDataDescriptor_t,
    x: *const :: core :: ffi ::c_void,
    hdesc: ffi::cudnnTensorDescriptor_t,
    hx: *const :: core :: ffi ::c_void,
    ydesc: ffi::cudnnRNNDataDescriptor_t,
    y: *const :: core :: ffi ::c_void,
    weightspacesize: usize,
    dweightspace: *mut :: core :: ffi ::c_void,
    workspacesize: usize,
    workspace: *mut :: core :: ffi ::c_void,
    reservespacesize: usize,
    reservespace: *mut :: core :: ffi ::c_void
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnRNNBackwardWeights_v8(handle, rnndesc, addgrad, devseqlengths, xdesc, x, hdesc, hx, ydesc, y, weightspacesize, dweightspace, workspacesize, workspace, reservespacesize, reservespace);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

}

// Additional methods for cudaLogsCallback_t
impl CudaLogsCallback {
    #[inline]
    pub fn logs_register_callback(
        &mut self,
        userdata: *mut :: core :: ffi ::c_void,
        callback_out: *mutffi::cudaLogsCallbackHandle,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if userdata.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if userdata.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if callback_out.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if callback_out.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudaLogsRegisterCallback").entered();
            let result = ffi::cudaLogsRegisterCallback(self.handle, userdata, callback_out);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

}

// Additional methods for cudnnFusedOpsConstParamPack_t
impl CudnnFusedOpsConstParamPack {
    #[inline]
    pub fn set_fused_ops_const_param_pack_attribute(
        &mut self,
        paramlabel: ffi::cudnnFusedOpsConstParamLabel_t,
        param: *const :: core :: ffi ::c_void,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if param.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if param.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnSetFusedOpsConstParamPackAttribute").entered();
            let result = ffi::cudnnSetFusedOpsConstParamPackAttribute(self.handle, paramlabel, param);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn get_fused_ops_const_param_pack_attribute(
        &mut self,
        paramlabel: ffi::cudnnFusedOpsConstParamLabel_t,
        param: *mut :: core :: ffi ::c_void,
        isnull: *mut :: core :: ffi ::c_int,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if param.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if param.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if isnull.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if isnull.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnGetFusedOpsConstParamPackAttribute").entered();
            let result = ffi::cudnnGetFusedOpsConstParamPackAttribute(self.handle, paramlabel, param, isnull);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

pub fn make_fused_ops_plan(
    handle: ffi::cudnnHandle_t,
    plan: ffi::cudnnFusedOpsPlan_t,
    constpack: ffi::cudnnFusedOpsConstParamPack_t,
    workspacesizeinbytes: *mutusize
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnMakeFusedOpsPlan(handle, plan, constpack, workspacesizeinbytes);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

}

// Additional methods for cudnnReduceTensorDescriptor_t
impl CudnnReduceTensorDescriptor {
    #[inline]
    pub fn set_reduce_tensor_descriptor(
        &mut self,
        reducetensorop: ffi::cudnnReduceTensorOp_t,
        reducetensorcomptype: ffi::cudnnDataType_t,
        reducetensornanopt: ffi::cudnnNanPropagation_t,
        reducetensorindices: ffi::cudnnReduceTensorIndices_t,
        reducetensorindicestype: ffi::cudnnIndicesType_t,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnSetReduceTensorDescriptor").entered();
            let result = ffi::cudnnSetReduceTensorDescriptor(self.handle, reducetensorop, reducetensorcomptype, reducetensornanopt, reducetensorindices, reducetensorindicestype);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn get_reduce_tensor_descriptor(
        &mut self,
        reducetensorop: *mutffi::cudnnReduceTensorOp_t,
        reducetensorcomptype: *mutffi::cudnnDataType_t,
        reducetensornanopt: *mutffi::cudnnNanPropagation_t,
        reducetensorindices: *mutffi::cudnnReduceTensorIndices_t,
        reducetensorindicestype: *mutffi::cudnnIndicesType_t,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if reducetensorop.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if reducetensorop.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if reducetensorcomptype.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if reducetensorcomptype.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if reducetensornanopt.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if reducetensorindices.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if reducetensorindices.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if reducetensorindicestype.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if reducetensorindicestype.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnGetReduceTensorDescriptor").entered();
            let result = ffi::cudnnGetReduceTensorDescriptor(self.handle, reducetensorop, reducetensorcomptype, reducetensornanopt, reducetensorindices, reducetensorindicestype);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

pub fn get_reduction_indices_size(
    handle: ffi::cudnnHandle_t,
    reducetensordesc: ffi::cudnnReduceTensorDescriptor_t,
    adesc: ffi::cudnnTensorDescriptor_t,
    cdesc: ffi::cudnnTensorDescriptor_t,
    sizeinbytes: *mutusize
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnGetReductionIndicesSize(handle, reducetensordesc, adesc, cdesc, sizeinbytes);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn get_reduction_workspace_size(
    handle: ffi::cudnnHandle_t,
    reducetensordesc: ffi::cudnnReduceTensorDescriptor_t,
    adesc: ffi::cudnnTensorDescriptor_t,
    cdesc: ffi::cudnnTensorDescriptor_t,
    sizeinbytes: *mutusize
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnGetReductionWorkspaceSize(handle, reducetensordesc, adesc, cdesc, sizeinbytes);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn reduce_tensor(
    handle: ffi::cudnnHandle_t,
    reducetensordesc: ffi::cudnnReduceTensorDescriptor_t,
    indices: *mut :: core :: ffi ::c_void,
    indicessizeinbytes: usize,
    workspace: *mut :: core :: ffi ::c_void,
    workspacesizeinbytes: usize,
    alpha: *const :: core :: ffi ::c_void,
    adesc: ffi::cudnnTensorDescriptor_t,
    a: *const :: core :: ffi ::c_void,
    beta: *const :: core :: ffi ::c_void,
    cdesc: ffi::cudnnTensorDescriptor_t,
    c: *mut :: core :: ffi ::c_void
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnReduceTensor(handle, reducetensordesc, indices, indicessizeinbytes, workspace, workspacesizeinbytes, alpha, adesc, a, beta, cdesc, c);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

}

// Additional methods for cudnnDropoutDescriptor_t
impl CudnnDropoutDescriptor {
    #[inline]
    pub fn set_dropout_descriptor(
        &mut self,
        handle: ffi::cudnnHandle_t,
        dropout: f32,
        states: *mut :: core :: ffi ::c_void,
        statesizeinbytes: usize,
        seed: :: core :: ffi :: c_ulonglong,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if states.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if states.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnSetDropoutDescriptor").entered();
            let result = ffi::cudnnSetDropoutDescriptor(self.handle, handle, dropout, states, statesizeinbytes, seed);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn restore_dropout_descriptor(
        &mut self,
        handle: ffi::cudnnHandle_t,
        dropout: f32,
        states: *mut :: core :: ffi ::c_void,
        statesizeinbytes: usize,
        seed: :: core :: ffi :: c_ulonglong,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if states.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if states.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnRestoreDropoutDescriptor").entered();
            let result = ffi::cudnnRestoreDropoutDescriptor(self.handle, handle, dropout, states, statesizeinbytes, seed);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn get_dropout_descriptor(
        &mut self,
        handle: ffi::cudnnHandle_t,
        dropout: *mutf32,
        states: *mut *mut :: core :: ffi ::c_void,
        seed: *mut :: core :: ffi ::c_ulonglong,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if dropout.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if dropout.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if states.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if states.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if seed.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if seed.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnGetDropoutDescriptor").entered();
            let result = ffi::cudnnGetDropoutDescriptor(self.handle, handle, dropout, states, seed);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

pub fn dropout_forward(
    handle: ffi::cudnnHandle_t,
    dropoutdesc: ffi::cudnnDropoutDescriptor_t,
    xdesc: ffi::cudnnTensorDescriptor_t,
    x: *const :: core :: ffi ::c_void,
    ydesc: ffi::cudnnTensorDescriptor_t,
    y: *mut :: core :: ffi ::c_void,
    reservespace: *mut :: core :: ffi ::c_void,
    reservespacesizeinbytes: usize
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnDropoutForward(handle, dropoutdesc, xdesc, x, ydesc, y, reservespace, reservespacesizeinbytes);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn dropout_backward(
    handle: ffi::cudnnHandle_t,
    dropoutdesc: ffi::cudnnDropoutDescriptor_t,
    dydesc: ffi::cudnnTensorDescriptor_t,
    dy: *const :: core :: ffi ::c_void,
    dxdesc: ffi::cudnnTensorDescriptor_t,
    dx: *mut :: core :: ffi ::c_void,
    reservespace: *mut :: core :: ffi ::c_void,
    reservespacesizeinbytes: usize
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnDropoutBackward(handle, dropoutdesc, dydesc, dy, dxdesc, dx, reservespace, reservespacesizeinbytes);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn set_rnn_descriptor_v_8(
    rnndesc: ffi::cudnnRNNDescriptor_t,
    algo: ffi::cudnnRNNAlgo_t,
    cellmode: ffi::cudnnRNNMode_t,
    biasmode: ffi::cudnnRNNBiasMode_t,
    dirmode: ffi::cudnnDirectionMode_t,
    inputmode: ffi::cudnnRNNInputMode_t,
    datatype: ffi::cudnnDataType_t,
    mathprec: ffi::cudnnDataType_t,
    mathtype: ffi::cudnnMathType_t,
    inputsize: i32,
    hiddensize: i32,
    projsize: i32,
    numlayers: i32,
    dropoutdesc: ffi::cudnnDropoutDescriptor_t,
    auxflags: u32
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnSetRNNDescriptor_v8(rnndesc, algo, cellmode, biasmode, dirmode, inputmode, datatype, mathprec, mathtype, inputsize, hiddensize, projsize, numlayers, dropoutdesc, auxflags);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn get_rnn_descriptor_v_8(
    rnndesc: ffi::cudnnRNNDescriptor_t,
    algo: *mutffi::cudnnRNNAlgo_t,
    cellmode: *mutffi::cudnnRNNMode_t,
    biasmode: *mutffi::cudnnRNNBiasMode_t,
    dirmode: *mutffi::cudnnDirectionMode_t,
    inputmode: *mutffi::cudnnRNNInputMode_t,
    datatype: *mutffi::cudnnDataType_t,
    mathprec: *mutffi::cudnnDataType_t,
    mathtype: *mutffi::cudnnMathType_t,
    inputsize: *muti32,
    hiddensize: *muti32,
    projsize: *muti32,
    numlayers: *muti32,
    dropoutdesc: *mutffi::cudnnDropoutDescriptor_t,
    auxflags: *mutu32
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnGetRNNDescriptor_v8(rnndesc, algo, cellmode, biasmode, dirmode, inputmode, datatype, mathprec, mathtype, inputsize, hiddensize, projsize, numlayers, dropoutdesc, auxflags);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn set_attn_descriptor(
    attndesc: ffi::cudnnAttnDescriptor_t,
    attnmode: :: core :: ffi :: c_uint,
    nheads: :: core :: ffi :: c_int,
    smscaler: f64,
    datatype: ffi::cudnnDataType_t,
    computeprec: ffi::cudnnDataType_t,
    mathtype: ffi::cudnnMathType_t,
    attndropoutdesc: ffi::cudnnDropoutDescriptor_t,
    postdropoutdesc: ffi::cudnnDropoutDescriptor_t,
    qsize: :: core :: ffi :: c_int,
    ksize: :: core :: ffi :: c_int,
    vsize: :: core :: ffi :: c_int,
    qprojsize: :: core :: ffi :: c_int,
    kprojsize: :: core :: ffi :: c_int,
    vprojsize: :: core :: ffi :: c_int,
    oprojsize: :: core :: ffi :: c_int,
    qomaxseqlength: :: core :: ffi :: c_int,
    kvmaxseqlength: :: core :: ffi :: c_int,
    maxbatchsize: :: core :: ffi :: c_int,
    maxbeamsize: :: core :: ffi :: c_int
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnSetAttnDescriptor(attndesc, attnmode, nheads, smscaler, datatype, computeprec, mathtype, attndropoutdesc, postdropoutdesc, qsize, ksize, vsize, qprojsize, kprojsize, vprojsize, oprojsize, qomaxseqlength, kvmaxseqlength, maxbatchsize, maxbeamsize);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn get_attn_descriptor(
    attndesc: ffi::cudnnAttnDescriptor_t,
    attnmode: *mut :: core :: ffi ::c_uint,
    nheads: *mut :: core :: ffi ::c_int,
    smscaler: *mutf64,
    datatype: *mutffi::cudnnDataType_t,
    computeprec: *mutffi::cudnnDataType_t,
    mathtype: *mutffi::cudnnMathType_t,
    attndropoutdesc: *mutffi::cudnnDropoutDescriptor_t,
    postdropoutdesc: *mutffi::cudnnDropoutDescriptor_t,
    qsize: *mut :: core :: ffi ::c_int,
    ksize: *mut :: core :: ffi ::c_int,
    vsize: *mut :: core :: ffi ::c_int,
    qprojsize: *mut :: core :: ffi ::c_int,
    kprojsize: *mut :: core :: ffi ::c_int,
    vprojsize: *mut :: core :: ffi ::c_int,
    oprojsize: *mut :: core :: ffi ::c_int,
    qomaxseqlength: *mut :: core :: ffi ::c_int,
    kvmaxseqlength: *mut :: core :: ffi ::c_int,
    maxbatchsize: *mut :: core :: ffi ::c_int,
    maxbeamsize: *mut :: core :: ffi ::c_int
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnGetAttnDescriptor(attndesc, attnmode, nheads, smscaler, datatype, computeprec, mathtype, attndropoutdesc, postdropoutdesc, qsize, ksize, vsize, qprojsize, kprojsize, vprojsize, oprojsize, qomaxseqlength, kvmaxseqlength, maxbatchsize, maxbeamsize);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

}

// Additional methods for cudaEvent_t
impl CudaEvent {
pub fn ipc_get_event_handle(
    handle: *mutffi::cudaIpcEventHandle_t,
    event: ffi::cudaEvent_t
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaIpcGetEventHandle(handle, event);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn stream_wait_event(
    stream: ffi::cudaStream_t,
    event: ffi::cudaEvent_t,
    flags: :: core :: ffi :: c_uint
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaStreamWaitEvent(stream, event, flags);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

    #[inline]
    pub fn event_record(
        &mut self,
        stream: ffi::cudaStream_t,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudaEventRecord").entered();
            let result = ffi::cudaEventRecord(self.handle, stream);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn event_record_with_flags(
        &mut self,
        stream: ffi::cudaStream_t,
        flags: :: core :: ffi :: c_uint,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudaEventRecordWithFlags").entered();
            let result = ffi::cudaEventRecordWithFlags(self.handle, stream, flags);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn event_query(
        &mut self,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudaEventQuery").entered();
            let result = ffi::cudaEventQuery(self.handle);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn event_synchronize(
        &mut self,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudaEventSynchronize").entered();
            let result = ffi::cudaEventSynchronize(self.handle);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

pub fn event_elapsed_time(
    ms: *mutf32,
    start: ffi::cudaEvent_t,
    end: ffi::cudaEvent_t
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaEventElapsedTime(ms, start, end);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn graph_add_event_record_node(
    pgraphnode: *mutffi::cudaGraphNode_t,
    graph: ffi::cudaGraph_t,
    pdependencies: *constffi::cudaGraphNode_t,
    numdependencies: usize,
    event: ffi::cudaEvent_t
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaGraphAddEventRecordNode(pgraphnode, graph, pdependencies, numdependencies, event);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn graph_event_record_node_get_event(
    node: ffi::cudaGraphNode_t,
    event_out: *mutffi::cudaEvent_t
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaGraphEventRecordNodeGetEvent(node, event_out);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn graph_event_record_node_set_event(
    node: ffi::cudaGraphNode_t,
    event: ffi::cudaEvent_t
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaGraphEventRecordNodeSetEvent(node, event);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn graph_add_event_wait_node(
    pgraphnode: *mutffi::cudaGraphNode_t,
    graph: ffi::cudaGraph_t,
    pdependencies: *constffi::cudaGraphNode_t,
    numdependencies: usize,
    event: ffi::cudaEvent_t
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaGraphAddEventWaitNode(pgraphnode, graph, pdependencies, numdependencies, event);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn graph_event_wait_node_get_event(
    node: ffi::cudaGraphNode_t,
    event_out: *mutffi::cudaEvent_t
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaGraphEventWaitNodeGetEvent(node, event_out);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn graph_event_wait_node_set_event(
    node: ffi::cudaGraphNode_t,
    event: ffi::cudaEvent_t
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaGraphEventWaitNodeSetEvent(node, event);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn graph_exec_event_record_node_set_event(
    hgraphexec: ffi::cudaGraphExec_t,
    hnode: ffi::cudaGraphNode_t,
    event: ffi::cudaEvent_t
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaGraphExecEventRecordNodeSetEvent(hgraphexec, hnode, event);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn graph_exec_event_wait_node_set_event(
    hgraphexec: ffi::cudaGraphExec_t,
    hnode: ffi::cudaGraphNode_t,
    event: ffi::cudaEvent_t
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaGraphExecEventWaitNodeSetEvent(hgraphexec, hnode, event);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

}

// Additional methods for cudaMemPool_t
impl CudaMemPool {
pub fn device_set_mem_pool(
    device: :: core :: ffi :: c_int,
    mempool: ffi::cudaMemPool_t
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaDeviceSetMemPool(device, mempool);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

    #[inline]
    pub fn mem_pool_trim_to(
        &mut self,
        minbytestokeep: usize,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudaMemPoolTrimTo").entered();
            let result = ffi::cudaMemPoolTrimTo(self.handle, minbytestokeep);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn mem_pool_set_attribute(
        &mut self,
        attr: ffi::cudaMemPoolAttr,
        value: *mut :: core :: ffi ::c_void,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if value.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if value.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudaMemPoolSetAttribute").entered();
            let result = ffi::cudaMemPoolSetAttribute(self.handle, attr, value);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn mem_pool_get_attribute(
        &mut self,
        attr: ffi::cudaMemPoolAttr,
        value: *mut :: core :: ffi ::c_void,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if value.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if value.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudaMemPoolGetAttribute").entered();
            let result = ffi::cudaMemPoolGetAttribute(self.handle, attr, value);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn mem_pool_set_access(
        &mut self,
        desclist: *constffi::cudaMemAccessDesc,
        count: usize,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if desclist.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if desclist.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudaMemPoolSetAccess").entered();
            let result = ffi::cudaMemPoolSetAccess(self.handle, desclist, count);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

pub fn mem_pool_get_access(
    flags: *mutffi::cudaMemAccessFlags,
    mempool: ffi::cudaMemPool_t,
    location: *mutffi::cudaMemLocation
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaMemPoolGetAccess(flags, mempool, location);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn mem_set_mem_pool(
    location: *mutffi::cudaMemLocation,
    type_: ffi::cudaMemAllocationType,
    mempool: ffi::cudaMemPool_t
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaMemSetMemPool(location, type_, mempool);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn malloc_from_pool_async(
    ptr: *mut *mut :: core :: ffi ::c_void,
    size: usize,
    mempool: ffi::cudaMemPool_t,
    stream: ffi::cudaStream_t
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaMallocFromPoolAsync(ptr, size, mempool, stream);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn mem_pool_export_to_shareable_handle(
    shareablehandle: *mut :: core :: ffi ::c_void,
    mempool: ffi::cudaMemPool_t,
    handletype: ffi::cudaMemAllocationHandleType,
    flags: :: core :: ffi :: c_uint
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaMemPoolExportToShareableHandle(shareablehandle, mempool, handletype, flags);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn mem_pool_import_pointer(
    ptr: *mut *mut :: core :: ffi ::c_void,
    mempool: ffi::cudaMemPool_t,
    exportdata: *mutffi::cudaMemPoolPtrExportData
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaMemPoolImportPointer(ptr, mempool, exportdata);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

}

// Additional methods for cudaArray_const_t
impl CudaArrayConst {
pub fn memcpy_2d_from_array(
    dst: *mut :: core :: ffi ::c_void,
    dpitch: usize,
    src: ffi::cudaArray_const_t,
    woffset: usize,
    hoffset: usize,
    width: usize,
    height: usize,
    kind: ffi::cudaMemcpyKind
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaMemcpy2DFromArray(dst, dpitch, src, woffset, hoffset, width, height, kind);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn memcpy_2d_array_to_array(
    dst: ffi::cudaArray_t,
    woffsetdst: usize,
    hoffsetdst: usize,
    src: ffi::cudaArray_const_t,
    woffsetsrc: usize,
    hoffsetsrc: usize,
    width: usize,
    height: usize,
    kind: ffi::cudaMemcpyKind
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaMemcpy2DArrayToArray(dst, woffsetdst, hoffsetdst, src, woffsetsrc, hoffsetsrc, width, height, kind);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn memcpy_2d_from_array_async(
    dst: *mut :: core :: ffi ::c_void,
    dpitch: usize,
    src: ffi::cudaArray_const_t,
    woffset: usize,
    hoffset: usize,
    width: usize,
    height: usize,
    kind: ffi::cudaMemcpyKind,
    stream: ffi::cudaStream_t
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaMemcpy2DFromArrayAsync(dst, dpitch, src, woffset, hoffset, width, height, kind, stream);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn memcpy_from_array(
    dst: *mut :: core :: ffi ::c_void,
    src: ffi::cudaArray_const_t,
    woffset: usize,
    hoffset: usize,
    count: usize,
    kind: ffi::cudaMemcpyKind
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaMemcpyFromArray(dst, src, woffset, hoffset, count, kind);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn memcpy_array_to_array(
    dst: ffi::cudaArray_t,
    woffsetdst: usize,
    hoffsetdst: usize,
    src: ffi::cudaArray_const_t,
    woffsetsrc: usize,
    hoffsetsrc: usize,
    count: usize,
    kind: ffi::cudaMemcpyKind
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaMemcpyArrayToArray(dst, woffsetdst, hoffsetdst, src, woffsetsrc, hoffsetsrc, count, kind);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn memcpy_from_array_async(
    dst: *mut :: core :: ffi ::c_void,
    src: ffi::cudaArray_const_t,
    woffset: usize,
    hoffset: usize,
    count: usize,
    kind: ffi::cudaMemcpyKind,
    stream: ffi::cudaStream_t
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaMemcpyFromArrayAsync(dst, src, woffset, hoffset, count, kind, stream);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn get_channel_desc(
    desc: *mutffi::cudaChannelFormatDesc,
    array: ffi::cudaArray_const_t
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaGetChannelDesc(desc, array);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

}

// Additional methods for cudnnCallback_t
impl CudnnCallback {
pub fn set_callback(
    mask: :: core :: ffi :: c_uint,
    udata: *mut :: core :: ffi ::c_void,
    fptr: ffi::cudnnCallback_t
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnSetCallback(mask, udata, fptr);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn get_callback(
    mask: *mut :: core :: ffi ::c_uint,
    udata: *mut *mut :: core :: ffi ::c_void,
    fptr: *mutffi::cudnnCallback_t
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnGetCallback(mask, udata, fptr);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

}

// Additional methods for cudnnLRNDescriptor_t
impl CudnnLRNDescriptor {
    #[inline]
    pub fn set_lrn_descriptor(
        &mut self,
        lrnn: :: core :: ffi :: c_uint,
        lrnalpha: f64,
        lrnbeta: f64,
        lrnk: f64,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnSetLRNDescriptor").entered();
            let result = ffi::cudnnSetLRNDescriptor(self.handle, lrnn, lrnalpha, lrnbeta, lrnk);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn get_lrn_descriptor(
        &mut self,
        lrnn: *mut :: core :: ffi ::c_uint,
        lrnalpha: *mutf64,
        lrnbeta: *mutf64,
        lrnk: *mutf64,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if lrnn.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if lrnn.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if lrnalpha.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if lrnalpha.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if lrnbeta.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if lrnbeta.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if lrnk.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if lrnk.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnGetLRNDescriptor").entered();
            let result = ffi::cudnnGetLRNDescriptor(self.handle, lrnn, lrnalpha, lrnbeta, lrnk);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

pub fn lrn_cross_channel_forward(
    handle: ffi::cudnnHandle_t,
    normdesc: ffi::cudnnLRNDescriptor_t,
    lrnmode: ffi::cudnnLRNMode_t,
    alpha: *const :: core :: ffi ::c_void,
    xdesc: ffi::cudnnTensorDescriptor_t,
    x: *const :: core :: ffi ::c_void,
    beta: *const :: core :: ffi ::c_void,
    ydesc: ffi::cudnnTensorDescriptor_t,
    y: *mut :: core :: ffi ::c_void
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnLRNCrossChannelForward(handle, normdesc, lrnmode, alpha, xdesc, x, beta, ydesc, y);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn divisive_normalization_forward(
    handle: ffi::cudnnHandle_t,
    normdesc: ffi::cudnnLRNDescriptor_t,
    mode: ffi::cudnnDivNormMode_t,
    alpha: *const :: core :: ffi ::c_void,
    xdesc: ffi::cudnnTensorDescriptor_t,
    x: *const :: core :: ffi ::c_void,
    means: *const :: core :: ffi ::c_void,
    temp: *mut :: core :: ffi ::c_void,
    temp2: *mut :: core :: ffi ::c_void,
    beta: *const :: core :: ffi ::c_void,
    ydesc: ffi::cudnnTensorDescriptor_t,
    y: *mut :: core :: ffi ::c_void
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnDivisiveNormalizationForward(handle, normdesc, mode, alpha, xdesc, x, means, temp, temp2, beta, ydesc, y);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn lrn_cross_channel_backward(
    handle: ffi::cudnnHandle_t,
    normdesc: ffi::cudnnLRNDescriptor_t,
    lrnmode: ffi::cudnnLRNMode_t,
    alpha: *const :: core :: ffi ::c_void,
    ydesc: ffi::cudnnTensorDescriptor_t,
    y: *const :: core :: ffi ::c_void,
    dydesc: ffi::cudnnTensorDescriptor_t,
    dy: *const :: core :: ffi ::c_void,
    xdesc: ffi::cudnnTensorDescriptor_t,
    x: *const :: core :: ffi ::c_void,
    beta: *const :: core :: ffi ::c_void,
    dxdesc: ffi::cudnnTensorDescriptor_t,
    dx: *mut :: core :: ffi ::c_void
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnLRNCrossChannelBackward(handle, normdesc, lrnmode, alpha, ydesc, y, dydesc, dy, xdesc, x, beta, dxdesc, dx);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn divisive_normalization_backward(
    handle: ffi::cudnnHandle_t,
    normdesc: ffi::cudnnLRNDescriptor_t,
    mode: ffi::cudnnDivNormMode_t,
    alpha: *const :: core :: ffi ::c_void,
    xdesc: ffi::cudnnTensorDescriptor_t,
    x: *const :: core :: ffi ::c_void,
    means: *const :: core :: ffi ::c_void,
    dy: *const :: core :: ffi ::c_void,
    temp: *mut :: core :: ffi ::c_void,
    temp2: *mut :: core :: ffi ::c_void,
    beta: *const :: core :: ffi ::c_void,
    dxdmeansdesc: ffi::cudnnTensorDescriptor_t,
    dx: *mut :: core :: ffi ::c_void,
    dmeans: *mut :: core :: ffi ::c_void
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnDivisiveNormalizationBackward(handle, normdesc, mode, alpha, xdesc, x, means, dy, temp, temp2, beta, dxdmeansdesc, dx, dmeans);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

}

// Additional methods for cudaUserObject_t
impl CudaUserObject {
    #[inline]
    pub fn user_object_retain(
        &mut self,
        count: :: core :: ffi :: c_uint,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudaUserObjectRetain").entered();
            let result = ffi::cudaUserObjectRetain(self.handle, count);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

pub fn graph_retain_user_object(
    graph: ffi::cudaGraph_t,
    object: ffi::cudaUserObject_t,
    count: :: core :: ffi :: c_uint,
    flags: :: core :: ffi :: c_uint
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaGraphRetainUserObject(graph, object, count, flags);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn graph_release_user_object(
    graph: ffi::cudaGraph_t,
    object: ffi::cudaUserObject_t,
    count: :: core :: ffi :: c_uint
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaGraphReleaseUserObject(graph, object, count);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

}

// Additional methods for cudnnFusedOpsPlan_t
impl CudnnFusedOpsPlan {
pub fn make_fused_ops_plan(
    handle: ffi::cudnnHandle_t,
    plan: ffi::cudnnFusedOpsPlan_t,
    constpack: ffi::cudnnFusedOpsConstParamPack_t,
    workspacesizeinbytes: *mutusize
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnMakeFusedOpsPlan(handle, plan, constpack, workspacesizeinbytes);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn fused_ops_execute(
    handle: ffi::cudnnHandle_t,
    plan: ffi::cudnnFusedOpsPlan_t,
    varpack: ffi::cudnnFusedOpsVariantParamPack_t
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnFusedOpsExecute(handle, plan, varpack);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

}

// Additional methods for cudaLibrary_t
impl CudaLibrary {
    #[inline]
    pub fn library_unload(
        &mut self,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudaLibraryUnload").entered();
            let result = ffi::cudaLibraryUnload(self.handle);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

pub fn library_get_kernel(
    pkernel: *mutffi::cudaKernel_t,
    library: ffi::cudaLibrary_t,
    name: &str
) -> Result<(), Error> {
    unsafe {
        let name_cstr = std::ffi::CString::new(name)
            .map_err(|_| Error::InvalidString)?;
        let result = ffi::cudaLibraryGetKernel(pkernel, library, name_cstr.as_ptr());
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn library_get_global(
    dptr: *mut *mut :: core :: ffi ::c_void,
    bytes: *mutusize,
    library: ffi::cudaLibrary_t,
    name: &str
) -> Result<(), Error> {
    unsafe {
        let name_cstr = std::ffi::CString::new(name)
            .map_err(|_| Error::InvalidString)?;
        let result = ffi::cudaLibraryGetGlobal(dptr, bytes, library, name_cstr.as_ptr());
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn library_get_managed(
    dptr: *mut *mut :: core :: ffi ::c_void,
    bytes: *mutusize,
    library: ffi::cudaLibrary_t,
    name: &str
) -> Result<(), Error> {
    unsafe {
        let name_cstr = std::ffi::CString::new(name)
            .map_err(|_| Error::InvalidString)?;
        let result = ffi::cudaLibraryGetManaged(dptr, bytes, library, name_cstr.as_ptr());
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn library_get_unified_function(
    fptr: *mut *mut :: core :: ffi ::c_void,
    library: ffi::cudaLibrary_t,
    symbol: &str
) -> Result<(), Error> {
    unsafe {
        let symbol_cstr = std::ffi::CString::new(symbol)
            .map_err(|_| Error::InvalidString)?;
        let result = ffi::cudaLibraryGetUnifiedFunction(fptr, library, symbol_cstr.as_ptr());
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn library_get_kernel_count(
    count: *mut :: core :: ffi ::c_uint,
    lib: ffi::cudaLibrary_t
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaLibraryGetKernelCount(count, lib);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn library_enumerate_kernels(
    kernels: *mutffi::cudaKernel_t,
    numkernels: :: core :: ffi :: c_uint,
    lib: ffi::cudaLibrary_t
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaLibraryEnumerateKernels(kernels, numkernels, lib);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

}

// Additional methods for cudaStream_t
impl CudaStream {
    #[inline]
    pub fn stream_get_priority(
        &mut self,
        priority: *mut :: core :: ffi ::c_int,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if priority.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if priority.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudaStreamGetPriority").entered();
            let result = ffi::cudaStreamGetPriority(self.handle, priority);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn stream_get_flags(
        &mut self,
        flags: *mut :: core :: ffi ::c_uint,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if flags.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if flags.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudaStreamGetFlags").entered();
            let result = ffi::cudaStreamGetFlags(self.handle, flags);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn stream_get_id(
        &mut self,
        streamid: *mut :: core :: ffi ::c_ulonglong,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if streamid.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if streamid.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudaStreamGetId").entered();
            let result = ffi::cudaStreamGetId(self.handle, streamid);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn stream_get_device(
        &mut self,
        device: *mut :: core :: ffi ::c_int,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if device.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if device.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudaStreamGetDevice").entered();
            let result = ffi::cudaStreamGetDevice(self.handle, device);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn stream_copy_attributes(
        &mut self,
        src: ffi::cudaStream_t,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudaStreamCopyAttributes").entered();
            let result = ffi::cudaStreamCopyAttributes(self.handle, src);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn stream_get_attribute(
        &mut self,
        attr: ffi::cudaLaunchAttributeID,
        value_out: *mutffi::cudaLaunchAttributeValue,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if value_out.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if value_out.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudaStreamGetAttribute").entered();
            let result = ffi::cudaStreamGetAttribute(self.handle, attr, value_out);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn stream_set_attribute(
        &mut self,
        attr: ffi::cudaLaunchAttributeID,
        value: *constffi::cudaLaunchAttributeValue,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if value.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if value.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudaStreamSetAttribute").entered();
            let result = ffi::cudaStreamSetAttribute(self.handle, attr, value);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn stream_wait_event(
        &mut self,
        event: ffi::cudaEvent_t,
        flags: :: core :: ffi :: c_uint,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudaStreamWaitEvent").entered();
            let result = ffi::cudaStreamWaitEvent(self.handle, event, flags);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn stream_add_callback(
        &mut self,
        callback: ffi::cudaStreamCallback_t,
        userdata: *mut :: core :: ffi ::c_void,
        flags: :: core :: ffi :: c_uint,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if userdata.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if userdata.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudaStreamAddCallback").entered();
            let result = ffi::cudaStreamAddCallback(self.handle, callback, userdata, flags);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn stream_synchronize(
        &mut self,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudaStreamSynchronize").entered();
            let result = ffi::cudaStreamSynchronize(self.handle);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn stream_query(
        &mut self,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudaStreamQuery").entered();
            let result = ffi::cudaStreamQuery(self.handle);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn stream_attach_mem_async(
        &mut self,
        devptr: *mut :: core :: ffi ::c_void,
        length: usize,
        flags: :: core :: ffi :: c_uint,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if devptr.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if devptr.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudaStreamAttachMemAsync").entered();
            let result = ffi::cudaStreamAttachMemAsync(self.handle, devptr, length, flags);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn stream_begin_capture(
        &mut self,
        mode: ffi::cudaStreamCaptureMode,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudaStreamBeginCapture").entered();
            let result = ffi::cudaStreamBeginCapture(self.handle, mode);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn stream_begin_capture_to_graph(
        &mut self,
        graph: ffi::cudaGraph_t,
        dependencies: *constffi::cudaGraphNode_t,
        dependencydata: *constffi::cudaGraphEdgeData,
        numdependencies: usize,
        mode: ffi::cudaStreamCaptureMode,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if dependencies.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if dependencies.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if dependencydata.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if dependencydata.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudaStreamBeginCaptureToGraph").entered();
            let result = ffi::cudaStreamBeginCaptureToGraph(self.handle, graph, dependencies, dependencydata, numdependencies, mode);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn stream_end_capture(
        &mut self,
        pgraph: *mutffi::cudaGraph_t,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if pgraph.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if pgraph.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudaStreamEndCapture").entered();
            let result = ffi::cudaStreamEndCapture(self.handle, pgraph);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn stream_is_capturing(
        &mut self,
        pcapturestatus: *mutffi::cudaStreamCaptureStatus,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if pcapturestatus.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if pcapturestatus.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudaStreamIsCapturing").entered();
            let result = ffi::cudaStreamIsCapturing(self.handle, pcapturestatus);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn stream_get_capture_info(
        &mut self,
        capturestatus_out: *mutffi::cudaStreamCaptureStatus,
        id_out: *mut :: core :: ffi ::c_ulonglong,
        graph_out: *mutffi::cudaGraph_t,
        dependencies_out: *mut *constffi::cudaGraphNode_t,
        edgedata_out: *mut *constffi::cudaGraphEdgeData,
        numdependencies_out: *mutusize,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if capturestatus_out.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if capturestatus_out.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if id_out.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if id_out.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if graph_out.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if graph_out.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if dependencies_out.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if dependencies_out.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if edgedata_out.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if edgedata_out.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if numdependencies_out.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if numdependencies_out.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudaStreamGetCaptureInfo").entered();
            let result = ffi::cudaStreamGetCaptureInfo(self.handle, capturestatus_out, id_out, graph_out, dependencies_out, edgedata_out, numdependencies_out);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn stream_update_capture_dependencies(
        &mut self,
        dependencies: *mutffi::cudaGraphNode_t,
        dependencydata: *constffi::cudaGraphEdgeData,
        numdependencies: usize,
        flags: :: core :: ffi :: c_uint,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if dependencies.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if dependencies.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if dependencydata.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if dependencydata.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudaStreamUpdateCaptureDependencies").entered();
            let result = ffi::cudaStreamUpdateCaptureDependencies(self.handle, dependencies, dependencydata, numdependencies, flags);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

pub fn event_record(
    event: ffi::cudaEvent_t,
    stream: ffi::cudaStream_t
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaEventRecord(event, stream);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn event_record_with_flags(
    event: ffi::cudaEvent_t,
    stream: ffi::cudaStream_t,
    flags: :: core :: ffi :: c_uint
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaEventRecordWithFlags(event, stream, flags);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn signal_external_semaphores_async(
    extsemarray: *constffi::cudaExternalSemaphore_t,
    paramsarray: *constffi::cudaExternalSemaphoreSignalParams,
    numextsems: :: core :: ffi :: c_uint,
    stream: ffi::cudaStream_t
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaSignalExternalSemaphoresAsync(extsemarray, paramsarray, numextsems, stream);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn wait_external_semaphores_async(
    extsemarray: *constffi::cudaExternalSemaphore_t,
    paramsarray: *constffi::cudaExternalSemaphoreWaitParams,
    numextsems: :: core :: ffi :: c_uint,
    stream: ffi::cudaStream_t
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaWaitExternalSemaphoresAsync(extsemarray, paramsarray, numextsems, stream);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn launch_kernel(
    func: *const :: core :: ffi ::c_void,
    griddim: ffi::dim3,
    blockdim: ffi::dim3,
    args: *mut *mut :: core :: ffi ::c_void,
    sharedmem: usize,
    stream: ffi::cudaStream_t
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaLaunchKernel(func, griddim, blockdim, args, sharedmem, stream);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn launch_cooperative_kernel(
    func: *const :: core :: ffi ::c_void,
    griddim: ffi::dim3,
    blockdim: ffi::dim3,
    args: *mut *mut :: core :: ffi ::c_void,
    sharedmem: usize,
    stream: ffi::cudaStream_t
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaLaunchCooperativeKernel(func, griddim, blockdim, args, sharedmem, stream);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

    #[inline]
    pub fn launch_host_func(
        &mut self,
        fn_: ffi::cudaHostFn_t,
        userdata: *mut :: core :: ffi ::c_void,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if userdata.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if userdata.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudaLaunchHostFunc").entered();
            let result = ffi::cudaLaunchHostFunc(self.handle, fn_, userdata);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

pub fn memcpy_3d_async(
    p: *constffi::cudaMemcpy3DParms,
    stream: ffi::cudaStream_t
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaMemcpy3DAsync(p, stream);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn memcpy_3d_peer_async(
    p: *constffi::cudaMemcpy3DPeerParms,
    stream: ffi::cudaStream_t
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaMemcpy3DPeerAsync(p, stream);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn memcpy_async(
    dst: *mut :: core :: ffi ::c_void,
    src: *const :: core :: ffi ::c_void,
    count: usize,
    kind: ffi::cudaMemcpyKind,
    stream: ffi::cudaStream_t
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaMemcpyAsync(dst, src, count, kind, stream);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn memcpy_peer_async(
    dst: *mut :: core :: ffi ::c_void,
    dstdevice: :: core :: ffi :: c_int,
    src: *const :: core :: ffi ::c_void,
    srcdevice: :: core :: ffi :: c_int,
    count: usize,
    stream: ffi::cudaStream_t
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaMemcpyPeerAsync(dst, dstdevice, src, srcdevice, count, stream);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn memcpy_batch_async(
    dsts: *const *mut :: core :: ffi ::c_void,
    srcs: *const *const :: core :: ffi ::c_void,
    sizes: *constusize,
    count: usize,
    attrs: *mutffi::cudaMemcpyAttributes,
    attrsidxs: *mutusize,
    numattrs: usize,
    stream: ffi::cudaStream_t
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaMemcpyBatchAsync(dsts, srcs, sizes, count, attrs, attrsidxs, numattrs, stream);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn memcpy_3d_batch_async(
    numops: usize,
    oplist: *mutffi::cudaMemcpy3DBatchOp,
    flags: :: core :: ffi :: c_ulonglong,
    stream: ffi::cudaStream_t
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaMemcpy3DBatchAsync(numops, oplist, flags, stream);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn memcpy_2d_async(
    dst: *mut :: core :: ffi ::c_void,
    dpitch: usize,
    src: *const :: core :: ffi ::c_void,
    spitch: usize,
    width: usize,
    height: usize,
    kind: ffi::cudaMemcpyKind,
    stream: ffi::cudaStream_t
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaMemcpy2DAsync(dst, dpitch, src, spitch, width, height, kind, stream);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn memcpy_2d_to_array_async(
    dst: ffi::cudaArray_t,
    woffset: usize,
    hoffset: usize,
    src: *const :: core :: ffi ::c_void,
    spitch: usize,
    width: usize,
    height: usize,
    kind: ffi::cudaMemcpyKind,
    stream: ffi::cudaStream_t
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaMemcpy2DToArrayAsync(dst, woffset, hoffset, src, spitch, width, height, kind, stream);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn memcpy_2d_from_array_async(
    dst: *mut :: core :: ffi ::c_void,
    dpitch: usize,
    src: ffi::cudaArray_const_t,
    woffset: usize,
    hoffset: usize,
    width: usize,
    height: usize,
    kind: ffi::cudaMemcpyKind,
    stream: ffi::cudaStream_t
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaMemcpy2DFromArrayAsync(dst, dpitch, src, woffset, hoffset, width, height, kind, stream);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn memcpy_to_symbol_async(
    symbol: *const :: core :: ffi ::c_void,
    src: *const :: core :: ffi ::c_void,
    count: usize,
    offset: usize,
    kind: ffi::cudaMemcpyKind,
    stream: ffi::cudaStream_t
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaMemcpyToSymbolAsync(symbol, src, count, offset, kind, stream);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn memcpy_from_symbol_async(
    dst: *mut :: core :: ffi ::c_void,
    symbol: *const :: core :: ffi ::c_void,
    count: usize,
    offset: usize,
    kind: ffi::cudaMemcpyKind,
    stream: ffi::cudaStream_t
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaMemcpyFromSymbolAsync(dst, symbol, count, offset, kind, stream);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn memset_async(
    devptr: *mut :: core :: ffi ::c_void,
    value: :: core :: ffi :: c_int,
    count: usize,
    stream: ffi::cudaStream_t
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaMemsetAsync(devptr, value, count, stream);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn memset_2d_async(
    devptr: *mut :: core :: ffi ::c_void,
    pitch: usize,
    value: :: core :: ffi :: c_int,
    width: usize,
    height: usize,
    stream: ffi::cudaStream_t
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaMemset2DAsync(devptr, pitch, value, width, height, stream);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn memset_3d_async(
    pitcheddevptr: ffi::cudaPitchedPtr,
    value: :: core :: ffi :: c_int,
    extent: ffi::cudaExtent,
    stream: ffi::cudaStream_t
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaMemset3DAsync(pitcheddevptr, value, extent, stream);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn mem_prefetch_async(
    devptr: *const :: core :: ffi ::c_void,
    count: usize,
    location: ffi::cudaMemLocation,
    flags: :: core :: ffi :: c_uint,
    stream: ffi::cudaStream_t
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaMemPrefetchAsync(devptr, count, location, flags, stream);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn mem_prefetch_batch_async(
    dptrs: *mut *mut :: core :: ffi ::c_void,
    sizes: *mutusize,
    count: usize,
    prefetchlocs: *mutffi::cudaMemLocation,
    prefetchlocidxs: *mutusize,
    numprefetchlocs: usize,
    flags: :: core :: ffi :: c_ulonglong,
    stream: ffi::cudaStream_t
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaMemPrefetchBatchAsync(dptrs, sizes, count, prefetchlocs, prefetchlocidxs, numprefetchlocs, flags, stream);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn mem_discard_batch_async(
    dptrs: *mut *mut :: core :: ffi ::c_void,
    sizes: *mutusize,
    count: usize,
    flags: :: core :: ffi :: c_ulonglong,
    stream: ffi::cudaStream_t
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaMemDiscardBatchAsync(dptrs, sizes, count, flags, stream);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn mem_discard_and_prefetch_batch_async(
    dptrs: *mut *mut :: core :: ffi ::c_void,
    sizes: *mutusize,
    count: usize,
    prefetchlocs: *mutffi::cudaMemLocation,
    prefetchlocidxs: *mutusize,
    numprefetchlocs: usize,
    flags: :: core :: ffi :: c_ulonglong,
    stream: ffi::cudaStream_t
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaMemDiscardAndPrefetchBatchAsync(dptrs, sizes, count, prefetchlocs, prefetchlocidxs, numprefetchlocs, flags, stream);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn memcpy_to_array_async(
    dst: ffi::cudaArray_t,
    woffset: usize,
    hoffset: usize,
    src: *const :: core :: ffi ::c_void,
    count: usize,
    kind: ffi::cudaMemcpyKind,
    stream: ffi::cudaStream_t
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaMemcpyToArrayAsync(dst, woffset, hoffset, src, count, kind, stream);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn memcpy_from_array_async(
    dst: *mut :: core :: ffi ::c_void,
    src: ffi::cudaArray_const_t,
    woffset: usize,
    hoffset: usize,
    count: usize,
    kind: ffi::cudaMemcpyKind,
    stream: ffi::cudaStream_t
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaMemcpyFromArrayAsync(dst, src, woffset, hoffset, count, kind, stream);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn malloc_async(
    devptr: *mut *mut :: core :: ffi ::c_void,
    size: usize,
    hstream: ffi::cudaStream_t
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaMallocAsync(devptr, size, hstream);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn free_async(
    devptr: *mut :: core :: ffi ::c_void,
    hstream: ffi::cudaStream_t
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaFreeAsync(devptr, hstream);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn malloc_from_pool_async(
    ptr: *mut *mut :: core :: ffi ::c_void,
    size: usize,
    mempool: ffi::cudaMemPool_t,
    stream: ffi::cudaStream_t
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaMallocFromPoolAsync(ptr, size, mempool, stream);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn graphics_map_resources(
    count: :: core :: ffi :: c_int,
    resources: *mutffi::cudaGraphicsResource_t,
    stream: ffi::cudaStream_t
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaGraphicsMapResources(count, resources, stream);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn graphics_unmap_resources(
    count: :: core :: ffi :: c_int,
    resources: *mutffi::cudaGraphicsResource_t,
    stream: ffi::cudaStream_t
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaGraphicsUnmapResources(count, resources, stream);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn graph_upload(
    graphexec: ffi::cudaGraphExec_t,
    stream: ffi::cudaStream_t
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaGraphUpload(graphexec, stream);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn graph_launch(
    graphexec: ffi::cudaGraphExec_t,
    stream: ffi::cudaStream_t
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaGraphLaunch(graphexec, stream);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn set_stream(
    handle: ffi::cudnnHandle_t,
    streamid: ffi::cudaStream_t
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnSetStream(handle, streamid);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn get_stream(
    handle: ffi::cudnnHandle_t,
    streamid: *mutffi::cudaStream_t
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnGetStream(handle, streamid);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

}

// Additional methods for cudaMipmappedArray_t
impl CudaMipmappedArray {
pub fn mipmapped_array_get_memory_requirements(
    memoryrequirements: *mutffi::cudaArrayMemoryRequirements,
    mipmap: ffi::cudaMipmappedArray_t,
    device: :: core :: ffi :: c_int
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaMipmappedArrayGetMemoryRequirements(memoryrequirements, mipmap, device);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn mipmapped_array_get_sparse_properties(
    sparseproperties: *mutffi::cudaArraySparseProperties,
    mipmap: ffi::cudaMipmappedArray_t
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaMipmappedArrayGetSparseProperties(sparseproperties, mipmap);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

}

// Additional methods for cudaArray_t
impl CudaArray {
pub fn array_get_info(
    desc: *mutffi::cudaChannelFormatDesc,
    extent: *mutffi::cudaExtent,
    flags: *mut :: core :: ffi ::c_uint,
    array: ffi::cudaArray_t
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaArrayGetInfo(desc, extent, flags, array);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn array_get_memory_requirements(
    memoryrequirements: *mutffi::cudaArrayMemoryRequirements,
    array: ffi::cudaArray_t,
    device: :: core :: ffi :: c_int
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaArrayGetMemoryRequirements(memoryrequirements, array, device);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn array_get_sparse_properties(
    sparseproperties: *mutffi::cudaArraySparseProperties,
    array: ffi::cudaArray_t
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaArrayGetSparseProperties(sparseproperties, array);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

    #[inline]
    pub fn memcpy_2d_to_array(
        &mut self,
        woffset: usize,
        hoffset: usize,
        src: *const :: core :: ffi ::c_void,
        spitch: usize,
        width: usize,
        height: usize,
        kind: ffi::cudaMemcpyKind,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if src.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if src.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudaMemcpy2DToArray").entered();
            let result = ffi::cudaMemcpy2DToArray(self.handle, woffset, hoffset, src, spitch, width, height, kind);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn memcpy_2d_array_to_array(
        &mut self,
        woffsetdst: usize,
        hoffsetdst: usize,
        src: ffi::cudaArray_const_t,
        woffsetsrc: usize,
        hoffsetsrc: usize,
        width: usize,
        height: usize,
        kind: ffi::cudaMemcpyKind,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudaMemcpy2DArrayToArray").entered();
            let result = ffi::cudaMemcpy2DArrayToArray(self.handle, woffsetdst, hoffsetdst, src, woffsetsrc, hoffsetsrc, width, height, kind);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn memcpy_2d_to_array_async(
        &mut self,
        woffset: usize,
        hoffset: usize,
        src: *const :: core :: ffi ::c_void,
        spitch: usize,
        width: usize,
        height: usize,
        kind: ffi::cudaMemcpyKind,
        stream: ffi::cudaStream_t,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if src.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if src.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudaMemcpy2DToArrayAsync").entered();
            let result = ffi::cudaMemcpy2DToArrayAsync(self.handle, woffset, hoffset, src, spitch, width, height, kind, stream);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn memcpy_to_array(
        &mut self,
        woffset: usize,
        hoffset: usize,
        src: *const :: core :: ffi ::c_void,
        count: usize,
        kind: ffi::cudaMemcpyKind,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if src.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if src.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudaMemcpyToArray").entered();
            let result = ffi::cudaMemcpyToArray(self.handle, woffset, hoffset, src, count, kind);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn memcpy_array_to_array(
        &mut self,
        woffsetdst: usize,
        hoffsetdst: usize,
        src: ffi::cudaArray_const_t,
        woffsetsrc: usize,
        hoffsetsrc: usize,
        count: usize,
        kind: ffi::cudaMemcpyKind,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudaMemcpyArrayToArray").entered();
            let result = ffi::cudaMemcpyArrayToArray(self.handle, woffsetdst, hoffsetdst, src, woffsetsrc, hoffsetsrc, count, kind);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn memcpy_to_array_async(
        &mut self,
        woffset: usize,
        hoffset: usize,
        src: *const :: core :: ffi ::c_void,
        count: usize,
        kind: ffi::cudaMemcpyKind,
        stream: ffi::cudaStream_t,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if src.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if src.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudaMemcpyToArrayAsync").entered();
            let result = ffi::cudaMemcpyToArrayAsync(self.handle, woffset, hoffset, src, count, kind, stream);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

}

// Additional methods for cudnnLossNormalizationMode_t
impl CudnnLossNormalizationMode {
pub fn set_ctc_loss_descriptor_ex(
    ctclossdesc: ffi::cudnnCTCLossDescriptor_t,
    comptype: ffi::cudnnDataType_t,
    normmode: ffi::cudnnLossNormalizationMode_t,
    gradmode: ffi::cudnnNanPropagation_t
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnSetCTCLossDescriptorEx(ctclossdesc, comptype, normmode, gradmode);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn set_ctc_loss_descriptor_v_8(
    ctclossdesc: ffi::cudnnCTCLossDescriptor_t,
    comptype: ffi::cudnnDataType_t,
    normmode: ffi::cudnnLossNormalizationMode_t,
    gradmode: ffi::cudnnNanPropagation_t,
    maxlabellength: :: core :: ffi :: c_int
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnSetCTCLossDescriptor_v8(ctclossdesc, comptype, normmode, gradmode, maxlabellength);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn set_ctc_loss_descriptor_v_9(
    ctclossdesc: ffi::cudnnCTCLossDescriptor_t,
    comptype: ffi::cudnnDataType_t,
    normmode: ffi::cudnnLossNormalizationMode_t,
    ctcgradmode: ffi::cudnnCTCGradMode_t,
    maxlabellength: :: core :: ffi :: c_int
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnSetCTCLossDescriptor_v9(ctclossdesc, comptype, normmode, ctcgradmode, maxlabellength);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn get_ctc_loss_descriptor_ex(
    ctclossdesc: ffi::cudnnCTCLossDescriptor_t,
    comptype: *mutffi::cudnnDataType_t,
    normmode: *mutffi::cudnnLossNormalizationMode_t,
    gradmode: *mutffi::cudnnNanPropagation_t
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnGetCTCLossDescriptorEx(ctclossdesc, comptype, normmode, gradmode);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn get_ctc_loss_descriptor_v_8(
    ctclossdesc: ffi::cudnnCTCLossDescriptor_t,
    comptype: *mutffi::cudnnDataType_t,
    normmode: *mutffi::cudnnLossNormalizationMode_t,
    gradmode: *mutffi::cudnnNanPropagation_t,
    maxlabellength: *mut :: core :: ffi ::c_int
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnGetCTCLossDescriptor_v8(ctclossdesc, comptype, normmode, gradmode, maxlabellength);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn get_ctc_loss_descriptor_v_9(
    ctclossdesc: ffi::cudnnCTCLossDescriptor_t,
    comptype: *mutffi::cudnnDataType_t,
    normmode: *mutffi::cudnnLossNormalizationMode_t,
    ctcgradmode: *mutffi::cudnnCTCGradMode_t,
    maxlabellength: *mut :: core :: ffi ::c_int
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnGetCTCLossDescriptor_v9(ctclossdesc, comptype, normmode, ctcgradmode, maxlabellength);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

}

// Additional methods for cudaAsyncCallbackHandle_t
impl CudaAsyncCallbackHandle {
pub fn device_register_async_notification(
    device: :: core :: ffi :: c_int,
    callbackfunc: ffi::cudaAsyncCallback,
    userdata: *mut :: core :: ffi ::c_void,
    callback: *mutffi::cudaAsyncCallbackHandle_t
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaDeviceRegisterAsyncNotification(device, callbackfunc, userdata, callback);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn device_unregister_async_notification(
    device: :: core :: ffi :: c_int,
    callback: ffi::cudaAsyncCallbackHandle_t
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaDeviceUnregisterAsyncNotification(device, callback);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

}

// Additional methods for cudnnFusedOpsVariantParamPack_t
impl CudnnFusedOpsVariantParamPack {
    #[inline]
    pub fn set_fused_ops_variant_param_pack_attribute(
        &mut self,
        paramlabel: ffi::cudnnFusedOpsVariantParamLabel_t,
        ptr: *mut :: core :: ffi ::c_void,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if ptr.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if ptr.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnSetFusedOpsVariantParamPackAttribute").entered();
            let result = ffi::cudnnSetFusedOpsVariantParamPackAttribute(self.handle, paramlabel, ptr);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn get_fused_ops_variant_param_pack_attribute(
        &mut self,
        paramlabel: ffi::cudnnFusedOpsVariantParamLabel_t,
        ptr: *mut :: core :: ffi ::c_void,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if ptr.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if ptr.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnGetFusedOpsVariantParamPackAttribute").entered();
            let result = ffi::cudnnGetFusedOpsVariantParamPackAttribute(self.handle, paramlabel, ptr);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

pub fn fused_ops_execute(
    handle: ffi::cudnnHandle_t,
    plan: ffi::cudnnFusedOpsPlan_t,
    varpack: ffi::cudnnFusedOpsVariantParamPack_t
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnFusedOpsExecute(handle, plan, varpack);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

}

// Additional methods for cudaGraphicsResource_t
impl CudaGraphicsResource {
    #[inline]
    pub fn graphics_unregister_resource(
        &mut self,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudaGraphicsUnregisterResource").entered();
            let result = ffi::cudaGraphicsUnregisterResource(self.handle);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn graphics_resource_set_map_flags(
        &mut self,
        flags: :: core :: ffi :: c_uint,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudaGraphicsResourceSetMapFlags").entered();
            let result = ffi::cudaGraphicsResourceSetMapFlags(self.handle, flags);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

pub fn graphics_map_resources(
    count: :: core :: ffi :: c_int,
    resources: *mutffi::cudaGraphicsResource_t,
    stream: ffi::cudaStream_t
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaGraphicsMapResources(count, resources, stream);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn graphics_unmap_resources(
    count: :: core :: ffi :: c_int,
    resources: *mutffi::cudaGraphicsResource_t,
    stream: ffi::cudaStream_t
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaGraphicsUnmapResources(count, resources, stream);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn graphics_resource_get_mapped_pointer(
    devptr: *mut *mut :: core :: ffi ::c_void,
    size: *mutusize,
    resource: ffi::cudaGraphicsResource_t
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaGraphicsResourceGetMappedPointer(devptr, size, resource);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn graphics_sub_resource_get_mapped_array(
    array: *mutffi::cudaArray_t,
    resource: ffi::cudaGraphicsResource_t,
    arrayindex: :: core :: ffi :: c_uint,
    miplevel: :: core :: ffi :: c_uint
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaGraphicsSubResourceGetMappedArray(array, resource, arrayindex, miplevel);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn graphics_resource_get_mapped_mipmapped_array(
    mipmappedarray: *mutffi::cudaMipmappedArray_t,
    resource: ffi::cudaGraphicsResource_t
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudaGraphicsResourceGetMappedMipmappedArray(mipmappedarray, resource);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

}

// Additional methods for cudnnTensorTransformDescriptor_t
impl CudnnTensorTransformDescriptor {
    #[inline]
    pub fn init_transform_dest(
        &mut self,
        srcdesc: ffi::cudnnTensorDescriptor_t,
        destdesc: ffi::cudnnTensorDescriptor_t,
        destsizeinbytes: *mutusize,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if destsizeinbytes.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if destsizeinbytes.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnInitTransformDest").entered();
            let result = ffi::cudnnInitTransformDest(self.handle, srcdesc, destdesc, destsizeinbytes);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn set_tensor_transform_descriptor(
        &mut self,
        nbdims: u32,
        destformat: ffi::cudnnTensorFormat_t,
        padbeforea: *consti32,
        padaftera: *consti32,
        folda: *constu32,
        direction: ffi::cudnnFoldingDirection_t,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if padbeforea.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if padbeforea.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if padaftera.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if padaftera.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if folda.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if folda.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnSetTensorTransformDescriptor").entered();
            let result = ffi::cudnnSetTensorTransformDescriptor(self.handle, nbdims, destformat, padbeforea, padaftera, folda, direction);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

    #[inline]
    pub fn get_tensor_transform_descriptor(
        &mut self,
        nbdimsrequested: u32,
        destformat: *mutffi::cudnnTensorFormat_t,
        padbeforea: *muti32,
        padaftera: *muti32,
        folda: *mutu32,
        direction: *mutffi::cudnnFoldingDirection_t,
) -> Result<(), Error> {
        unsafe {
            #[cfg(feature = "strict")]
            if destformat.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if destformat.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if padbeforea.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if padbeforea.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if padaftera.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if padaftera.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if folda.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if folda.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "strict")]
            if direction.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(all(feature = "balanced", not(feature = "strict")))]
            if direction.is_null() {
                return Err(Error::NullPointer);
            }
            #[cfg(feature = "tracing")]
            let _span = tracing::trace_span!("ffi_call", function = "cudnnGetTensorTransformDescriptor").entered();
            let result = ffi::cudnnGetTensorTransformDescriptor(self.handle, nbdimsrequested, destformat, padbeforea, padaftera, folda, direction);
            if result == 0 {
                Ok(())
            } else {
                Err(Error::from(result))
            }
        }
    }

pub fn transform_tensor_ex(
    handle: ffi::cudnnHandle_t,
    transdesc: ffi::cudnnTensorTransformDescriptor_t,
    alpha: *const :: core :: ffi ::c_void,
    srcdesc: ffi::cudnnTensorDescriptor_t,
    srcdata: *const :: core :: ffi ::c_void,
    beta: *const :: core :: ffi ::c_void,
    destdesc: ffi::cudnnTensorDescriptor_t,
    destdata: *mut :: core :: ffi ::c_void
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnTransformTensorEx(handle, transdesc, alpha, srcdesc, srcdata, beta, destdesc, destdata);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn transform_filter(
    handle: ffi::cudnnHandle_t,
    transdesc: ffi::cudnnTensorTransformDescriptor_t,
    alpha: *const :: core :: ffi ::c_void,
    srcdesc: ffi::cudnnFilterDescriptor_t,
    srcdata: *const :: core :: ffi ::c_void,
    beta: *const :: core :: ffi ::c_void,
    destdesc: ffi::cudnnFilterDescriptor_t,
    destdata: *mut :: core :: ffi ::c_void
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnTransformFilter(handle, transdesc, alpha, srcdesc, srcdata, beta, destdesc, destdata);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

pub fn get_folded_conv_backward_data_descriptors(
    handle: ffi::cudnnHandle_t,
    filterdesc: ffi::cudnnFilterDescriptor_t,
    diffdesc: ffi::cudnnTensorDescriptor_t,
    convdesc: ffi::cudnnConvolutionDescriptor_t,
    graddesc: ffi::cudnnTensorDescriptor_t,
    transformformat: ffi::cudnnTensorFormat_t,
    foldedfilterdesc: ffi::cudnnFilterDescriptor_t,
    paddeddiffdesc: ffi::cudnnTensorDescriptor_t,
    foldedconvdesc: ffi::cudnnConvolutionDescriptor_t,
    foldedgraddesc: ffi::cudnnTensorDescriptor_t,
    filterfoldtransdesc: ffi::cudnnTensorTransformDescriptor_t,
    diffpadtransdesc: ffi::cudnnTensorTransformDescriptor_t,
    gradfoldtransdesc: ffi::cudnnTensorTransformDescriptor_t,
    gradunfoldtransdesc: ffi::cudnnTensorTransformDescriptor_t
) -> Result<(), Error> {
    unsafe {
        let result = ffi::cudnnGetFoldedConvBackwardDataDescriptors(handle, filterdesc, diffdesc, convdesc, graddesc, transformformat, foldedfilterdesc, paddeddiffdesc, foldedconvdesc, foldedgraddesc, filterfoldtransdesc, diffpadtransdesc, gradfoldtransdesc, gradunfoldtransdesc);
        if result == 0 {
            Ok(())
        } else {
            Err(Error::from(result))
        }
    }
}

}

// Typestate builder for MutCudaTextureObjectT
// Ensures compile-time enforcement of builder order

// State marker types
pub struct MutCudaTextureObjectTBuilderInitial;
pub struct MutCudaTextureObjectTBuilderBuilt;

pub struct MutCudaTextureObjectTBuilder<State> {
    _state: std::marker::PhantomData<State>,
    // Builder fields
}

impl MutCudaTextureObjectTBuilder<MutCudaTextureObjectTBuilderInitial> {
    pub fn new() -> Self {
        Self {
            _state: std::marker::PhantomData,
        }
    }
}

impl MutCudaTextureObjectTBuilder<MutCudaTextureObjectTBuilderInitial> {
    pub fn cuda_create_texture_object(self) -> MutCudaTextureObjectTBuilder<MutCudaTextureObjectTBuilderBuilt> {
        // Call FFI: cudaCreateTextureObject()
        MutCudaTextureObjectTBuilder::<MutCudaTextureObjectTBuilderBuilt> {
            _state: std::marker::PhantomData,
        }
    }
}

impl MutCudaTextureObjectTBuilder<MutCudaTextureObjectTBuilderBuilt> {
    pub fn build(self) -> MutCudaTextureObjectT {
        // Finalize and return MutCudaTextureObjectT
        todo!("Implement MutCudaTextureObjectT  construction")
    }
}

// Usage example:
// let obj = MutCudaTextureObjectTBuilder::new()
//     .create()
//     .build();

// Typestate builder for MutCudaSurfaceObjectT
// Ensures compile-time enforcement of builder order

// State marker types
pub struct MutCudaSurfaceObjectTBuilderInitial;
pub struct MutCudaSurfaceObjectTBuilderBuilt;

pub struct MutCudaSurfaceObjectTBuilder<State> {
    _state: std::marker::PhantomData<State>,
    // Builder fields
}

impl MutCudaSurfaceObjectTBuilder<MutCudaSurfaceObjectTBuilderInitial> {
    pub fn new() -> Self {
        Self {
            _state: std::marker::PhantomData,
        }
    }
}

impl MutCudaSurfaceObjectTBuilder<MutCudaSurfaceObjectTBuilderInitial> {
    pub fn cuda_create_surface_object(self) -> MutCudaSurfaceObjectTBuilder<MutCudaSurfaceObjectTBuilderBuilt> {
        // Call FFI: cudaCreateSurfaceObject()
        MutCudaSurfaceObjectTBuilder::<MutCudaSurfaceObjectTBuilderBuilt> {
            _state: std::marker::PhantomData,
        }
    }
}

impl MutCudaSurfaceObjectTBuilder<MutCudaSurfaceObjectTBuilderBuilt> {
    pub fn build(self) -> MutCudaSurfaceObjectT {
        // Finalize and return MutCudaSurfaceObjectT
        todo!("Implement MutCudaSurfaceObjectT  construction")
    }
}

// Usage example:
// let obj = MutCudaSurfaceObjectTBuilder::new()
//     .create()
//     .build();

// Typestate builder for MutCudaGraphConditionalHandle
// Ensures compile-time enforcement of builder order

// State marker types
pub struct MutCudaGraphConditionalHandleBuilderInitial;
pub struct MutCudaGraphConditionalHandleBuilderBuilt;

pub struct MutCudaGraphConditionalHandleBuilder<State> {
    _state: std::marker::PhantomData<State>,
    // Builder fields
}

impl MutCudaGraphConditionalHandleBuilder<MutCudaGraphConditionalHandleBuilderInitial> {
    pub fn new() -> Self {
        Self {
            _state: std::marker::PhantomData,
        }
    }
}

impl MutCudaGraphConditionalHandleBuilder<MutCudaGraphConditionalHandleBuilderInitial> {
    pub fn cuda_graph_conditional_handle_create(self) -> MutCudaGraphConditionalHandleBuilder<MutCudaGraphConditionalHandleBuilderBuilt> {
        // Call FFI: cudaGraphConditionalHandleCreate()
        MutCudaGraphConditionalHandleBuilder::<MutCudaGraphConditionalHandleBuilderBuilt> {
            _state: std::marker::PhantomData,
        }
    }
}

impl MutCudaGraphConditionalHandleBuilder<MutCudaGraphConditionalHandleBuilderBuilt> {
    pub fn build(self) -> MutCudaGraphConditionalHandle {
        // Finalize and return MutCudaGraphConditionalHandle
        todo!("Implement MutCudaGraphConditionalHandle  construction")
    }
}

// Usage example:
// let obj = MutCudaGraphConditionalHandleBuilder::new()
//     .create()
//     .build();

// Typestate builder for MutCudnnHandleT
// Ensures compile-time enforcement of builder order

// State marker types
pub struct MutCudnnHandleTBuilderInitial;
pub struct MutCudnnHandleTBuilderBuilt;

pub struct MutCudnnHandleTBuilder<State> {
    _state: std::marker::PhantomData<State>,
    // Builder fields
}

impl MutCudnnHandleTBuilder<MutCudnnHandleTBuilderInitial> {
    pub fn new() -> Self {
        Self {
            _state: std::marker::PhantomData,
        }
    }
}

impl MutCudnnHandleTBuilder<MutCudnnHandleTBuilderInitial> {
    pub fn cudnn_create(self) -> MutCudnnHandleTBuilder<MutCudnnHandleTBuilderBuilt> {
        // Call FFI: cudnnCreate()
        MutCudnnHandleTBuilder::<MutCudnnHandleTBuilderBuilt> {
            _state: std::marker::PhantomData,
        }
    }
}

impl MutCudnnHandleTBuilder<MutCudnnHandleTBuilderBuilt> {
    pub fn build(self) -> MutCudnnHandleT {
        // Finalize and return MutCudnnHandleT
        todo!("Implement MutCudnnHandleT  construction")
    }
}

// Usage example:
// let obj = MutCudnnHandleTBuilder::new()
//     .create()
//     .build();

// Typestate builder for CudnnBackendInitialize
// Ensures compile-time enforcement of builder order

// State marker types
pub struct CudnnBackendInitializeBuilderInitial;
pub struct CudnnBackendInitializeBuilderBuilt;

pub struct CudnnBackendInitializeBuilder<State> {
    _state: std::marker::PhantomData<State>,
    // Builder fields
}

impl CudnnBackendInitializeBuilder<CudnnBackendInitializeBuilderInitial> {
    pub fn new() -> Self {
        Self {
            _state: std::marker::PhantomData,
        }
    }
}

impl CudnnBackendInitializeBuilder<CudnnBackendInitializeBuilderInitial> {
    pub fn cudnn_backend_initialize(self) -> CudnnBackendInitializeBuilder<CudnnBackendInitializeBuilderBuilt> {
        // Call FFI: cudnnBackendInitialize()
        CudnnBackendInitializeBuilder::<CudnnBackendInitializeBuilderBuilt> {
            _state: std::marker::PhantomData,
        }
    }
}

impl CudnnBackendInitializeBuilder<CudnnBackendInitializeBuilderBuilt> {
    pub fn build(self) -> CudnnBackendInitialize {
        // Finalize and return CudnnBackendInitialize
        todo!("Implement CudnnBackendInitialize  construction")
    }
}

// Usage example:
// let obj = CudnnBackendInitializeBuilder::new()
//     .create()
//     .build();

// Typestate builder for MutCudnnReduceTensorDescriptorT
// Ensures compile-time enforcement of builder order

// State marker types
pub struct MutCudnnReduceTensorDescriptorTBuilderInitial;
pub struct MutCudnnReduceTensorDescriptorTBuilderBuilt;

pub struct MutCudnnReduceTensorDescriptorTBuilder<State> {
    _state: std::marker::PhantomData<State>,
    // Builder fields
}

impl MutCudnnReduceTensorDescriptorTBuilder<MutCudnnReduceTensorDescriptorTBuilderInitial> {
    pub fn new() -> Self {
        Self {
            _state: std::marker::PhantomData,
        }
    }
}

impl MutCudnnReduceTensorDescriptorTBuilder<MutCudnnReduceTensorDescriptorTBuilderInitial> {
    pub fn cudnn_create_reduce_tensor_descriptor(self) -> MutCudnnReduceTensorDescriptorTBuilder<MutCudnnReduceTensorDescriptorTBuilderBuilt> {
        // Call FFI: cudnnCreateReduceTensorDescriptor()
        MutCudnnReduceTensorDescriptorTBuilder::<MutCudnnReduceTensorDescriptorTBuilderBuilt> {
            _state: std::marker::PhantomData,
        }
    }
}

impl MutCudnnReduceTensorDescriptorTBuilder<MutCudnnReduceTensorDescriptorTBuilderBuilt> {
    pub fn build(self) -> MutCudnnReduceTensorDescriptorT {
        // Finalize and return MutCudnnReduceTensorDescriptorT
        todo!("Implement MutCudnnReduceTensorDescriptorT  construction")
    }
}

// Usage example:
// let obj = MutCudnnReduceTensorDescriptorTBuilder::new()
//     .create()
//     .build();

// Typestate builder for MutCudnnTensorTransformDescriptorT
// Ensures compile-time enforcement of builder order

// State marker types
pub struct MutCudnnTensorTransformDescriptorTBuilderInitial;
pub struct MutCudnnTensorTransformDescriptorTBuilderBuilt;

pub struct MutCudnnTensorTransformDescriptorTBuilder<State> {
    _state: std::marker::PhantomData<State>,
    // Builder fields
}

impl MutCudnnTensorTransformDescriptorTBuilder<MutCudnnTensorTransformDescriptorTBuilderInitial> {
    pub fn new() -> Self {
        Self {
            _state: std::marker::PhantomData,
        }
    }
}

impl MutCudnnTensorTransformDescriptorTBuilder<MutCudnnTensorTransformDescriptorTBuilderInitial> {
    pub fn cudnn_create_tensor_transform_descriptor(self) -> MutCudnnTensorTransformDescriptorTBuilder<MutCudnnTensorTransformDescriptorTBuilderBuilt> {
        // Call FFI: cudnnCreateTensorTransformDescriptor()
        MutCudnnTensorTransformDescriptorTBuilder::<MutCudnnTensorTransformDescriptorTBuilderBuilt> {
            _state: std::marker::PhantomData,
        }
    }
}

impl MutCudnnTensorTransformDescriptorTBuilder<MutCudnnTensorTransformDescriptorTBuilderBuilt> {
    pub fn build(self) -> MutCudnnTensorTransformDescriptorT {
        // Finalize and return MutCudnnTensorTransformDescriptorT
        todo!("Implement MutCudnnTensorTransformDescriptorT  construction")
    }
}

// Usage example:
// let obj = MutCudnnTensorTransformDescriptorTBuilder::new()
//     .create()
//     .build();

// Typestate builder for SecurityInitCookie
// Ensures compile-time enforcement of builder order

// State marker types
pub struct SecurityInitCookieBuilderInitial;
pub struct SecurityInitCookieBuilderBuilt;

pub struct SecurityInitCookieBuilder<State> {
    _state: std::marker::PhantomData<State>,
    // Builder fields
}

impl SecurityInitCookieBuilder<SecurityInitCookieBuilderInitial> {
    pub fn new() -> Self {
        Self {
            _state: std::marker::PhantomData,
        }
    }
}

impl SecurityInitCookieBuilder<SecurityInitCookieBuilderInitial> {
    pub fn __security_init_cookie(self) -> SecurityInitCookieBuilder<SecurityInitCookieBuilderBuilt> {
        // Call FFI: __security_init_cookie()
        SecurityInitCookieBuilder::<SecurityInitCookieBuilderBuilt> {
            _state: std::marker::PhantomData,
        }
    }
}

impl SecurityInitCookieBuilder<SecurityInitCookieBuilderBuilt> {
    pub fn build(self) -> SecurityInitCookie {
        // Finalize and return SecurityInitCookie
        todo!("Implement SecurityInitCookie  construction")
    }
}

// Usage example:
// let obj = SecurityInitCookieBuilder::new()
//     .create()
//     .build();

// Typestate builder for MutCudnnLRNDescriptorT
// Ensures compile-time enforcement of builder order

// State marker types
pub struct MutCudnnLRNDescriptorTBuilderInitial;
pub struct MutCudnnLRNDescriptorTBuilderBuilt;

pub struct MutCudnnLRNDescriptorTBuilder<State> {
    _state: std::marker::PhantomData<State>,
    // Builder fields
}

impl MutCudnnLRNDescriptorTBuilder<MutCudnnLRNDescriptorTBuilderInitial> {
    pub fn new() -> Self {
        Self {
            _state: std::marker::PhantomData,
        }
    }
}

impl MutCudnnLRNDescriptorTBuilder<MutCudnnLRNDescriptorTBuilderInitial> {
    pub fn cudnn_create_l_r_n_descriptor(self) -> MutCudnnLRNDescriptorTBuilder<MutCudnnLRNDescriptorTBuilderBuilt> {
        // Call FFI: cudnnCreateLRNDescriptor()
        MutCudnnLRNDescriptorTBuilder::<MutCudnnLRNDescriptorTBuilderBuilt> {
            _state: std::marker::PhantomData,
        }
    }
}

impl MutCudnnLRNDescriptorTBuilder<MutCudnnLRNDescriptorTBuilderBuilt> {
    pub fn build(self) -> MutCudnnLRNDescriptorT {
        // Finalize and return MutCudnnLRNDescriptorT
        todo!("Implement MutCudnnLRNDescriptorT  construction")
    }
}

// Usage example:
// let obj = MutCudnnLRNDescriptorTBuilder::new()
//     .create()
//     .build();

// Typestate builder for MutCudnnSpatialTransformerDescriptorT
// Ensures compile-time enforcement of builder order

// State marker types
pub struct MutCudnnSpatialTransformerDescriptorTBuilderInitial;
pub struct MutCudnnSpatialTransformerDescriptorTBuilderBuilt;

pub struct MutCudnnSpatialTransformerDescriptorTBuilder<State> {
    _state: std::marker::PhantomData<State>,
    // Builder fields
}

impl MutCudnnSpatialTransformerDescriptorTBuilder<MutCudnnSpatialTransformerDescriptorTBuilderInitial> {
    pub fn new() -> Self {
        Self {
            _state: std::marker::PhantomData,
        }
    }
}

impl MutCudnnSpatialTransformerDescriptorTBuilder<MutCudnnSpatialTransformerDescriptorTBuilderInitial> {
    pub fn cudnn_create_spatial_transformer_descriptor(self) -> MutCudnnSpatialTransformerDescriptorTBuilder<MutCudnnSpatialTransformerDescriptorTBuilderBuilt> {
        // Call FFI: cudnnCreateSpatialTransformerDescriptor()
        MutCudnnSpatialTransformerDescriptorTBuilder::<MutCudnnSpatialTransformerDescriptorTBuilderBuilt> {
            _state: std::marker::PhantomData,
        }
    }
}

impl MutCudnnSpatialTransformerDescriptorTBuilder<MutCudnnSpatialTransformerDescriptorTBuilderBuilt> {
    pub fn build(self) -> MutCudnnSpatialTransformerDescriptorT {
        // Finalize and return MutCudnnSpatialTransformerDescriptorT
        todo!("Implement MutCudnnSpatialTransformerDescriptorT  construction")
    }
}

// Usage example:
// let obj = MutCudnnSpatialTransformerDescriptorTBuilder::new()
//     .create()
//     .build();

// Typestate builder for MutCudnnConvolutionDescriptorT
// Ensures compile-time enforcement of builder order

// State marker types
pub struct MutCudnnConvolutionDescriptorTBuilderInitial;
pub struct MutCudnnConvolutionDescriptorTBuilderBuilt;

pub struct MutCudnnConvolutionDescriptorTBuilder<State> {
    _state: std::marker::PhantomData<State>,
    // Builder fields
}

impl MutCudnnConvolutionDescriptorTBuilder<MutCudnnConvolutionDescriptorTBuilderInitial> {
    pub fn new() -> Self {
        Self {
            _state: std::marker::PhantomData,
        }
    }
}

impl MutCudnnConvolutionDescriptorTBuilder<MutCudnnConvolutionDescriptorTBuilderInitial> {
    pub fn cudnn_create_convolution_descriptor(self) -> MutCudnnConvolutionDescriptorTBuilder<MutCudnnConvolutionDescriptorTBuilderBuilt> {
        // Call FFI: cudnnCreateConvolutionDescriptor()
        MutCudnnConvolutionDescriptorTBuilder::<MutCudnnConvolutionDescriptorTBuilderBuilt> {
            _state: std::marker::PhantomData,
        }
    }
}

impl MutCudnnConvolutionDescriptorTBuilder<MutCudnnConvolutionDescriptorTBuilderBuilt> {
    pub fn build(self) -> MutCudnnConvolutionDescriptorT {
        // Finalize and return MutCudnnConvolutionDescriptorT
        todo!("Implement MutCudnnConvolutionDescriptorT  construction")
    }
}

// Usage example:
// let obj = MutCudnnConvolutionDescriptorTBuilder::new()
//     .create()
//     .build();

// Typestate builder for MutCudnnDropoutDescriptorT
// Ensures compile-time enforcement of builder order

// State marker types
pub struct MutCudnnDropoutDescriptorTBuilderInitial;
pub struct MutCudnnDropoutDescriptorTBuilderBuilt;

pub struct MutCudnnDropoutDescriptorTBuilder<State> {
    _state: std::marker::PhantomData<State>,
    // Builder fields
}

impl MutCudnnDropoutDescriptorTBuilder<MutCudnnDropoutDescriptorTBuilderInitial> {
    pub fn new() -> Self {
        Self {
            _state: std::marker::PhantomData,
        }
    }
}

impl MutCudnnDropoutDescriptorTBuilder<MutCudnnDropoutDescriptorTBuilderInitial> {
    pub fn cudnn_create_dropout_descriptor(self) -> MutCudnnDropoutDescriptorTBuilder<MutCudnnDropoutDescriptorTBuilderBuilt> {
        // Call FFI: cudnnCreateDropoutDescriptor()
        MutCudnnDropoutDescriptorTBuilder::<MutCudnnDropoutDescriptorTBuilderBuilt> {
            _state: std::marker::PhantomData,
        }
    }
}

impl MutCudnnDropoutDescriptorTBuilder<MutCudnnDropoutDescriptorTBuilderBuilt> {
    pub fn build(self) -> MutCudnnDropoutDescriptorT {
        // Finalize and return MutCudnnDropoutDescriptorT
        todo!("Implement MutCudnnDropoutDescriptorT  construction")
    }
}

// Usage example:
// let obj = MutCudnnDropoutDescriptorTBuilder::new()
//     .create()
//     .build();

// Typestate builder for MutCudnnFusedOpsConstParamPackT
// Ensures compile-time enforcement of builder order

// State marker types
pub struct MutCudnnFusedOpsConstParamPackTBuilderInitial;
pub struct MutCudnnFusedOpsConstParamPackTBuilderBuilt;

pub struct MutCudnnFusedOpsConstParamPackTBuilder<State> {
    _state: std::marker::PhantomData<State>,
    // Builder fields
}

impl MutCudnnFusedOpsConstParamPackTBuilder<MutCudnnFusedOpsConstParamPackTBuilderInitial> {
    pub fn new() -> Self {
        Self {
            _state: std::marker::PhantomData,
        }
    }
}

impl MutCudnnFusedOpsConstParamPackTBuilder<MutCudnnFusedOpsConstParamPackTBuilderInitial> {
    pub fn cudnn_create_fused_ops_const_param_pack(self) -> MutCudnnFusedOpsConstParamPackTBuilder<MutCudnnFusedOpsConstParamPackTBuilderBuilt> {
        // Call FFI: cudnnCreateFusedOpsConstParamPack()
        MutCudnnFusedOpsConstParamPackTBuilder::<MutCudnnFusedOpsConstParamPackTBuilderBuilt> {
            _state: std::marker::PhantomData,
        }
    }
}

impl MutCudnnFusedOpsConstParamPackTBuilder<MutCudnnFusedOpsConstParamPackTBuilderBuilt> {
    pub fn build(self) -> MutCudnnFusedOpsConstParamPackT {
        // Finalize and return MutCudnnFusedOpsConstParamPackT
        todo!("Implement MutCudnnFusedOpsConstParamPackT  construction")
    }
}

// Usage example:
// let obj = MutCudnnFusedOpsConstParamPackTBuilder::new()
//     .create()
//     .build();

// Typestate builder for MutCudnnFusedOpsPlanT
// Ensures compile-time enforcement of builder order

// State marker types
pub struct MutCudnnFusedOpsPlanTBuilderInitial;
pub struct MutCudnnFusedOpsPlanTBuilderBuilt;

pub struct MutCudnnFusedOpsPlanTBuilder<State> {
    _state: std::marker::PhantomData<State>,
    // Builder fields
}

impl MutCudnnFusedOpsPlanTBuilder<MutCudnnFusedOpsPlanTBuilderInitial> {
    pub fn new() -> Self {
        Self {
            _state: std::marker::PhantomData,
        }
    }
}

impl MutCudnnFusedOpsPlanTBuilder<MutCudnnFusedOpsPlanTBuilderInitial> {
    pub fn cudnn_create_fused_ops_plan(self) -> MutCudnnFusedOpsPlanTBuilder<MutCudnnFusedOpsPlanTBuilderBuilt> {
        // Call FFI: cudnnCreateFusedOpsPlan()
        MutCudnnFusedOpsPlanTBuilder::<MutCudnnFusedOpsPlanTBuilderBuilt> {
            _state: std::marker::PhantomData,
        }
    }
}

impl MutCudnnFusedOpsPlanTBuilder<MutCudnnFusedOpsPlanTBuilderBuilt> {
    pub fn build(self) -> MutCudnnFusedOpsPlanT {
        // Finalize and return MutCudnnFusedOpsPlanT
        todo!("Implement MutCudnnFusedOpsPlanT  construction")
    }
}

// Usage example:
// let obj = MutCudnnFusedOpsPlanTBuilder::new()
//     .create()
//     .build();

// Typestate builder for MutCudaStreamT
// Ensures compile-time enforcement of builder order

// State marker types
pub struct MutCudaStreamTBuilderInitial;
pub struct MutCudaStreamTBuilderBuilt;

pub struct MutCudaStreamTBuilder<State> {
    _state: std::marker::PhantomData<State>,
    // Builder fields
}

impl MutCudaStreamTBuilder<MutCudaStreamTBuilderInitial> {
    pub fn new() -> Self {
        Self {
            _state: std::marker::PhantomData,
        }
    }
}

impl MutCudaStreamTBuilder<MutCudaStreamTBuilderInitial> {
    pub fn cuda_stream_create(self) -> MutCudaStreamTBuilder<MutCudaStreamTBuilderBuilt> {
        // Call FFI: cudaStreamCreate()
        MutCudaStreamTBuilder::<MutCudaStreamTBuilderBuilt> {
            _state: std::marker::PhantomData,
        }
    }
    pub fn cuda_stream_create_with_flags(self) -> MutCudaStreamTBuilder<MutCudaStreamTBuilderBuilt> {
        // Call FFI: cudaStreamCreateWithFlags()
        MutCudaStreamTBuilder::<MutCudaStreamTBuilderBuilt> {
            _state: std::marker::PhantomData,
        }
    }
    pub fn cuda_stream_create_with_priority(self) -> MutCudaStreamTBuilder<MutCudaStreamTBuilderBuilt> {
        // Call FFI: cudaStreamCreateWithPriority()
        MutCudaStreamTBuilder::<MutCudaStreamTBuilderBuilt> {
            _state: std::marker::PhantomData,
        }
    }
}

impl MutCudaStreamTBuilder<MutCudaStreamTBuilderBuilt> {
    pub fn build(self) -> MutCudaStreamT {
        // Finalize and return MutCudaStreamT
        todo!("Implement MutCudaStreamT  construction")
    }
}

// Usage example:
// let obj = MutCudaStreamTBuilder::new()
//     .create()
//     .cuda_stream_create_with_flags()
//     .cuda_stream_create_with_priority()
//     .build();

// Typestate builder for MutCudnnOpTensorDescriptorT
// Ensures compile-time enforcement of builder order

// State marker types
pub struct MutCudnnOpTensorDescriptorTBuilderInitial;
pub struct MutCudnnOpTensorDescriptorTBuilderBuilt;

pub struct MutCudnnOpTensorDescriptorTBuilder<State> {
    _state: std::marker::PhantomData<State>,
    // Builder fields
}

impl MutCudnnOpTensorDescriptorTBuilder<MutCudnnOpTensorDescriptorTBuilderInitial> {
    pub fn new() -> Self {
        Self {
            _state: std::marker::PhantomData,
        }
    }
}

impl MutCudnnOpTensorDescriptorTBuilder<MutCudnnOpTensorDescriptorTBuilderInitial> {
    pub fn cudnn_create_op_tensor_descriptor(self) -> MutCudnnOpTensorDescriptorTBuilder<MutCudnnOpTensorDescriptorTBuilderBuilt> {
        // Call FFI: cudnnCreateOpTensorDescriptor()
        MutCudnnOpTensorDescriptorTBuilder::<MutCudnnOpTensorDescriptorTBuilderBuilt> {
            _state: std::marker::PhantomData,
        }
    }
}

impl MutCudnnOpTensorDescriptorTBuilder<MutCudnnOpTensorDescriptorTBuilderBuilt> {
    pub fn build(self) -> MutCudnnOpTensorDescriptorT {
        // Finalize and return MutCudnnOpTensorDescriptorT
        todo!("Implement MutCudnnOpTensorDescriptorT  construction")
    }
}

// Usage example:
// let obj = MutCudnnOpTensorDescriptorTBuilder::new()
//     .create()
//     .build();

// Typestate builder for MutUsize
// Ensures compile-time enforcement of builder order

// State marker types
pub struct MutUsizeBuilderInitial;
pub struct MutUsizeBuilderBuilt;

pub struct MutUsizeBuilder<State> {
    _state: std::marker::PhantomData<State>,
    // Builder fields
}

impl MutUsizeBuilder<MutUsizeBuilderInitial> {
    pub fn new() -> Self {
        Self {
            _state: std::marker::PhantomData,
        }
    }
}

impl MutUsizeBuilder<MutUsizeBuilderInitial> {
    pub fn cudnn_init_transform_dest(self) -> MutUsizeBuilder<MutUsizeBuilderBuilt> {
        // Call FFI: cudnnInitTransformDest()
        MutUsizeBuilder::<MutUsizeBuilderBuilt> {
            _state: std::marker::PhantomData,
        }
    }
}

impl MutUsizeBuilder<MutUsizeBuilderBuilt> {
    pub fn build(self) -> MutUsize {
        // Finalize and return MutUsize
        todo!("Implement MutUsize  construction")
    }
}

// Usage example:
// let obj = MutUsizeBuilder::new()
//     .create()
//     .build();

// Typestate builder for MutCudnnPoolingDescriptorT
// Ensures compile-time enforcement of builder order

// State marker types
pub struct MutCudnnPoolingDescriptorTBuilderInitial;
pub struct MutCudnnPoolingDescriptorTBuilderBuilt;

pub struct MutCudnnPoolingDescriptorTBuilder<State> {
    _state: std::marker::PhantomData<State>,
    // Builder fields
}

impl MutCudnnPoolingDescriptorTBuilder<MutCudnnPoolingDescriptorTBuilderInitial> {
    pub fn new() -> Self {
        Self {
            _state: std::marker::PhantomData,
        }
    }
}

impl MutCudnnPoolingDescriptorTBuilder<MutCudnnPoolingDescriptorTBuilderInitial> {
    pub fn cudnn_create_pooling_descriptor(self) -> MutCudnnPoolingDescriptorTBuilder<MutCudnnPoolingDescriptorTBuilderBuilt> {
        // Call FFI: cudnnCreatePoolingDescriptor()
        MutCudnnPoolingDescriptorTBuilder::<MutCudnnPoolingDescriptorTBuilderBuilt> {
            _state: std::marker::PhantomData,
        }
    }
}

impl MutCudnnPoolingDescriptorTBuilder<MutCudnnPoolingDescriptorTBuilderBuilt> {
    pub fn build(self) -> MutCudnnPoolingDescriptorT {
        // Finalize and return MutCudnnPoolingDescriptorT
        todo!("Implement MutCudnnPoolingDescriptorT  construction")
    }
}

// Usage example:
// let obj = MutCudnnPoolingDescriptorTBuilder::new()
//     .create()
//     .build();

// Typestate builder for MutCudnnCTCLossDescriptorT
// Ensures compile-time enforcement of builder order

// State marker types
pub struct MutCudnnCTCLossDescriptorTBuilderInitial;
pub struct MutCudnnCTCLossDescriptorTBuilderBuilt;

pub struct MutCudnnCTCLossDescriptorTBuilder<State> {
    _state: std::marker::PhantomData<State>,
    // Builder fields
}

impl MutCudnnCTCLossDescriptorTBuilder<MutCudnnCTCLossDescriptorTBuilderInitial> {
    pub fn new() -> Self {
        Self {
            _state: std::marker::PhantomData,
        }
    }
}

impl MutCudnnCTCLossDescriptorTBuilder<MutCudnnCTCLossDescriptorTBuilderInitial> {
    pub fn cudnn_create_c_t_c_loss_descriptor(self) -> MutCudnnCTCLossDescriptorTBuilder<MutCudnnCTCLossDescriptorTBuilderBuilt> {
        // Call FFI: cudnnCreateCTCLossDescriptor()
        MutCudnnCTCLossDescriptorTBuilder::<MutCudnnCTCLossDescriptorTBuilderBuilt> {
            _state: std::marker::PhantomData,
        }
    }
}

impl MutCudnnCTCLossDescriptorTBuilder<MutCudnnCTCLossDescriptorTBuilderBuilt> {
    pub fn build(self) -> MutCudnnCTCLossDescriptorT {
        // Finalize and return MutCudnnCTCLossDescriptorT
        todo!("Implement MutCudnnCTCLossDescriptorT  construction")
    }
}

// Usage example:
// let obj = MutCudnnCTCLossDescriptorTBuilder::new()
//     .create()
//     .build();

// Typestate builder for CudaCreateChannelDesc
// Ensures compile-time enforcement of builder order

// State marker types
pub struct CudaCreateChannelDescBuilderInitial;
pub struct CudaCreateChannelDescBuilderBuilt;

pub struct CudaCreateChannelDescBuilder<State> {
    _state: std::marker::PhantomData<State>,
    // Builder fields
}

impl CudaCreateChannelDescBuilder<CudaCreateChannelDescBuilderInitial> {
    pub fn new() -> Self {
        Self {
            _state: std::marker::PhantomData,
        }
    }
}

impl CudaCreateChannelDescBuilder<CudaCreateChannelDescBuilderInitial> {
    pub fn cuda_create_channel_desc(self) -> CudaCreateChannelDescBuilder<CudaCreateChannelDescBuilderBuilt> {
        // Call FFI: cudaCreateChannelDesc()
        CudaCreateChannelDescBuilder::<CudaCreateChannelDescBuilderBuilt> {
            _state: std::marker::PhantomData,
        }
    }
}

impl CudaCreateChannelDescBuilder<CudaCreateChannelDescBuilderBuilt> {
    pub fn build(self) -> CudaCreateChannelDesc {
        // Finalize and return CudaCreateChannelDesc
        todo!("Implement CudaCreateChannelDesc  construction")
    }
}

// Usage example:
// let obj = CudaCreateChannelDescBuilder::new()
//     .create()
//     .build();

// Typestate builder for MutCudaGraphT
// Ensures compile-time enforcement of builder order

// State marker types
pub struct MutCudaGraphTBuilderInitial;
pub struct MutCudaGraphTBuilderBuilt;

pub struct MutCudaGraphTBuilder<State> {
    _state: std::marker::PhantomData<State>,
    // Builder fields
}

impl MutCudaGraphTBuilder<MutCudaGraphTBuilderInitial> {
    pub fn new() -> Self {
        Self {
            _state: std::marker::PhantomData,
        }
    }
}

impl MutCudaGraphTBuilder<MutCudaGraphTBuilderInitial> {
    pub fn cuda_graph_create(self) -> MutCudaGraphTBuilder<MutCudaGraphTBuilderBuilt> {
        // Call FFI: cudaGraphCreate()
        MutCudaGraphTBuilder::<MutCudaGraphTBuilderBuilt> {
            _state: std::marker::PhantomData,
        }
    }
}

impl MutCudaGraphTBuilder<MutCudaGraphTBuilderBuilt> {
    pub fn build(self) -> MutCudaGraphT {
        // Finalize and return MutCudaGraphT
        todo!("Implement MutCudaGraphT  construction")
    }
}

// Usage example:
// let obj = MutCudaGraphTBuilder::new()
//     .create()
//     .build();

// Typestate builder for MutCudnnFusedOpsVariantParamPackT
// Ensures compile-time enforcement of builder order

// State marker types
pub struct MutCudnnFusedOpsVariantParamPackTBuilderInitial;
pub struct MutCudnnFusedOpsVariantParamPackTBuilderBuilt;

pub struct MutCudnnFusedOpsVariantParamPackTBuilder<State> {
    _state: std::marker::PhantomData<State>,
    // Builder fields
}

impl MutCudnnFusedOpsVariantParamPackTBuilder<MutCudnnFusedOpsVariantParamPackTBuilderInitial> {
    pub fn new() -> Self {
        Self {
            _state: std::marker::PhantomData,
        }
    }
}

impl MutCudnnFusedOpsVariantParamPackTBuilder<MutCudnnFusedOpsVariantParamPackTBuilderInitial> {
    pub fn cudnn_create_fused_ops_variant_param_pack(self) -> MutCudnnFusedOpsVariantParamPackTBuilder<MutCudnnFusedOpsVariantParamPackTBuilderBuilt> {
        // Call FFI: cudnnCreateFusedOpsVariantParamPack()
        MutCudnnFusedOpsVariantParamPackTBuilder::<MutCudnnFusedOpsVariantParamPackTBuilderBuilt> {
            _state: std::marker::PhantomData,
        }
    }
}

impl MutCudnnFusedOpsVariantParamPackTBuilder<MutCudnnFusedOpsVariantParamPackTBuilderBuilt> {
    pub fn build(self) -> MutCudnnFusedOpsVariantParamPackT {
        // Finalize and return MutCudnnFusedOpsVariantParamPackT
        todo!("Implement MutCudnnFusedOpsVariantParamPackT  construction")
    }
}

// Usage example:
// let obj = MutCudnnFusedOpsVariantParamPackTBuilder::new()
//     .create()
//     .build();

// Typestate builder for MutCudaUserObjectT
// Ensures compile-time enforcement of builder order

// State marker types
pub struct MutCudaUserObjectTBuilderInitial;
pub struct MutCudaUserObjectTBuilderBuilt;

pub struct MutCudaUserObjectTBuilder<State> {
    _state: std::marker::PhantomData<State>,
    // Builder fields
}

impl MutCudaUserObjectTBuilder<MutCudaUserObjectTBuilderInitial> {
    pub fn new() -> Self {
        Self {
            _state: std::marker::PhantomData,
        }
    }
}

impl MutCudaUserObjectTBuilder<MutCudaUserObjectTBuilderInitial> {
    pub fn cuda_user_object_create(self) -> MutCudaUserObjectTBuilder<MutCudaUserObjectTBuilderBuilt> {
        // Call FFI: cudaUserObjectCreate()
        MutCudaUserObjectTBuilder::<MutCudaUserObjectTBuilderBuilt> {
            _state: std::marker::PhantomData,
        }
    }
}

impl MutCudaUserObjectTBuilder<MutCudaUserObjectTBuilderBuilt> {
    pub fn build(self) -> MutCudaUserObjectT {
        // Finalize and return MutCudaUserObjectT
        todo!("Implement MutCudaUserObjectT  construction")
    }
}

// Usage example:
// let obj = MutCudaUserObjectTBuilder::new()
//     .create()
//     .build();

// Typestate builder for MutCudnnRNNDescriptorT
// Ensures compile-time enforcement of builder order

// State marker types
pub struct MutCudnnRNNDescriptorTBuilderInitial;
pub struct MutCudnnRNNDescriptorTBuilderBuilt;

pub struct MutCudnnRNNDescriptorTBuilder<State> {
    _state: std::marker::PhantomData<State>,
    // Builder fields
}

impl MutCudnnRNNDescriptorTBuilder<MutCudnnRNNDescriptorTBuilderInitial> {
    pub fn new() -> Self {
        Self {
            _state: std::marker::PhantomData,
        }
    }
}

impl MutCudnnRNNDescriptorTBuilder<MutCudnnRNNDescriptorTBuilderInitial> {
    pub fn cudnn_create_r_n_n_descriptor(self) -> MutCudnnRNNDescriptorTBuilder<MutCudnnRNNDescriptorTBuilderBuilt> {
        // Call FFI: cudnnCreateRNNDescriptor()
        MutCudnnRNNDescriptorTBuilder::<MutCudnnRNNDescriptorTBuilderBuilt> {
            _state: std::marker::PhantomData,
        }
    }
}

impl MutCudnnRNNDescriptorTBuilder<MutCudnnRNNDescriptorTBuilderBuilt> {
    pub fn build(self) -> MutCudnnRNNDescriptorT {
        // Finalize and return MutCudnnRNNDescriptorT
        todo!("Implement MutCudnnRNNDescriptorT  construction")
    }
}

// Usage example:
// let obj = MutCudnnRNNDescriptorTBuilder::new()
//     .create()
//     .build();

// Typestate builder for MutCudaEventT
// Ensures compile-time enforcement of builder order

// State marker types
pub struct MutCudaEventTBuilderInitial;
pub struct MutCudaEventTBuilderBuilt;

pub struct MutCudaEventTBuilder<State> {
    _state: std::marker::PhantomData<State>,
    // Builder fields
}

impl MutCudaEventTBuilder<MutCudaEventTBuilderInitial> {
    pub fn new() -> Self {
        Self {
            _state: std::marker::PhantomData,
        }
    }
}

impl MutCudaEventTBuilder<MutCudaEventTBuilderInitial> {
    pub fn cuda_event_create(self) -> MutCudaEventTBuilder<MutCudaEventTBuilderBuilt> {
        // Call FFI: cudaEventCreate()
        MutCudaEventTBuilder::<MutCudaEventTBuilderBuilt> {
            _state: std::marker::PhantomData,
        }
    }
    pub fn cuda_event_create_with_flags(self) -> MutCudaEventTBuilder<MutCudaEventTBuilderBuilt> {
        // Call FFI: cudaEventCreateWithFlags()
        MutCudaEventTBuilder::<MutCudaEventTBuilderBuilt> {
            _state: std::marker::PhantomData,
        }
    }
}

impl MutCudaEventTBuilder<MutCudaEventTBuilderBuilt> {
    pub fn build(self) -> MutCudaEventT {
        // Finalize and return MutCudaEventT
        todo!("Implement MutCudaEventT  construction")
    }
}

// Usage example:
// let obj = MutCudaEventTBuilder::new()
//     .create()
//     .cuda_event_create_with_flags()
//     .build();

// Typestate builder for MutCudnnActivationDescriptorT
// Ensures compile-time enforcement of builder order

// State marker types
pub struct MutCudnnActivationDescriptorTBuilderInitial;
pub struct MutCudnnActivationDescriptorTBuilderBuilt;

pub struct MutCudnnActivationDescriptorTBuilder<State> {
    _state: std::marker::PhantomData<State>,
    // Builder fields
}

impl MutCudnnActivationDescriptorTBuilder<MutCudnnActivationDescriptorTBuilderInitial> {
    pub fn new() -> Self {
        Self {
            _state: std::marker::PhantomData,
        }
    }
}

impl MutCudnnActivationDescriptorTBuilder<MutCudnnActivationDescriptorTBuilderInitial> {
    pub fn cudnn_create_activation_descriptor(self) -> MutCudnnActivationDescriptorTBuilder<MutCudnnActivationDescriptorTBuilderBuilt> {
        // Call FFI: cudnnCreateActivationDescriptor()
        MutCudnnActivationDescriptorTBuilder::<MutCudnnActivationDescriptorTBuilderBuilt> {
            _state: std::marker::PhantomData,
        }
    }
}

impl MutCudnnActivationDescriptorTBuilder<MutCudnnActivationDescriptorTBuilderBuilt> {
    pub fn build(self) -> MutCudnnActivationDescriptorT {
        // Finalize and return MutCudnnActivationDescriptorT
        todo!("Implement MutCudnnActivationDescriptorT  construction")
    }
}

// Usage example:
// let obj = MutCudnnActivationDescriptorTBuilder::new()
//     .create()
//     .build();

// Typestate builder for MutCudnnRNNDataDescriptorT
// Ensures compile-time enforcement of builder order

// State marker types
pub struct MutCudnnRNNDataDescriptorTBuilderInitial;
pub struct MutCudnnRNNDataDescriptorTBuilderBuilt;

pub struct MutCudnnRNNDataDescriptorTBuilder<State> {
    _state: std::marker::PhantomData<State>,
    // Builder fields
}

impl MutCudnnRNNDataDescriptorTBuilder<MutCudnnRNNDataDescriptorTBuilderInitial> {
    pub fn new() -> Self {
        Self {
            _state: std::marker::PhantomData,
        }
    }
}

impl MutCudnnRNNDataDescriptorTBuilder<MutCudnnRNNDataDescriptorTBuilderInitial> {
    pub fn cudnn_create_r_n_n_data_descriptor(self) -> MutCudnnRNNDataDescriptorTBuilder<MutCudnnRNNDataDescriptorTBuilderBuilt> {
        // Call FFI: cudnnCreateRNNDataDescriptor()
        MutCudnnRNNDataDescriptorTBuilder::<MutCudnnRNNDataDescriptorTBuilderBuilt> {
            _state: std::marker::PhantomData,
        }
    }
}

impl MutCudnnRNNDataDescriptorTBuilder<MutCudnnRNNDataDescriptorTBuilderBuilt> {
    pub fn build(self) -> MutCudnnRNNDataDescriptorT {
        // Finalize and return MutCudnnRNNDataDescriptorT
        todo!("Implement MutCudnnRNNDataDescriptorT  construction")
    }
}

// Usage example:
// let obj = MutCudnnRNNDataDescriptorTBuilder::new()
//     .create()
//     .build();

// Typestate builder for MutCudaMemPoolT
// Ensures compile-time enforcement of builder order

// State marker types
pub struct MutCudaMemPoolTBuilderInitial;
pub struct MutCudaMemPoolTBuilderBuilt;

pub struct MutCudaMemPoolTBuilder<State> {
    _state: std::marker::PhantomData<State>,
    // Builder fields
}

impl MutCudaMemPoolTBuilder<MutCudaMemPoolTBuilderInitial> {
    pub fn new() -> Self {
        Self {
            _state: std::marker::PhantomData,
        }
    }
}

impl MutCudaMemPoolTBuilder<MutCudaMemPoolTBuilderInitial> {
    pub fn cuda_mem_pool_create(self) -> MutCudaMemPoolTBuilder<MutCudaMemPoolTBuilderBuilt> {
        // Call FFI: cudaMemPoolCreate()
        MutCudaMemPoolTBuilder::<MutCudaMemPoolTBuilderBuilt> {
            _state: std::marker::PhantomData,
        }
    }
}

impl MutCudaMemPoolTBuilder<MutCudaMemPoolTBuilderBuilt> {
    pub fn build(self) -> MutCudaMemPoolT {
        // Finalize and return MutCudaMemPoolT
        todo!("Implement MutCudaMemPoolT  construction")
    }
}

// Usage example:
// let obj = MutCudaMemPoolTBuilder::new()
//     .create()
//     .build();

// Typestate builder for MutCudnnBackendDescriptorT
// Ensures compile-time enforcement of builder order

// State marker types
pub struct MutCudnnBackendDescriptorTBuilderInitial;
pub struct MutCudnnBackendDescriptorTBuilderBuilt;

pub struct MutCudnnBackendDescriptorTBuilder<State> {
    _state: std::marker::PhantomData<State>,
    // Builder fields
}

impl MutCudnnBackendDescriptorTBuilder<MutCudnnBackendDescriptorTBuilderInitial> {
    pub fn new() -> Self {
        Self {
            _state: std::marker::PhantomData,
        }
    }
}

impl MutCudnnBackendDescriptorTBuilder<MutCudnnBackendDescriptorTBuilderInitial> {
    pub fn cudnn_backend_create_descriptor(self) -> MutCudnnBackendDescriptorTBuilder<MutCudnnBackendDescriptorTBuilderBuilt> {
        // Call FFI: cudnnBackendCreateDescriptor()
        MutCudnnBackendDescriptorTBuilder::<MutCudnnBackendDescriptorTBuilderBuilt> {
            _state: std::marker::PhantomData,
        }
    }
}

impl MutCudnnBackendDescriptorTBuilder<MutCudnnBackendDescriptorTBuilderBuilt> {
    pub fn build(self) -> MutCudnnBackendDescriptorT {
        // Finalize and return MutCudnnBackendDescriptorT
        todo!("Implement MutCudnnBackendDescriptorT  construction")
    }
}

// Usage example:
// let obj = MutCudnnBackendDescriptorTBuilder::new()
//     .create()
//     .build();

// Typestate builder for MutCudnnFilterDescriptorT
// Ensures compile-time enforcement of builder order

// State marker types
pub struct MutCudnnFilterDescriptorTBuilderInitial;
pub struct MutCudnnFilterDescriptorTBuilderBuilt;

pub struct MutCudnnFilterDescriptorTBuilder<State> {
    _state: std::marker::PhantomData<State>,
    // Builder fields
}

impl MutCudnnFilterDescriptorTBuilder<MutCudnnFilterDescriptorTBuilderInitial> {
    pub fn new() -> Self {
        Self {
            _state: std::marker::PhantomData,
        }
    }
}

impl MutCudnnFilterDescriptorTBuilder<MutCudnnFilterDescriptorTBuilderInitial> {
    pub fn cudnn_create_filter_descriptor(self) -> MutCudnnFilterDescriptorTBuilder<MutCudnnFilterDescriptorTBuilderBuilt> {
        // Call FFI: cudnnCreateFilterDescriptor()
        MutCudnnFilterDescriptorTBuilder::<MutCudnnFilterDescriptorTBuilderBuilt> {
            _state: std::marker::PhantomData,
        }
    }
}

impl MutCudnnFilterDescriptorTBuilder<MutCudnnFilterDescriptorTBuilderBuilt> {
    pub fn build(self) -> MutCudnnFilterDescriptorT {
        // Finalize and return MutCudnnFilterDescriptorT
        todo!("Implement MutCudnnFilterDescriptorT  construction")
    }
}

// Usage example:
// let obj = MutCudnnFilterDescriptorTBuilder::new()
//     .create()
//     .build();

// Typestate builder for MutCudnnAttnDescriptorT
// Ensures compile-time enforcement of builder order

// State marker types
pub struct MutCudnnAttnDescriptorTBuilderInitial;
pub struct MutCudnnAttnDescriptorTBuilderBuilt;

pub struct MutCudnnAttnDescriptorTBuilder<State> {
    _state: std::marker::PhantomData<State>,
    // Builder fields
}

impl MutCudnnAttnDescriptorTBuilder<MutCudnnAttnDescriptorTBuilderInitial> {
    pub fn new() -> Self {
        Self {
            _state: std::marker::PhantomData,
        }
    }
}

impl MutCudnnAttnDescriptorTBuilder<MutCudnnAttnDescriptorTBuilderInitial> {
    pub fn cudnn_create_attn_descriptor(self) -> MutCudnnAttnDescriptorTBuilder<MutCudnnAttnDescriptorTBuilderBuilt> {
        // Call FFI: cudnnCreateAttnDescriptor()
        MutCudnnAttnDescriptorTBuilder::<MutCudnnAttnDescriptorTBuilderBuilt> {
            _state: std::marker::PhantomData,
        }
    }
}

impl MutCudnnAttnDescriptorTBuilder<MutCudnnAttnDescriptorTBuilderBuilt> {
    pub fn build(self) -> MutCudnnAttnDescriptorT {
        // Finalize and return MutCudnnAttnDescriptorT
        todo!("Implement MutCudnnAttnDescriptorT  construction")
    }
}

// Usage example:
// let obj = MutCudnnAttnDescriptorTBuilder::new()
//     .create()
//     .build();

// Typestate builder for MutCudnnTensorDescriptorT
// Ensures compile-time enforcement of builder order

// State marker types
pub struct MutCudnnTensorDescriptorTBuilderInitial;
pub struct MutCudnnTensorDescriptorTBuilderBuilt;

pub struct MutCudnnTensorDescriptorTBuilder<State> {
    _state: std::marker::PhantomData<State>,
    // Builder fields
}

impl MutCudnnTensorDescriptorTBuilder<MutCudnnTensorDescriptorTBuilderInitial> {
    pub fn new() -> Self {
        Self {
            _state: std::marker::PhantomData,
        }
    }
}

impl MutCudnnTensorDescriptorTBuilder<MutCudnnTensorDescriptorTBuilderInitial> {
    pub fn cudnn_create_tensor_descriptor(self) -> MutCudnnTensorDescriptorTBuilder<MutCudnnTensorDescriptorTBuilderBuilt> {
        // Call FFI: cudnnCreateTensorDescriptor()
        MutCudnnTensorDescriptorTBuilder::<MutCudnnTensorDescriptorTBuilderBuilt> {
            _state: std::marker::PhantomData,
        }
    }
}

impl MutCudnnTensorDescriptorTBuilder<MutCudnnTensorDescriptorTBuilderBuilt> {
    pub fn build(self) -> MutCudnnTensorDescriptorT {
        // Finalize and return MutCudnnTensorDescriptorT
        todo!("Implement MutCudnnTensorDescriptorT  construction")
    }
}

// Usage example:
// let obj = MutCudnnTensorDescriptorTBuilder::new()
//     .create()
//     .build();

// Typestate builder for CudaInitDevice
// Ensures compile-time enforcement of builder order

// State marker types
pub struct CudaInitDeviceBuilderInitial;
pub struct CudaInitDeviceBuilderBuilt;

pub struct CudaInitDeviceBuilder<State> {
    _state: std::marker::PhantomData<State>,
    // Builder fields
}

impl CudaInitDeviceBuilder<CudaInitDeviceBuilderInitial> {
    pub fn new() -> Self {
        Self {
            _state: std::marker::PhantomData,
        }
    }
}

impl CudaInitDeviceBuilder<CudaInitDeviceBuilderInitial> {
    pub fn cuda_init_device(self) -> CudaInitDeviceBuilder<CudaInitDeviceBuilderBuilt> {
        // Call FFI: cudaInitDevice()
        CudaInitDeviceBuilder::<CudaInitDeviceBuilderBuilt> {
            _state: std::marker::PhantomData,
        }
    }
}

impl CudaInitDeviceBuilder<CudaInitDeviceBuilderBuilt> {
    pub fn build(self) -> CudaInitDevice {
        // Finalize and return CudaInitDevice
        todo!("Implement CudaInitDevice  construction")
    }
}

// Usage example:
// let obj = CudaInitDeviceBuilder::new()
//     .create()
//     .build();

// Typestate builder for MutCudnnSeqDataDescriptorT
// Ensures compile-time enforcement of builder order

// State marker types
pub struct MutCudnnSeqDataDescriptorTBuilderInitial;
pub struct MutCudnnSeqDataDescriptorTBuilderBuilt;

pub struct MutCudnnSeqDataDescriptorTBuilder<State> {
    _state: std::marker::PhantomData<State>,
    // Builder fields
}

impl MutCudnnSeqDataDescriptorTBuilder<MutCudnnSeqDataDescriptorTBuilderInitial> {
    pub fn new() -> Self {
        Self {
            _state: std::marker::PhantomData,
        }
    }
}

impl MutCudnnSeqDataDescriptorTBuilder<MutCudnnSeqDataDescriptorTBuilderInitial> {
    pub fn cudnn_create_seq_data_descriptor(self) -> MutCudnnSeqDataDescriptorTBuilder<MutCudnnSeqDataDescriptorTBuilderBuilt> {
        // Call FFI: cudnnCreateSeqDataDescriptor()
        MutCudnnSeqDataDescriptorTBuilder::<MutCudnnSeqDataDescriptorTBuilderBuilt> {
            _state: std::marker::PhantomData,
        }
    }
}

impl MutCudnnSeqDataDescriptorTBuilder<MutCudnnSeqDataDescriptorTBuilderBuilt> {
    pub fn build(self) -> MutCudnnSeqDataDescriptorT {
        // Finalize and return MutCudnnSeqDataDescriptorT
        todo!("Implement MutCudnnSeqDataDescriptorT  construction")
    }
}

// Usage example:
// let obj = MutCudnnSeqDataDescriptorTBuilder::new()
//     .create()
//     .build();


/// Platform detection utilities
#[cfg(test)]
mod platform_utils {
    /// Check if running on Windows
    #[cfg(target_os = "windows")]
    pub fn is_windows() -> bool { true }
    #[cfg(not(target_os = "windows"))]
    pub fn is_windows() -> bool { false }

    /// Check if running on Linux
    #[cfg(target_os = "linux")]
    pub fn is_linux() -> bool { true }
    #[cfg(not(target_os = "linux"))]
    pub fn is_linux() -> bool { false }

    /// Check if running on macOS
    #[cfg(target_os = "macos")]
    pub fn is_macos() -> bool { true }
    #[cfg(not(target_os = "macos"))]
    pub fn is_macos() -> bool { false }

    /// Check if running on Unix-like system
    #[cfg(unix)]
    pub fn is_unix() -> bool { true }
    #[cfg(not(unix))]
    pub fn is_unix() -> bool { false }

    /// Get current platform name
    pub fn current_platform() -> &'static str {
        if cfg!(target_os = "windows") {
            "Windows"
        } else if cfg!(target_os = "linux") {
            "Linux"
        } else if cfg!(target_os = "macos") {
            "macOS"
        } else if cfg!(unix) {
            "Unix"
        } else {
            "Unknown"
        }
    }
}
